# PROJECT: pytorch_audio FILE: torchaudio/functional/functional.py
# -*- coding: utf-8 -*-

from collections.abc import Sequence
import io
import math
import warnings
from typing import Optional, Tuple

import torch
from torch import Tensor
from torchaudio._internal import module_utils as _mod_utils
import torchaudio

__all__ = [
    "spectrogram",
    "inverse_spectrogram",
    "griffinlim",
    "amplitude_to_DB",
    "DB_to_amplitude",
    "compute_deltas",
    "compute_kaldi_pitch",
    "melscale_fbanks",
    "linear_fbanks",
    "create_dct",
    "compute_deltas",
    "detect_pitch_frequency",
    "DB_to_amplitude",
    "mu_law_encoding",
    "mu_law_decoding",
    "phase_vocoder",
    'mask_along_axis',
    'mask_along_axis_iid',
    'sliding_window_cmn',
    "spectral_centroid",
    "apply_codec",
    "resample",
    "edit_distance",
    "pitch_shift",
    "rnnt_loss",
]


def spectrogram(
        waveform: Tensor,
        pad: int,
        window: Tensor,
        n_fft: int,
        hop_length: int,
        win_length: int,
        power: Optional[float],
        normalized: bool,
        center: bool = True,
        pad_mode: str = "reflect",
        onesided: bool = True,
        return_complex: Optional[bool] = None,
) -> Tensor:
    r"""Create a spectrogram or a batch of spectrograms from a raw audio signal.
    The spectrogram can be either magnitude-only or complex.

    Args:
        waveform (Tensor): Tensor of audio of dimension `(..., time)`
        pad (int): Two sided padding of signal
        window (Tensor): Window tensor that is applied/multiplied to each frame/window
        n_fft (int): Size of FFT
        hop_length (int): Length of hop between STFT windows
        win_length (int): Window size
        power (float or None): Exponent for the magnitude spectrogram,
            (must be > 0) e.g., 1 for energy, 2 for power, etc.
            If None, then the complex spectrum is returned instead.
        normalized (bool): Whether to normalize by magnitude after stft
        center (bool, optional): whether to pad :attr:`waveform` on both sides so
            that the :math:`t`-th frame is centered at time :math:`t \times \text{hop\_length}`.
            Default: ``True``
        pad_mode (string, optional): controls the padding method used when
            :attr:`center` is ``True``. Default: ``"reflect"``
        onesided (bool, optional): controls whether to return half of results to
            avoid redundancy. Default: ``True``
        return_complex (bool, optional):
            Deprecated and not used.

    Returns:
        Tensor: Dimension `(..., freq, time)`, freq is
        ``n_fft // 2 + 1`` and ``n_fft`` is the number of
        Fourier bins, and time is the number of window hops (n_frame).
    """
    if return_complex is not None:
        warnings.warn(
            "`return_complex` argument is now deprecated and is not effective."
            "`torchaudio.functional.spectrogram(power=None)` always returns a tensor with "
            "complex dtype. Please remove the argument in the function call."
        )

    if pad > 0:
        # TODO add "with torch.no_grad():" back when JIT supports it
        waveform = torch.nn.functional.pad(waveform, (pad, pad), "constant")

    # pack batch
    shape = waveform.size()
    waveform = waveform.reshape(-1, shape[-1])

    # default values are consistent with librosa.core.spectrum._spectrogram
    spec_f = torch.stft(
        input=waveform,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        window=window,
        center=center,
        pad_mode=pad_mode,
        normalized=False,
        onesided=onesided,
        return_complex=True,
    )

    # unpack batch
    spec_f = spec_f.reshape(shape[:-1] + spec_f.shape[-2:])

    if normalized:
        spec_f /= window.pow(2.).sum().sqrt()
    if power is not None:
        if power == 1.0:
            return spec_f.abs()
        return spec_f.abs().pow(power)
    return spec_f


def inverse_spectrogram(
        spectrogram: Tensor,
        length: Optional[int],
        pad: int,
        window: Tensor,
        n_fft: int,
        hop_length: int,
        win_length: int,
        normalized: bool,
        center: bool = True,
        pad_mode: str = "reflect",
        onesided: bool = True,
) -> Tensor:
    r"""Create an inverse spectrogram or a batch of inverse spectrograms from the provided
    complex-valued spectrogram.

    Args:
        spectrogram (Tensor): Complex tensor of audio of dimension (..., freq, time).
        length (int or None): The output length of the waveform.
        pad (int): Two sided padding of signal. It is only effective when ``length`` is provided.
        window (Tensor): Window tensor that is applied/multiplied to each frame/window
        n_fft (int): Size of FFT
        hop_length (int): Length of hop between STFT windows
        win_length (int): Window size
        normalized (bool): Whether the stft output was normalized by magnitude
        center (bool, optional): whether the waveform was padded on both sides so
            that the :math:`t`-th frame is centered at time :math:`t \times \text{hop\_length}`.
            Default: ``True``
        pad_mode (string, optional): controls the padding method used when
            :attr:`center` is ``True``. This parameter is provided for compatibility with the
            spectrogram function and is not used. Default: ``"reflect"``
        onesided (bool, optional): controls whether spectrogram was done in onesided mode.
            Default: ``True``

    Returns:
        Tensor: Dimension `(..., time)`. Least squares estimation of the original signal.
    """

    if not spectrogram.is_complex():
        raise ValueError("Expected `spectrogram` to be complex dtype.")

    if normalized:
        spectrogram = spectrogram * window.pow(2.).sum().sqrt()

    # pack batch
    shape = spectrogram.size()
    spectrogram = spectrogram.reshape(-1, shape[-2], shape[-1])

    # default values are consistent with librosa.core.spectrum._spectrogram
    waveform = torch.istft(
        input=spectrogram,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        window=window,
        center=center,
        normalized=False,
        onesided=onesided,
        length=length + 2 * pad if length is not None else None,
        return_complex=False,
    )

    if length is not None and pad > 0:
        # remove padding from front and back
        waveform = waveform[:, pad:-pad]

    # unpack batch
    waveform = waveform.reshape(shape[:-2] + waveform.shape[-1:])

    return waveform


def _get_complex_dtype(real_dtype: torch.dtype):
    if real_dtype == torch.double:
        return torch.cdouble
    if real_dtype == torch.float:
        return torch.cfloat
    if real_dtype == torch.half:
        return torch.complex32
    raise ValueError(f'Unexpected dtype {real_dtype}')


def griffinlim(
        specgram: Tensor,
        window: Tensor,
        n_fft: int,
        hop_length: int,
        win_length: int,
        power: float,
        n_iter: int,
        momentum: float,
        length: Optional[int],
        rand_init: bool
) -> Tensor:
    r"""Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.

    Implementation ported from
    *librosa* [:footcite:`brian_mcfee-proc-scipy-2015`], *A fast Griffin-Lim algorithm* [:footcite:`6701851`]
    and *Signal estimation from modified short-time Fourier transform* [:footcite:`1172092`].

    Args:
        specgram (Tensor): A magnitude-only STFT spectrogram of dimension `(..., freq, frames)`
            where freq is ``n_fft // 2 + 1``.
        window (Tensor): Window tensor that is applied/multiplied to each frame/window
        n_fft (int): Size of FFT, creates ``n_fft // 2 + 1`` bins
        hop_length (int): Length of hop between STFT windows. (
            Default: ``win_length // 2``)
        win_length (int): Window size. (Default: ``n_fft``)
        power (float): Exponent for the magnitude spectrogram,
            (must be > 0) e.g., 1 for energy, 2 for power, etc.
        n_iter (int): Number of iteration for phase recovery process.
        momentum (float): The momentum parameter for fast Griffin-Lim.
            Setting this to 0 recovers the original Griffin-Lim method.
            Values near 1 can lead to faster convergence, but above 1 may not converge.
        length (int or None): Array length of the expected output.
        rand_init (bool): Initializes phase randomly if True, to zero otherwise.

    Returns:
        Tensor: waveform of `(..., time)`, where time equals the ``length`` parameter if given.
    """
    assert momentum < 1, 'momentum={} > 1 can be unstable'.format(momentum)
    assert momentum >= 0, 'momentum={} < 0'.format(momentum)

    # pack batch
    shape = specgram.size()
    specgram = specgram.reshape([-1] + list(shape[-2:]))

    specgram = specgram.pow(1 / power)

    # initialize the phase
    if rand_init:
        angles = torch.rand(
            specgram.size(),
            dtype=_get_complex_dtype(specgram.dtype), device=specgram.device)
    else:
        angles = torch.full(
            specgram.size(), 1,
            dtype=_get_complex_dtype(specgram.dtype), device=specgram.device)

    # And initialize the previous iterate to 0
    tprev = torch.tensor(0., dtype=specgram.dtype, device=specgram.device)
    for _ in range(n_iter):
        # Invert with our current estimate of the phases
        inverse = torch.istft(specgram * angles,
                              n_fft=n_fft,
                              hop_length=hop_length,
                              win_length=win_length,
                              window=window,
                              length=length)

        # Rebuild the spectrogram
        rebuilt = torch.stft(
            input=inverse,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            window=window,
            center=True,
            pad_mode='reflect',
            normalized=False,
            onesided=True,
            return_complex=True,
        )

        # Update our phase estimates
        angles = rebuilt
        if momentum:
            angles = angles - tprev.mul_(momentum / (1 + momentum))
        angles = angles.div(angles.abs().add(1e-16))

        # Store the previous iterate
        tprev = rebuilt

    # Return the final phase estimates
    waveform = torch.istft(specgram * angles,
                           n_fft=n_fft,
                           hop_length=hop_length,
                           win_length=win_length,
                           window=window,
                           length=length)

    # unpack batch
    waveform = waveform.reshape(shape[:-2] + waveform.shape[-1:])

    return waveform


def amplitude_to_DB(
        x: Tensor,
        multiplier: float,
        amin: float,
        db_multiplier: float,
        top_db: Optional[float] = None
) -> Tensor:
    r"""Turn a spectrogram from the power/amplitude scale to the decibel scale.

    The output of each tensor in a batch depends on the maximum value of that tensor,
    and so may return different values for an audio clip split into snippets vs. a full clip.

    Args:

        x (Tensor): Input spectrogram(s) before being converted to decibel scale. Input should take
          the form `(..., freq, time)`. Batched inputs should include a channel dimension and
          have the form `(batch, channel, freq, time)`.
        multiplier (float): Use 10. for power and 20. for amplitude
        amin (float): Number to clamp ``x``
        db_multiplier (float): Log10(max(reference value and amin))
        top_db (float or None, optional): Minimum negative cut-off in decibels. A reasonable number
            is 80. (Default: ``None``)

    Returns:
        Tensor: Output tensor in decibel scale
    """
    x_db = multiplier * torch.log10(torch.clamp(x, min=amin))
    x_db -= multiplier * db_multiplier

    if top_db is not None:
        # Expand batch
        shape = x_db.size()
        packed_channels = shape[-3] if x_db.dim() > 2 else 1
        x_db = x_db.reshape(-1, packed_channels, shape[-2], shape[-1])

        x_db = torch.max(x_db, (x_db.amax(dim=(-3, -2, -1)) - top_db).view(-1, 1, 1, 1))

        # Repack batch
        x_db = x_db.reshape(shape)

    return x_db


def DB_to_amplitude(
        x: Tensor,
        ref: float,
        power: float
) -> Tensor:
    r"""Turn a tensor from the decibel scale to the power/amplitude scale.

    Args:
        x (Tensor): Input tensor before being converted to power/amplitude scale.
        ref (float): Reference which the output will be scaled by.
        power (float): If power equals 1, will compute DB to power. If 0.5, will compute DB to amplitude.

    Returns:
        Tensor: Output tensor in power/amplitude scale.
    """
    return ref * torch.pow(torch.pow(10.0, 0.1 * x), power)


def _hz_to_mel(freq: float, mel_scale: str = "htk") -> float:
    r"""Convert Hz to Mels.

    Args:
        freqs (float): Frequencies in Hz
        mel_scale (str, optional): Scale to use: ``htk`` or ``slaney``. (Default: ``htk``)

    Returns:
        mels (float): Frequency in Mels
    """

    if mel_scale not in ['slaney', 'htk']:
        raise ValueError('mel_scale should be one of "htk" or "slaney".')

    if mel_scale == "htk":
        return 2595.0 * math.log10(1.0 + (freq / 700.0))

    # Fill in the linear part
    f_min = 0.0
    f_sp = 200.0 / 3

    mels = (freq - f_min) / f_sp

    # Fill in the log-scale part
    min_log_hz = 1000.0
    min_log_mel = (min_log_hz - f_min) / f_sp
    logstep = math.log(6.4) / 27.0

    if freq >= min_log_hz:
        mels = min_log_mel + math.log(freq / min_log_hz) / logstep

    return mels


def _mel_to_hz(mels: Tensor, mel_scale: str = "htk") -> Tensor:
    """Convert mel bin numbers to frequencies.

    Args:
        mels (Tensor): Mel frequencies
        mel_scale (str, optional): Scale to use: ``htk`` or ``slaney``. (Default: ``htk``)

    Returns:
        freqs (Tensor): Mels converted in Hz
    """

    if mel_scale not in ['slaney', 'htk']:
        raise ValueError('mel_scale should be one of "htk" or "slaney".')

    if mel_scale == "htk":
        return 700.0 * (10.0**(mels / 2595.0) - 1.0)

    # Fill in the linear scale
    f_min = 0.0
    f_sp = 200.0 / 3
    freqs = f_min + f_sp * mels

    # And now the nonlinear scale
    min_log_hz = 1000.0
    min_log_mel = (min_log_hz - f_min) / f_sp
    logstep = math.log(6.4) / 27.0

    log_t = (mels >= min_log_mel)
    freqs[log_t] = min_log_hz * torch.exp(logstep * (mels[log_t] - min_log_mel))

    return freqs


def _create_triangular_filterbank(
        all_freqs: Tensor,
        f_pts: Tensor,
) -> Tensor:
    """Create a triangular filter bank.

    Args:
        all_freqs (Tensor): STFT freq points of size (`n_freqs`).
        f_pts (Tensor): Filter mid points of size (`n_filter`).

    Returns:
        fb (Tensor): The filter bank of size (`n_freqs`, `n_filter`).
    """
    # Adopted from Librosa
    # calculate the difference between each filter mid point and each stft freq point in hertz
    f_diff = f_pts[1:] - f_pts[:-1]  # (n_filter + 1)
    slopes = f_pts.unsqueeze(0) - all_freqs.unsqueeze(1)  # (n_freqs, n_filter + 2)
    # create overlapping triangles
    zero = torch.zeros(1)
    down_slopes = (-1.0 * slopes[:, :-2]) / f_diff[:-1]  # (n_freqs, n_filter)
    up_slopes = slopes[:, 2:] / f_diff[1:]  # (n_freqs, n_filter)
    fb = torch.max(zero, torch.min(down_slopes, up_slopes))

    return fb


def melscale_fbanks(
        n_freqs: int,
        f_min: float,
        f_max: float,
        n_mels: int,
        sample_rate: int,
        norm: Optional[str] = None,
        mel_scale: str = "htk",
) -> Tensor:
    r"""Create a frequency bin conversion matrix.

    Note:
        For the sake of the numerical compatibility with librosa, not all the coefficients
        in the resulting filter bank has magnitude of 1.

        .. image:: https://download.pytorch.org/torchaudio/doc-assets/mel_fbanks.png
           :alt: Visualization of generated filter bank

    Args:
        n_freqs (int): Number of frequencies to highlight/apply
        f_min (float): Minimum frequency (Hz)
        f_max (float): Maximum frequency (Hz)
        n_mels (int): Number of mel filterbanks
        sample_rate (int): Sample rate of the audio waveform
        norm (str or None, optional): If 'slaney', divide the triangular mel weights by the width of the mel band
            (area normalization). (Default: ``None``)
        mel_scale (str, optional): Scale to use: ``htk`` or ``slaney``. (Default: ``htk``)

    Returns:
        Tensor: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_mels``)
        meaning number of frequencies to highlight/apply to x the number of filterbanks.
        Each column is a filterbank so that assuming there is a matrix A of
        size (..., ``n_freqs``), the applied result would be
        ``A * melscale_fbanks(A.size(-1), ...)``.

    """

    if norm is not None and norm != "slaney":
        raise ValueError("norm must be one of None or 'slaney'")

    # freq bins
    all_freqs = torch.linspace(0, sample_rate // 2, n_freqs)

    # calculate mel freq bins
    m_min = _hz_to_mel(f_min, mel_scale=mel_scale)
    m_max = _hz_to_mel(f_max, mel_scale=mel_scale)

    m_pts = torch.linspace(m_min, m_max, n_mels + 2)
    f_pts = _mel_to_hz(m_pts, mel_scale=mel_scale)

    # create filterbank
    fb = _create_triangular_filterbank(all_freqs, f_pts)

    if norm is not None and norm == "slaney":
        # Slaney-style mel is scaled to be approx constant energy per channel
        enorm = 2.0 / (f_pts[2:n_mels + 2] - f_pts[:n_mels])
        fb *= enorm.unsqueeze(0)

    if (fb.max(dim=0).values == 0.).any():
        warnings.warn(
            "At least one mel filterbank has all zero values. "
            f"The value for `n_mels` ({n_mels}) may be set too high. "
            f"Or, the value for `n_freqs` ({n_freqs}) may be set too low."
        )

    return fb


def linear_fbanks(
        n_freqs: int,
        f_min: float,
        f_max: float,
        n_filter: int,
        sample_rate: int,
) -> Tensor:
    r"""Creates a linear triangular filterbank.

    Note:
        For the sake of the numerical compatibility with librosa, not all the coefficients
        in the resulting filter bank has magnitude of 1.

        .. image:: https://download.pytorch.org/torchaudio/doc-assets/lin_fbanks.png
           :alt: Visualization of generated filter bank

    Args:
        n_freqs (int): Number of frequencies to highlight/apply
        f_min (float): Minimum frequency (Hz)
        f_max (float): Maximum frequency (Hz)
        n_filter (int): Number of (linear) triangular filter
        sample_rate (int): Sample rate of the audio waveform

    Returns:
        Tensor: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_filter``)
        meaning number of frequencies to highlight/apply to x the number of filterbanks.
        Each column is a filterbank so that assuming there is a matrix A of
        size (..., ``n_freqs``), the applied result would be
        ``A * linear_fbanks(A.size(-1), ...)``.
    """
    # freq bins
    all_freqs = torch.linspace(0, sample_rate // 2, n_freqs)

    # filter mid-points
    f_pts = torch.linspace(f_min, f_max, n_filter + 2)

    # create filterbank
    fb = _create_triangular_filterbank(all_freqs, f_pts)

    return fb


def create_dct(
        n_mfcc: int,
        n_mels: int,
        norm: Optional[str]
) -> Tensor:
    r"""Create a DCT transformation matrix with shape (``n_mels``, ``n_mfcc``),
    normalized depending on norm.

    Args:
        n_mfcc (int): Number of mfc coefficients to retain
        n_mels (int): Number of mel filterbanks
        norm (str or None): Norm to use (either 'ortho' or None)

    Returns:
        Tensor: The transformation matrix, to be right-multiplied to
        row-wise data of size (``n_mels``, ``n_mfcc``).
    """
    # http://en.wikipedia.org/wiki/Discrete_cosine_transform#DCT-II
    n = torch.arange(float(n_mels))
    k = torch.arange(float(n_mfcc)).unsqueeze(1)
    dct = torch.cos(math.pi / float(n_mels) * (n + 0.5) * k)  # size (n_mfcc, n_mels)
    if norm is None:
        dct *= 2.0
    else:
        assert norm == "ortho"
        dct[0] *= 1.0 / math.sqrt(2.0)
        dct *= math.sqrt(2.0 / float(n_mels))
    return dct.t()


def mu_law_encoding(
        x: Tensor,
        quantization_channels: int
) -> Tensor:
    r"""Encode signal based on mu-law companding.  For more info see the
    `Wikipedia Entry <https://en.wikipedia.org/wiki/%CE%9C-law_algorithm>`_

    This algorithm assumes the signal has been scaled to between -1 and 1 and
    returns a signal encoded with values from 0 to quantization_channels - 1.

    Args:
        x (Tensor): Input tensor
        quantization_channels (int): Number of channels

    Returns:
        Tensor: Input after mu-law encoding
    """
    mu = quantization_channels - 1.0
    if not x.is_floating_point():
        x = x.to(torch.float)
    mu = torch.tensor(mu, dtype=x.dtype)
    x_mu = torch.sign(x) * torch.log1p(mu * torch.abs(x)) / torch.log1p(mu)
    x_mu = ((x_mu + 1) / 2 * mu + 0.5).to(torch.int64)
    return x_mu


def mu_law_decoding(
        x_mu: Tensor,
        quantization_channels: int
) -> Tensor:
    r"""Decode mu-law encoded signal.  For more info see the
    `Wikipedia Entry <https://en.wikipedia.org/wiki/%CE%9C-law_algorithm>`_

    This expects an input with values between 0 and quantization_channels - 1
    and returns a signal scaled between -1 and 1.

    Args:
        x_mu (Tensor): Input tensor
        quantization_channels (int): Number of channels

    Returns:
        Tensor: Input after mu-law decoding
    """
    mu = quantization_channels - 1.0
    if not x_mu.is_floating_point():
        x_mu = x_mu.to(torch.float)
    mu = torch.tensor(mu, dtype=x_mu.dtype)
    x = ((x_mu) / mu) * 2 - 1.0
    x = torch.sign(x) * (torch.exp(torch.abs(x) * torch.log1p(mu)) - 1.0) / mu
    return x


def phase_vocoder(
        complex_specgrams: Tensor,
        rate: float,
        phase_advance: Tensor
) -> Tensor:
    r"""Given a STFT tensor, speed up in time without modifying pitch by a
    factor of ``rate``.

    Args:
        complex_specgrams (Tensor):
            A tensor of dimension `(..., freq, num_frame)` with complex dtype.
        rate (float): Speed-up factor
        phase_advance (Tensor): Expected phase advance in each bin. Dimension of `(freq, 1)`

    Returns:
        Tensor:
            Stretched spectrogram. The resulting tensor is of the same dtype as the input
            spectrogram, but the number of frames is changed to ``ceil(num_frame / rate)``.

    Example
        >>> freq, hop_length = 1025, 512
        >>> # (channel, freq, time)
        >>> complex_specgrams = torch.randn(2, freq, 300, dtype=torch.cfloat)
        >>> rate = 1.3 # Speed up by 30%
        >>> phase_advance = torch.linspace(
        >>>    0, math.pi * hop_length, freq)[..., None]
        >>> x = phase_vocoder(complex_specgrams, rate, phase_advance)
        >>> x.shape # with 231 == ceil(300 / 1.3)
        torch.Size([2, 1025, 231])
    """
    if rate == 1.0:
        return complex_specgrams

    # pack batch
    shape = complex_specgrams.size()
    complex_specgrams = complex_specgrams.reshape([-1] + list(shape[-2:]))

    # Figures out the corresponding real dtype, i.e. complex128 -> float64, complex64 -> float32
    # Note torch.real is a view so it does not incur any memory copy.
    real_dtype = torch.real(complex_specgrams).dtype
    time_steps = torch.arange(
        0,
        complex_specgrams.size(-1),
        rate,
        device=complex_specgrams.device,
        dtype=real_dtype)

    alphas = time_steps % 1.0
    phase_0 = complex_specgrams[..., :1].angle()

    # Time Padding
    complex_specgrams = torch.nn.functional.pad(complex_specgrams, [0, 2])

    # (new_bins, freq, 2)
    complex_specgrams_0 = complex_specgrams.index_select(-1, time_steps.long())
    complex_specgrams_1 = complex_specgrams.index_select(-1, (time_steps + 1).long())

    angle_0 = complex_specgrams_0.angle()
    angle_1 = complex_specgrams_1.angle()

    norm_0 = complex_specgrams_0.abs()
    norm_1 = complex_specgrams_1.abs()

    phase = angle_1 - angle_0 - phase_advance
    phase = phase - 2 * math.pi * torch.round(phase / (2 * math.pi))

    # Compute Phase Accum
    phase = phase + phase_advance
    phase = torch.cat([phase_0, phase[..., :-1]], dim=-1)
    phase_acc = torch.cumsum(phase, -1)

    mag = alphas * norm_1 + (1 - alphas) * norm_0

    complex_specgrams_stretch = torch.polar(mag, phase_acc)

    # unpack batch
    complex_specgrams_stretch = complex_specgrams_stretch.reshape(shape[:-2] + complex_specgrams_stretch.shape[1:])
    return complex_specgrams_stretch


def mask_along_axis_iid(
        specgrams: Tensor,
        mask_param: int,
        mask_value: float,
        axis: int
) -> Tensor:
    r"""
    Apply a mask along ``axis``. Mask will be applied from indices ``[v_0, v_0 + v)``, where
    ``v`` is sampled from ``uniform(0, mask_param)``, and ``v_0`` from ``uniform(0, max_v - v)``.

    Args:
        specgrams (Tensor): Real spectrograms `(batch, channel, freq, time)`
        mask_param (int): Number of columns to be masked will be uniformly sampled from [0, mask_param]
        mask_value (float): Value to assign to the masked columns
        axis (int): Axis to apply masking on (2 -> frequency, 3 -> time)

    Returns:
        Tensor: Masked spectrograms of dimensions `(batch, channel, freq, time)`
    """

    if axis not in [2, 3]:
        raise ValueError('Only Frequency and Time masking are supported')

    device = specgrams.device
    dtype = specgrams.dtype

    value = torch.rand(specgrams.shape[:2], device=device, dtype=dtype) * mask_param
    min_value = torch.rand(specgrams.shape[:2], device=device, dtype=dtype) * (specgrams.size(axis) - value)

    # Create broadcastable mask
    mask_start = min_value[..., None, None]
    mask_end = (min_value + value)[..., None, None]
    mask = torch.arange(0, specgrams.size(axis), device=device, dtype=dtype)

    # Per batch example masking
    specgrams = specgrams.transpose(axis, -1)
    specgrams = specgrams.masked_fill((mask >= mask_start) & (mask < mask_end), mask_value)
    specgrams = specgrams.transpose(axis, -1)

    return specgrams


def mask_along_axis(
        specgram: Tensor,
        mask_param: int,
        mask_value: float,
        axis: int
) -> Tensor:
    r"""
    Apply a mask along ``axis``. Mask will be applied from indices ``[v_0, v_0 + v)``, where
    ``v`` is sampled from ``uniform(0, mask_param)``, and ``v_0`` from ``uniform(0, max_v - v)``.
    All examples will have the same mask interval.

    Args:
        specgram (Tensor): Real spectrogram `(channel, freq, time)`
        mask_param (int): Number of columns to be masked will be uniformly sampled from [0, mask_param]
        mask_value (float): Value to assign to the masked columns
        axis (int): Axis to apply masking on (1 -> frequency, 2 -> time)

    Returns:
        Tensor: Masked spectrogram of dimensions `(channel, freq, time)`
    """
    if axis not in [1, 2]:
        raise ValueError('Only Frequency and Time masking are supported')

    # pack batch
    shape = specgram.size()
    specgram = specgram.reshape([-1] + list(shape[-2:]))
    value = torch.rand(1) * mask_param
    min_value = torch.rand(1) * (specgram.size(axis) - value)

    mask_start = (min_value.long()).squeeze()
    mask_end = (min_value.long() + value.long()).squeeze()
    mask = torch.arange(0, specgram.shape[axis], device=specgram.device, dtype=specgram.dtype)
    mask = (mask >= mask_start) & (mask < mask_end)
    if axis == 1:
        mask = mask.unsqueeze(-1)

    assert mask_end - mask_start < mask_param

    specgram = specgram.masked_fill(mask, mask_value)

    # unpack batch
    specgram = specgram.reshape(shape[:-2] + specgram.shape[-2:])

    return specgram


def compute_deltas(
        specgram: Tensor,
        win_length: int = 5,
        mode: str = "replicate"
) -> Tensor:
    r"""Compute delta coefficients of a tensor, usually a spectrogram:

    .. math::
       d_t = \frac{\sum_{n=1}^{\text{N}} n (c_{t+n} - c_{t-n})}{2 \sum_{n=1}^{\text{N}} n^2}

    where :math:`d_t` is the deltas at time :math:`t`,
    :math:`c_t` is the spectrogram coeffcients at time :math:`t`,
    :math:`N` is ``(win_length-1)//2``.

    Args:
        specgram (Tensor): Tensor of audio of dimension `(..., freq, time)`
        win_length (int, optional): The window length used for computing delta (Default: ``5``)
        mode (str, optional): Mode parameter passed to padding (Default: ``"replicate"``)

    Returns:
        Tensor: Tensor of deltas of dimension `(..., freq, time)`

    Example
        >>> specgram = torch.randn(1, 40, 1000)
        >>> delta = compute_deltas(specgram)
        >>> delta2 = compute_deltas(delta)
    """
    device = specgram.device
    dtype = specgram.dtype

    # pack batch
    shape = specgram.size()
    specgram = specgram.reshape(1, -1, shape[-1])

    assert win_length >= 3

    n = (win_length - 1) // 2

    # twice sum of integer squared
    denom = n * (n + 1) * (2 * n + 1) / 3

    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)

    kernel = torch.arange(-n, n + 1, 1, device=device, dtype=dtype).repeat(specgram.shape[1], 1, 1)

    output = torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom

    # unpack batch
    output = output.reshape(shape)

    return output


def _compute_nccf(
        waveform: Tensor,
        sample_rate: int,
        frame_time: float,
        freq_low: int
) -> Tensor:
    r"""
    Compute Normalized Cross-Correlation Function (NCCF).

    .. math::
        \phi_i(m) = \frac{\sum_{n=b_i}^{b_i + N-1} w(n) w(m+n)}{\sqrt{E(b_i) E(m+b_i)}},

    where
    :math:`\phi_i(m)` is the NCCF at frame :math:`i` with lag :math:`m`,
    :math:`w` is the waveform,
    :math:`N` is the length of a frame,
    :math:`b_i` is the beginning of frame :math:`i`,
    :math:`E(j)` is the energy :math:`\sum_{n=j}^{j+N-1} w^2(n)`.
    """

    EPSILON = 10 ** (-9)

    # Number of lags to check
    lags = int(math.ceil(sample_rate / freq_low))

    frame_size = int(math.ceil(sample_rate * frame_time))

    waveform_length = waveform.size()[-1]
    num_of_frames = int(math.ceil(waveform_length / frame_size))

    p = lags + num_of_frames * frame_size - waveform_length
    waveform = torch.nn.functional.pad(waveform, (0, p))

    # Compute lags
    output_lag = []
    for lag in range(1, lags + 1):
        s1 = waveform[..., :-lag].unfold(-1, frame_size, frame_size)[..., :num_of_frames, :]
        s2 = waveform[..., lag:].unfold(-1, frame_size, frame_size)[..., :num_of_frames, :]

        output_frames = (
            (s1 * s2).sum(-1)
            / (EPSILON + torch.norm(s1, p=2, dim=-1)).pow(2)
            / (EPSILON + torch.norm(s2, p=2, dim=-1)).pow(2)
        )

        output_lag.append(output_frames.unsqueeze(-1))

    nccf = torch.cat(output_lag, -1)

    return nccf


def _combine_max(
        a: Tuple[Tensor, Tensor],
        b: Tuple[Tensor, Tensor],
        thresh: float = 0.99
) -> Tuple[Tensor, Tensor]:
    """
    Take value from first if bigger than a multiplicative factor of the second, elementwise.
    """
    mask = (a[0] > thresh * b[0])
    values = mask * a[0] + ~mask * b[0]
    indices = mask * a[1] + ~mask * b[1]
    return values, indices


def _find_max_per_frame(
        nccf: Tensor,
        sample_rate: int,
        freq_high: int
) -> Tensor:
    r"""
    For each frame, take the highest value of NCCF,
    apply centered median smoothing, and convert to frequency.

    Note: If the max among all the lags is very close
    to the first half of lags, then the latter is taken.
    """

    lag_min = int(math.ceil(sample_rate / freq_high))

    # Find near enough max that is smallest

    best = torch.max(nccf[..., lag_min:], -1)

    half_size = nccf.shape[-1] // 2
    half = torch.max(nccf[..., lag_min:half_size], -1)

    best = _combine_max(half, best)
    indices = best[1]

    # Add back minimal lag
    indices += lag_min
    # Add 1 empirical calibration offset
    indices += 1

    return indices


def _median_smoothing(
        indices: Tensor,
        win_length: int
) -> Tensor:
    r"""
    Apply median smoothing to the 1D tensor over the given window.
    """

    # Centered windowed
    pad_length = (win_length - 1) // 2

    # "replicate" padding in any dimension
    indices = torch.nn.functional.pad(
        indices, (pad_length, 0), mode="constant", value=0.
    )

    indices[..., :pad_length] = torch.cat(pad_length * [indices[..., pad_length].unsqueeze(-1)], dim=-1)
    roll = indices.unfold(-1, win_length, 1)

    values, _ = torch.median(roll, -1)
    return values


def detect_pitch_frequency(
        waveform: Tensor,
        sample_rate: int,
        frame_time: float = 10 ** (-2),
        win_length: int = 30,
        freq_low: int = 85,
        freq_high: int = 3400,
) -> Tensor:
    r"""Detect pitch frequency.

    It is implemented using normalized cross-correlation function and median smoothing.

    Args:
        waveform (Tensor): Tensor of audio of dimension `(..., freq, time)`
        sample_rate (int): The sample rate of the waveform (Hz)
        frame_time (float, optional): Duration of a frame (Default: ``10 ** (-2)``).
        win_length (int, optional): The window length for median smoothing (in number of frames) (Default: ``30``).
        freq_low (int, optional): Lowest frequency that can be detected (Hz) (Default: ``85``).
        freq_high (int, optional): Highest frequency that can be detected (Hz) (Default: ``3400``).

    Returns:
        Tensor: Tensor of freq of dimension `(..., frame)`
    """
    # pack batch
    shape = list(waveform.size())
    waveform = waveform.reshape([-1] + shape[-1:])

    nccf = _compute_nccf(waveform, sample_rate, frame_time, freq_low)
    indices = _find_max_per_frame(nccf, sample_rate, freq_high)
    indices = _median_smoothing(indices, win_length)

    # Convert indices to frequency
    EPSILON = 10 ** (-9)
    freq = sample_rate / (EPSILON + indices.to(torch.float))

    # unpack batch
    freq = freq.reshape(shape[:-1] + list(freq.shape[-1:]))

    return freq


def sliding_window_cmn(
    specgram: Tensor,
    cmn_window: int = 600,
    min_cmn_window: int = 100,
    center: bool = False,
    norm_vars: bool = False,
) -> Tensor:
    r"""
    Apply sliding-window cepstral mean (and optionally variance) normalization per utterance.

    Args:
        specgram (Tensor): Tensor of spectrogram of dimension `(..., time, freq)`
        cmn_window (int, optional): Window in frames for running average CMN computation (int, default = 600)
        min_cmn_window (int, optional):  Minimum CMN window used at start of decoding (adds latency only at start).
            Only applicable if center == false, ignored if center==true (int, default = 100)
        center (bool, optional): If true, use a window centered on the current frame
            (to the extent possible, modulo end effects). If false, window is to the left. (bool, default = false)
        norm_vars (bool, optional): If true, normalize variance to one. (bool, default = false)

    Returns:
        Tensor: Tensor matching input shape `(..., freq, time)`
    """
    input_shape = specgram.shape
    num_frames, num_feats = input_shape[-2:]
    specgram = specgram.view(-1, num_frames, num_feats)
    num_channels = specgram.shape[0]

    dtype = specgram.dtype
    device = specgram.device
    last_window_start = last_window_end = -1
    cur_sum = torch.zeros(num_channels, num_feats, dtype=dtype, device=device)
    cur_sumsq = torch.zeros(num_channels, num_feats, dtype=dtype, device=device)
    cmn_specgram = torch.zeros(
        num_channels, num_frames, num_feats, dtype=dtype, device=device)
    for t in range(num_frames):
        window_start = 0
        window_end = 0
        if center:
            window_start = t - cmn_window // 2
            window_end = window_start + cmn_window
        else:
            window_start = t - cmn_window
            window_end = t + 1
        if window_start < 0:
            window_end -= window_start
            window_start = 0
        if not center:
            if window_end > t:
                window_end = max(t + 1, min_cmn_window)
        if window_end > num_frames:
            window_start -= (window_end - num_frames)
            window_end = num_frames
            if window_start < 0:
                window_start = 0
        if last_window_start == -1:
            input_part = specgram[:, window_start: window_end - window_start, :]
            cur_sum += torch.sum(input_part, 1)
            if norm_vars:
                cur_sumsq += torch.cumsum(input_part ** 2, 1)[:, -1, :]
        else:
            if window_start > last_window_start:
                frame_to_remove = specgram[:, last_window_start, :]
                cur_sum -= frame_to_remove
                if norm_vars:
                    cur_sumsq -= (frame_to_remove ** 2)
            if window_end > last_window_end:
                frame_to_add = specgram[:, last_window_end, :]
                cur_sum += frame_to_add
                if norm_vars:
                    cur_sumsq += (frame_to_add ** 2)
        window_frames = window_end - window_start
        last_window_start = window_start
        last_window_end = window_end
        cmn_specgram[:, t, :] = specgram[:, t, :] - cur_sum / window_frames
        if norm_vars:
            if window_frames == 1:
                cmn_specgram[:, t, :] = torch.zeros(
                    num_channels, num_feats, dtype=dtype, device=device)
            else:
                variance = cur_sumsq
                variance = variance / window_frames
                variance -= ((cur_sum ** 2) / (window_frames ** 2))
                variance = torch.pow(variance, -0.5)
                cmn_specgram[:, t, :] *= variance

    cmn_specgram = cmn_specgram.view(input_shape[:-2] + (num_frames, num_feats))
    if len(input_shape) == 2:
        cmn_specgram = cmn_specgram.squeeze(0)
    return cmn_specgram


def spectral_centroid(
        waveform: Tensor,
        sample_rate: int,
        pad: int,
        window: Tensor,
        n_fft: int,
        hop_length: int,
        win_length: int,
) -> Tensor:
    r"""
    Compute the spectral centroid for each channel along the time axis.

    The spectral centroid is defined as the weighted average of the
    frequency values, weighted by their magnitude.

    Args:
        waveform (Tensor): Tensor of audio of dimension `(..., time)`
        sample_rate (int): Sample rate of the audio waveform
        pad (int): Two sided padding of signal
        window (Tensor): Window tensor that is applied/multiplied to each frame/window
        n_fft (int): Size of FFT
        hop_length (int): Length of hop between STFT windows
        win_length (int): Window size

    Returns:
        Tensor: Dimension `(..., time)`
    """
    specgram = spectrogram(waveform, pad=pad, window=window, n_fft=n_fft, hop_length=hop_length,
                           win_length=win_length, power=1., normalized=False)
    freqs = torch.linspace(0, sample_rate // 2, steps=1 + n_fft // 2,
                           device=specgram.device).reshape((-1, 1))
    freq_dim = -2
    return (freqs * specgram).sum(dim=freq_dim) / specgram.sum(dim=freq_dim)


@_mod_utils.requires_sox()
def apply_codec(
    waveform: Tensor,
    sample_rate: int,
    format: str,
    channels_first: bool = True,
    compression: Optional[float] = None,
    encoding: Optional[str] = None,
    bits_per_sample: Optional[int] = None,
) -> Tensor:
    r"""
    Apply codecs as a form of augmentation.

    Args:
        waveform (Tensor): Audio data. Must be 2 dimensional. See also ```channels_first```.
        sample_rate (int): Sample rate of the audio waveform.
        format (str): File format.
        channels_first (bool, optional):
            When True, both the input and output Tensor have dimension `(channel, time)`.
            Otherwise, they have dimension `(time, channel)`.
        compression (float or None, optional): Used for formats other than WAV.
            For more details see :py:func:`torchaudio.backend.sox_io_backend.save`.
        encoding (str or None, optional): Changes the encoding for the supported formats.
            For more details see :py:func:`torchaudio.backend.sox_io_backend.save`.
        bits_per_sample (int or None, optional): Changes the bit depth for the supported formats.
            For more details see :py:func:`torchaudio.backend.sox_io_backend.save`.

    Returns:
        Tensor: Resulting Tensor.
        If ``channels_first=True``, it has `(channel, time)` else `(time, channel)`.
    """
    bytes = io.BytesIO()
    torchaudio.backend.sox_io_backend.save(bytes,
                                           waveform,
                                           sample_rate,
                                           channels_first,
                                           compression,
                                           format,
                                           encoding,
                                           bits_per_sample
                                           )
    bytes.seek(0)
    augmented, _ = torchaudio.sox_effects.sox_effects.apply_effects_file(
        bytes, effects=[["rate", f"{sample_rate}"]], channels_first=channels_first, format=format)
    return augmented


@_mod_utils.requires_kaldi()
def compute_kaldi_pitch(
        waveform: torch.Tensor,
        sample_rate: float,
        frame_length: float = 25.0,
        frame_shift: float = 10.0,
        min_f0: float = 50,
        max_f0: float = 400,
        soft_min_f0: float = 10.0,
        penalty_factor: float = 0.1,
        lowpass_cutoff: float = 1000,
        resample_frequency: float = 4000,
        delta_pitch: float = 0.005,
        nccf_ballast: float = 7000,
        lowpass_filter_width: int = 1,
        upsample_filter_width: int = 5,
        max_frames_latency: int = 0,
        frames_per_chunk: int = 0,
        simulate_first_pass_online: bool = False,
        recompute_frame: int = 500,
        snip_edges: bool = True,
) -> torch.Tensor:
    """Extract pitch based on method described in *A pitch extraction algorithm tuned
    for automatic speech recognition* [:footcite:`6854049`].

    This function computes the equivalent of `compute-kaldi-pitch-feats` from Kaldi.

    Args:
        waveform (Tensor):
            The input waveform of shape `(..., time)`.
        sample_rate (float):
            Sample rate of `waveform`.
        frame_length (float, optional):
            Frame length in milliseconds. (default: 25.0)
        frame_shift (float, optional):
            Frame shift in milliseconds. (default: 10.0)
        min_f0 (float, optional):
            Minimum F0 to search for (Hz)  (default: 50.0)
        max_f0 (float, optional):
            Maximum F0 to search for (Hz)  (default: 400.0)
        soft_min_f0 (float, optional):
            Minimum f0, applied in soft way, must not exceed min-f0  (default: 10.0)
        penalty_factor (float, optional):
            Cost factor for FO change.  (default: 0.1)
        lowpass_cutoff (float, optional):
            Cutoff frequency for LowPass filter (Hz) (default: 1000)
        resample_frequency (float, optional):
            Frequency that we down-sample the signal to. Must be more than twice lowpass-cutoff.
            (default: 4000)
        delta_pitch( float, optional):
            Smallest relative change in pitch that our algorithm measures. (default: 0.005)
        nccf_ballast (float, optional):
            Increasing this factor reduces NCCF for quiet frames (default: 7000)
        lowpass_filter_width (int, optional):
            Integer that determines filter width of lowpass filter, more gives sharper filter.
            (default: 1)
        upsample_filter_width (int, optional):
            Integer that determines filter width when upsampling NCCF. (default: 5)
        max_frames_latency (int, optional):
            Maximum number of frames of latency that we allow pitch tracking to introduce into
            the feature processing (affects output only if ``frames_per_chunk > 0`` and
            ``simulate_first_pass_online=True``) (default: 0)
        frames_per_chunk (int, optional):
            The number of frames used for energy normalization. (default: 0)
        simulate_first_pass_online (bool, optional):
            If true, the function will output features that correspond to what an online decoder
            would see in the first pass of decoding -- not the final version of the features,
            which is the default. (default: False)
            Relevant if ``frames_per_chunk > 0``.
        recompute_frame (int, optional):
            Only relevant for compatibility with online pitch extraction.
            A non-critical parameter; the frame at which we recompute some of the forward pointers,
            after revising our estimate of the signal energy.
            Relevant if ``frames_per_chunk > 0``. (default: 500)
        snip_edges (bool, optional):
            If this is set to false, the incomplete frames near the ending edge won't be snipped,
            so that the number of frames is the file size divided by the frame-shift.
            This makes different types of features give the same number of frames. (default: True)

    Returns:
       Tensor: Pitch feature. Shape: `(batch, frames 2)` where the last dimension
       corresponds to pitch and NCCF.
    """
    shape = waveform.shape
    waveform = waveform.reshape(-1, shape[-1])
    result = torch.ops.torchaudio.kaldi_ComputeKaldiPitch(
        waveform, sample_rate, frame_length, frame_shift,
        min_f0, max_f0, soft_min_f0, penalty_factor, lowpass_cutoff,
        resample_frequency, delta_pitch, nccf_ballast,
        lowpass_filter_width, upsample_filter_width, max_frames_latency,
        frames_per_chunk, simulate_first_pass_online, recompute_frame,
        snip_edges,
    )
    result = result.reshape(shape[:-1] + result.shape[-2:])
    return result


def _get_sinc_resample_kernel(
        orig_freq: int,
        new_freq: int,
        gcd: int,
        lowpass_filter_width: int,
        rolloff: float,
        resampling_method: str,
        beta: Optional[float],
        device: torch.device = torch.device("cpu"),
        dtype: Optional[torch.dtype] = None):

    if not (int(orig_freq) == orig_freq and int(new_freq) == new_freq):
        raise Exception(
            "Frequencies must be of integer type to ensure quality resampling computation. "
            "To work around this, manually convert both frequencies to integer values "
            "that maintain their resampling rate ratio before passing them into the function. "
            "Example: To downsample a 44100 hz waveform by a factor of 8, use "
            "`orig_freq=8` and `new_freq=1` instead of `orig_freq=44100` and `new_freq=5512.5`. "
            "For more information, please refer to https://github.com/pytorch/audio/issues/1487."
        )

    if resampling_method not in ['sinc_interpolation', 'kaiser_window']:
        raise ValueError('Invalid resampling method: {}'.format(resampling_method))

    orig_freq = int(orig_freq) // gcd
    new_freq = int(new_freq) // gcd

    assert lowpass_filter_width > 0
    kernels = []
    base_freq = min(orig_freq, new_freq)
    # This will perform antialiasing filtering by removing the highest frequencies.
    # At first I thought I only needed this when downsampling, but when upsampling
    # you will get edge artifacts without this, as the edge is equivalent to zero padding,
    # which will add high freq artifacts.
    base_freq *= rolloff

    # The key idea of the algorithm is that x(t) can be exactly reconstructed from x[i] (tensor)
    # using the sinc interpolation formula:
    #   x(t) = sum_i x[i] sinc(pi * orig_freq * (i / orig_freq - t))
    # We can then sample the function x(t) with a different sample rate:
    #    y[j] = x(j / new_freq)
    # or,
    #    y[j] = sum_i x[i] sinc(pi * orig_freq * (i / orig_freq - j / new_freq))

    # We see here that y[j] is the convolution of x[i] with a specific filter, for which
    # we take an FIR approximation, stopping when we see at least `lowpass_filter_width` zeros crossing.
    # But y[j+1] is going to have a different set of weights and so on, until y[j + new_freq].
    # Indeed:
    # y[j + new_freq] = sum_i x[i] sinc(pi * orig_freq * ((i / orig_freq - (j + new_freq) / new_freq))
    #                 = sum_i x[i] sinc(pi * orig_freq * ((i - orig_freq) / orig_freq - j / new_freq))
    #                 = sum_i x[i + orig_freq] sinc(pi * orig_freq * (i / orig_freq - j / new_freq))
    # so y[j+new_freq] uses the same filter as y[j], but on a shifted version of x by `orig_freq`.
    # This will explain the F.conv1d after, with a stride of orig_freq.
    width = math.ceil(lowpass_filter_width * orig_freq / base_freq)
    # If orig_freq is still big after GCD reduction, most filters will be very unbalanced, i.e.,
    # they will have a lot of almost zero values to the left or to the right...
    # There is probably a way to evaluate those filters more efficiently, but this is kept for
    # future work.
    idx_dtype = dtype if dtype is not None else torch.float64
    idx = torch.arange(-width, width + orig_freq, device=device, dtype=idx_dtype)

    for i in range(new_freq):
        t = (-i / new_freq + idx / orig_freq) * base_freq
        t = t.clamp_(-lowpass_filter_width, lowpass_filter_width)

        # we do not use built in torch windows here as we need to evaluate the window
        # at specific positions, not over a regular grid.
        if resampling_method == "sinc_interpolation":
            window = torch.cos(t * math.pi / lowpass_filter_width / 2)**2
        else:
            # kaiser_window
            if beta is None:
                beta = 14.769656459379492
            beta_tensor = torch.tensor(float(beta))
            window = torch.i0(beta_tensor * torch.sqrt(1 - (t / lowpass_filter_width) ** 2)) / torch.i0(beta_tensor)
        t *= math.pi
        kernel = torch.where(t == 0, torch.tensor(1.).to(t), torch.sin(t) / t)
        kernel.mul_(window)
        kernels.append(kernel)

    scale = base_freq / orig_freq
    kernels = torch.stack(kernels).view(new_freq, 1, -1).mul_(scale)
    if dtype is None:
        kernels = kernels.to(dtype=torch.float32)
    return kernels, width


def _apply_sinc_resample_kernel(
        waveform: Tensor,
        orig_freq: int,
        new_freq: int,
        gcd: int,
        kernel: Tensor,
        width: int,
):
    orig_freq = int(orig_freq) // gcd
    new_freq = int(new_freq) // gcd

    # pack batch
    shape = waveform.size()
    waveform = waveform.view(-1, shape[-1])

    num_wavs, length = waveform.shape
    waveform = torch.nn.functional.pad(waveform, (width, width + orig_freq))
    resampled = torch.nn.functional.conv1d(waveform[:, None], kernel, stride=orig_freq)
    resampled = resampled.transpose(1, 2).reshape(num_wavs, -1)
    target_length = int(math.ceil(new_freq * length / orig_freq))
    resampled = resampled[..., :target_length]

    # unpack batch
    resampled = resampled.view(shape[:-1] + resampled.shape[-1:])
    return resampled


def resample(
        waveform: Tensor,
        orig_freq: int,
        new_freq: int,
        lowpass_filter_width: int = 6,
        rolloff: float = 0.99,
        resampling_method: str = "sinc_interpolation",
        beta: Optional[float] = None,
) -> Tensor:
    r"""Resamples the waveform at the new frequency using bandlimited interpolation.

    https://ccrma.stanford.edu/~jos/resample/Theory_Ideal_Bandlimited_Interpolation.html

    Note:
        ``transforms.Resample`` precomputes and reuses the resampling kernel, so using it will result in
        more efficient computation if resampling multiple waveforms with the same resampling parameters.

    Args:
        waveform (Tensor): The input signal of dimension `(..., time)`
        orig_freq (int): The original frequency of the signal
        new_freq (int): The desired frequency
        lowpass_filter_width (int, optional): Controls the sharpness of the filter, more == sharper
            but less efficient. (Default: ``6``)
        rolloff (float, optional): The roll-off frequency of the filter, as a fraction of the Nyquist.
            Lower values reduce anti-aliasing, but also reduce some of the highest frequencies. (Default: ``0.99``)
        resampling_method (str, optional): The resampling method to use.
            Options: [``sinc_interpolation``, ``kaiser_window``] (Default: ``'sinc_interpolation'``)
        beta (float or None, optional): The shape parameter used for kaiser window.

    Returns:
        Tensor: The waveform at the new frequency of dimension `(..., time).`
    """

    assert orig_freq > 0.0 and new_freq > 0.0

    if orig_freq == new_freq:
        return waveform

    gcd = math.gcd(int(orig_freq), int(new_freq))

    kernel, width = _get_sinc_resample_kernel(orig_freq, new_freq, gcd, lowpass_filter_width, rolloff,
                                              resampling_method, beta, waveform.device, waveform.dtype)
    resampled = _apply_sinc_resample_kernel(waveform, orig_freq, new_freq, gcd, kernel, width)
    return resampled


@torch.jit.unused
def edit_distance(seq1: Sequence, seq2: Sequence) -> int:
    """
    Calculate the word level edit (Levenshtein) distance between two sequences.

    The function computes an edit distance allowing deletion, insertion and
    substitution. The result is an integer.

    For most applications, the two input sequences should be the same type. If
    two strings are given, the output is the edit distance between the two
    strings (character edit distance). If two lists of strings are given, the
    output is the edit distance between sentences (word edit distance). Users
    may want to normalize the output by the length of the reference sequence.

    torchscipt is not supported for this function.

    Args:
        seq1 (Sequence): the first sequence to compare.
        seq2 (Sequence): the second sequence to compare.
    Returns:
        int: The distance between the first and second sequences.
    """
    len_sent2 = len(seq2)
    dold = list(range(len_sent2 + 1))
    dnew = [0 for _ in range(len_sent2 + 1)]

    for i in range(1, len(seq1) + 1):
        dnew[0] = i
        for j in range(1, len_sent2 + 1):
            if seq1[i - 1] == seq2[j - 1]:
                dnew[j] = dold[j - 1]
            else:
                substitution = dold[j - 1] + 1
                insertion = dnew[j - 1] + 1
                deletion = dold[j] + 1
                dnew[j] = min(substitution, insertion, deletion)

        dnew, dold = dold, dnew

    return int(dold[-1])


def pitch_shift(
    waveform: Tensor,
    sample_rate: int,
    n_steps: int,
    bins_per_octave: int = 12,
    n_fft: int = 512,
    win_length: Optional[int] = None,
    hop_length: Optional[int] = None,
    window: Optional[Tensor] = None,
) -> Tensor:
    """
    Shift the pitch of a waveform by ``n_steps`` steps.

    Args:
        waveform (Tensor): The input waveform of shape `(..., time)`.
        sample_rate (int): Sample rate of `waveform`.
        n_steps (int): The (fractional) steps to shift `waveform`.
        bins_per_octave (int, optional): The number of steps per octave (Default: ``12``).
        n_fft (int, optional): Size of FFT, creates ``n_fft // 2 + 1`` bins (Default: ``512``).
        win_length (int or None, optional): Window size. If None, then ``n_fft`` is used. (Default: ``None``).
        hop_length (int or None, optional): Length of hop between STFT windows. If None, then
            ``win_length // 4`` is used (Default: ``None``).
        window (Tensor or None, optional): Window tensor that is applied/multiplied to each frame/window.
            If None, then ``torch.hann_window(win_length)`` is used (Default: ``None``).


    Returns:
        Tensor: The pitch-shifted audio waveform of shape `(..., time)`.
    """
    if hop_length is None:
        hop_length = n_fft // 4
    if win_length is None:
        win_length = n_fft
    if window is None:
        window = torch.hann_window(window_length=win_length, device=waveform.device)

    # pack batch
    shape = waveform.size()
    waveform = waveform.reshape(-1, shape[-1])

    ori_len = shape[-1]
    rate = 2.0 ** (-float(n_steps) / bins_per_octave)
    spec_f = torch.stft(input=waveform,
                        n_fft=n_fft,
                        hop_length=hop_length,
                        win_length=win_length,
                        window=window,
                        center=True,
                        pad_mode='reflect',
                        normalized=False,
                        onesided=True,
                        return_complex=True)
    phase_advance = torch.linspace(0, math.pi * hop_length, spec_f.shape[-2], device=spec_f.device)[..., None]
    spec_stretch = phase_vocoder(spec_f, rate, phase_advance)
    len_stretch = int(round(ori_len / rate))
    waveform_stretch = torch.istft(spec_stretch,
                                   n_fft=n_fft,
                                   hop_length=hop_length,
                                   win_length=win_length,
                                   window=window,
                                   length=len_stretch)
    waveform_shift = resample(waveform_stretch, int(sample_rate / rate), sample_rate)
    shift_len = waveform_shift.size()[-1]
    if shift_len > ori_len:
        waveform_shift = waveform_shift[..., :ori_len]
    else:
        waveform_shift = torch.nn.functional.pad(waveform_shift, [0, ori_len - shift_len])

    # unpack batch
    waveform_shift = waveform_shift.view(shape[:-1] + waveform_shift.shape[-1:])
    return waveform_shift


def rnnt_loss(
    logits: Tensor,
    targets: Tensor,
    logit_lengths: Tensor,
    target_lengths: Tensor,
    blank: int = -1,
    clamp: float = -1,
    reduction: str = "mean",
):
    """Compute the RNN Transducer loss from *Sequence Transduction with Recurrent Neural Networks*
    [:footcite:`graves2012sequence`].
    The RNN Transducer loss extends the CTC loss by defining a distribution over output
    sequences of all lengths, and by jointly modelling both input-output and output-output
    dependencies.

    Args:
        logits (Tensor): Tensor of dimension `(batch, max seq length, max target length + 1, class)`
            containing output from joiner
        targets (Tensor): Tensor of dimension `(batch, max target length)` containing targets with zero padded
        logit_lengths (Tensor): Tensor of dimension `(batch)` containing lengths of each sequence from encoder
        target_lengths (Tensor): Tensor of dimension `(batch)` containing lengths of targets for each sequence
        blank (int, optional): blank label (Default: ``-1``)
        clamp (float, optional): clamp for gradients (Default: ``-1``)
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. (Default: ``'mean'``)
    Returns:
        Tensor: Loss with the reduction option applied. If ``reduction`` is  ``'none'``, then size `(batch)`,
        otherwise scalar.
    """
    if reduction not in ['none', 'mean', 'sum']:
        raise ValueError("reduction should be one of 'none', 'mean', or 'sum'")

    if blank < 0:  # reinterpret blank index if blank < 0.
        blank = logits.shape[-1] + blank

    costs, _ = torch.ops.torchaudio.rnnt_loss(
        logits=logits,
        targets=targets,
        logit_lengths=logit_lengths,
        target_lengths=target_lengths,
        blank=blank,
        clamp=clamp,
    )

    if reduction == 'mean':
        return costs.mean()
    elif reduction == 'sum':
        return costs.sum()

    return costs

# PROJECT: pytorch_audio FILE: torchaudio/_extension.py
import os
import warnings
from pathlib import Path

import torch
from torchaudio._internal import module_utils as _mod_utils  # noqa: F401


def _init_extension():
    if not _mod_utils.is_module_available('torchaudio._torchaudio'):
        warnings.warn('torchaudio C++ extension is not available.')
        return

    suffix = 'pyd' if os.name == 'nt' else 'so'
    path = Path(__file__).parent / 'lib' / f'libtorchaudio.{suffix}'
    # In case `torchaudio` is deployed with `pex` format, this file does not exist.
    # In this case, we expect that `libtorchaudio` is available somewhere
    # in the search path of dynamic loading mechanism, and importing `_torchaudio`,
    # which depends on `libtorchaudio` and dynamic loader will handle it for us.
    if path.exists():
        torch.ops.load_library(path)
        torch.classes.load_library(path)
    # This import is for initializing the methods registered via PyBind11
    from torchaudio import _torchaudio  # noqa


_init_extension()

# PROJECT: pytorch_audio FILE: .circleci/unittest/linux/scripts/run_clang_format.py
#!/usr/bin/env python
"""A wrapper script around clang-format, suitable for linting multiple files
and to use for continuous integration.

This is an alternative API for the clang-format command line.
It runs over multiple files and directories in parallel.
A diff output is produced and a sensible exit code is returned.

"""

import argparse
import codecs
import difflib
import fnmatch
import io
import multiprocessing
import os
import signal
import subprocess
import sys
import traceback

from functools import partial

try:
    from subprocess import DEVNULL  # py3k
except ImportError:
    DEVNULL = open(os.devnull, "wb")


DEFAULT_EXTENSIONS = 'c,h,C,H,cpp,hpp,cc,hh,c++,h++,cxx,hxx,cu'


class ExitStatus:
    SUCCESS = 0
    DIFF = 1
    TROUBLE = 2


def list_files(files, recursive=False, extensions=None, exclude=None):
    if extensions is None:
        extensions = []
    if exclude is None:
        exclude = []

    out = []
    for file in files:
        if recursive and os.path.isdir(file):
            for dirpath, dnames, fnames in os.walk(file):
                fpaths = [os.path.join(dirpath, fname) for fname in fnames]
                for pattern in exclude:
                    # os.walk() supports trimming down the dnames list
                    # by modifying it in-place,
                    # to avoid unnecessary directory listings.
                    dnames[:] = [
                        x for x in dnames
                        if
                        not fnmatch.fnmatch(os.path.join(dirpath, x), pattern)
                    ]
                    fpaths = [
                        x for x in fpaths if not fnmatch.fnmatch(x, pattern)
                    ]
                for f in fpaths:
                    ext = os.path.splitext(f)[1][1:]
                    if ext in extensions:
                        out.append(f)
        else:
            out.append(file)
    return out


def make_diff(file, original, reformatted):
    return list(
        difflib.unified_diff(
            original,
            reformatted,
            fromfile='{}\t(original)'.format(file),
            tofile='{}\t(reformatted)'.format(file),
            n=3))


class DiffError(Exception):
    def __init__(self, message, errs=None):
        super(DiffError, self).__init__(message)
        self.errs = errs or []


class UnexpectedError(Exception):
    def __init__(self, message, exc=None):
        super(UnexpectedError, self).__init__(message)
        self.formatted_traceback = traceback.format_exc()
        self.exc = exc


def run_clang_format_diff_wrapper(args, file):
    try:
        ret = run_clang_format_diff(args, file)
        return ret
    except DiffError:
        raise
    except Exception as e:
        raise UnexpectedError('{}: {}: {}'.format(file, e.__class__.__name__,
                                                  e), e)


def run_clang_format_diff(args, file):
    try:
        with io.open(file, 'r', encoding='utf-8') as f:
            original = f.readlines()
    except IOError as exc:
        raise DiffError(str(exc))
    invocation = [args.clang_format_executable, file]

    # Use of utf-8 to decode the process output.
    #
    # Hopefully, this is the correct thing to do.
    #
    # It's done due to the following assumptions (which may be incorrect):
    # - clang-format will returns the bytes read from the files as-is,
    #   without conversion, and it is already assumed that the files use utf-8.
    # - if the diagnostics were internationalized, they would use utf-8:
    #   > Adding Translations to Clang
    #   >
    #   > Not possible yet!
    #   > Diagnostic strings should be written in UTF-8,
    #   > the client can translate to the relevant code page if needed.
    #   > Each translation completely replaces the format string
    #   > for the diagnostic.
    #   > -- http://clang.llvm.org/docs/InternalsManual.html#internals-diag-translation

    try:
        proc = subprocess.Popen(
            invocation,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True,
            encoding='utf-8')
    except OSError as exc:
        raise DiffError(
            "Command '{}' failed to start: {}".format(
                subprocess.list2cmdline(invocation), exc
            )
        )
    proc_stdout = proc.stdout
    proc_stderr = proc.stderr

    # hopefully the stderr pipe won't get full and block the process
    outs = list(proc_stdout.readlines())
    errs = list(proc_stderr.readlines())
    proc.wait()
    if proc.returncode:
        raise DiffError(
            "Command '{}' returned non-zero exit status {}".format(
                subprocess.list2cmdline(invocation), proc.returncode
            ),
            errs,
        )
    return make_diff(file, original, outs), errs


def bold_red(s):
    return '\x1b[1m\x1b[31m' + s + '\x1b[0m'


def colorize(diff_lines):
    def bold(s):
        return '\x1b[1m' + s + '\x1b[0m'

    def cyan(s):
        return '\x1b[36m' + s + '\x1b[0m'

    def green(s):
        return '\x1b[32m' + s + '\x1b[0m'

    def red(s):
        return '\x1b[31m' + s + '\x1b[0m'

    for line in diff_lines:
        if line[:4] in ['--- ', '+++ ']:
            yield bold(line)
        elif line.startswith('@@ '):
            yield cyan(line)
        elif line.startswith('+'):
            yield green(line)
        elif line.startswith('-'):
            yield red(line)
        else:
            yield line


def print_diff(diff_lines, use_color):
    if use_color:
        diff_lines = colorize(diff_lines)
    sys.stdout.writelines(diff_lines)


def print_trouble(prog, message, use_colors):
    error_text = 'error:'
    if use_colors:
        error_text = bold_red(error_text)
    print("{}: {} {}".format(prog, error_text, message), file=sys.stderr)


def main():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        '--clang-format-executable',
        metavar='EXECUTABLE',
        help='path to the clang-format executable',
        default='clang-format')
    parser.add_argument(
        '--extensions',
        help='comma separated list of file extensions (default: {})'.format(
            DEFAULT_EXTENSIONS),
        default=DEFAULT_EXTENSIONS)
    parser.add_argument(
        '-r',
        '--recursive',
        action='store_true',
        help='run recursively over directories')
    parser.add_argument('files', metavar='file', nargs='+')
    parser.add_argument(
        '-q',
        '--quiet',
        action='store_true')
    parser.add_argument(
        '-j',
        metavar='N',
        type=int,
        default=0,
        help='run N clang-format jobs in parallel'
        ' (default number of cpus + 1)')
    parser.add_argument(
        '--color',
        default='auto',
        choices=['auto', 'always', 'never'],
        help='show colored diff (default: auto)')
    parser.add_argument(
        '-e',
        '--exclude',
        metavar='PATTERN',
        action='append',
        default=[],
        help='exclude paths matching the given glob-like pattern(s)'
        ' from recursive search')

    args = parser.parse_args()

    # use default signal handling, like diff return SIGINT value on ^C
    # https://bugs.python.org/issue14229#msg156446
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    try:
        signal.SIGPIPE
    except AttributeError:
        # compatibility, SIGPIPE does not exist on Windows
        pass
    else:
        signal.signal(signal.SIGPIPE, signal.SIG_DFL)

    colored_stdout = False
    colored_stderr = False
    if args.color == 'always':
        colored_stdout = True
        colored_stderr = True
    elif args.color == 'auto':
        colored_stdout = sys.stdout.isatty()
        colored_stderr = sys.stderr.isatty()

    version_invocation = [args.clang_format_executable, str("--version")]
    try:
        subprocess.check_call(version_invocation, stdout=DEVNULL)
    except subprocess.CalledProcessError as e:
        print_trouble(parser.prog, str(e), use_colors=colored_stderr)
        return ExitStatus.TROUBLE
    except OSError as e:
        print_trouble(
            parser.prog,
            "Command '{}' failed to start: {}".format(
                subprocess.list2cmdline(version_invocation), e
            ),
            use_colors=colored_stderr,
        )
        return ExitStatus.TROUBLE

    retcode = ExitStatus.SUCCESS
    files = list_files(
        args.files,
        recursive=args.recursive,
        exclude=args.exclude,
        extensions=args.extensions.split(','))

    if not files:
        return

    njobs = args.j
    if njobs == 0:
        njobs = multiprocessing.cpu_count() + 1
    njobs = min(len(files), njobs)

    if njobs == 1:
        # execute directly instead of in a pool,
        # less overhead, simpler stacktraces
        it = (run_clang_format_diff_wrapper(args, file) for file in files)
        pool = None
    else:
        pool = multiprocessing.Pool(njobs)
        it = pool.imap_unordered(
            partial(run_clang_format_diff_wrapper, args), files)
    while True:
        try:
            outs, errs = next(it)
        except StopIteration:
            break
        except DiffError as e:
            print_trouble(parser.prog, str(e), use_colors=colored_stderr)
            retcode = ExitStatus.TROUBLE
            sys.stderr.writelines(e.errs)
        except UnexpectedError as e:
            print_trouble(parser.prog, str(e), use_colors=colored_stderr)
            sys.stderr.write(e.formatted_traceback)
            retcode = ExitStatus.TROUBLE
            # stop at the first unexpected error,
            # something could be very wrong,
            # don't process all files unnecessarily
            if pool:
                pool.terminate()
            break
        else:
            sys.stderr.writelines(errs)
            if outs == []:
                continue
            if not args.quiet:
                print_diff(outs, use_color=colored_stdout)
            if retcode == ExitStatus.SUCCESS:
                retcode = ExitStatus.DIFF
    return retcode


if __name__ == '__main__':
    sys.exit(main())

# PROJECT: pytorch_audio FILE: .circleci/regenerate.py
#!/usr/bin/env python3

"""
This script should use a very simple, functional programming style.
Avoid Jinja macros in favor of native Python functions.

Don't go overboard on code generation; use Python only to generate
content that can't be easily declared statically using CircleCI's YAML API.

Data declarations (e.g. the nested loops for defining the configuration matrix)
should be at the top of the file for easy updating.

See this comment for design rationale:
https://github.com/pytorch/vision/pull/1321#issuecomment-531033978
"""

import jinja2
from jinja2 import select_autoescape
import yaml
import os.path


PYTHON_VERSIONS = ["3.6", "3.7", "3.8", "3.9"]
CU_VERSIONS_DICT = {"linux": ["cpu", "cu102", "cu111","cu113", "rocm4.1"],
                    "windows": ["cpu", "cu113"],
                    "macos": ["cpu"]}


DOC_VERSION = ('linux', '3.8')


def build_workflows(prefix='', upload=False, filter_branch=None, indentation=6):
    w = []
    w += build_download_job(filter_branch)
    for btype in ["wheel", "conda"]:
        for os_type in ["linux", "macos", "windows"]:
            for python_version in PYTHON_VERSIONS:
                for cu_version in CU_VERSIONS_DICT[os_type]:
                    fb = filter_branch
                    if cu_version.startswith("rocm") and btype=="conda":
                        continue
                    if not fb and (os_type == 'linux' and
                                   btype == 'wheel' and
                                   python_version == '3.8' and
                                   cu_version == 'cpu'):
                        # the fields must match the build_docs "requires" dependency
                        fb = '/.*/'
                    w += build_workflow_pair(btype, os_type, python_version, cu_version, fb, prefix, upload)

    if not filter_branch:
        # Build on every pull request, but upload only on nightly and tags
        w += build_doc_job('/.*/')
        w += upload_doc_job('nightly')
        w += docstring_parameters_sync_job(None)


    return indent(indentation, w)


def build_download_job(filter_branch):
    job = {
        "name": "download_third_parties_nix",
    }

    if filter_branch:
        job["filters"] = gen_filter_branch_tree(filter_branch)
    return [{"download_third_parties_nix": job}]


def build_workflow_pair(btype, os_type, python_version, cu_version, filter_branch, prefix='', upload=False):

    w = []
    base_workflow_name = f"{prefix}binary_{os_type}_{btype}_py{python_version}_{cu_version}"
    w.append(generate_base_workflow(base_workflow_name, python_version, cu_version, filter_branch, os_type, btype))

    if upload:

        w.append(generate_upload_workflow(base_workflow_name, filter_branch, os_type, btype, cu_version))

        if filter_branch == 'nightly' and os_type != 'macos':
            pydistro = 'pip' if btype == 'wheel' else 'conda'
            w.append(generate_smoketest_workflow(pydistro, base_workflow_name, filter_branch, python_version, cu_version, os_type))

    return w


def build_doc_job(filter_branch):
    job = {
        "name": "build_docs",
        "python_version": "3.8",
        "requires": ["binary_linux_wheel_py3.8_cpu", ],
    }

    if filter_branch:
        job["filters"] = gen_filter_branch_tree(filter_branch)
    return [{"build_docs": job}]


def upload_doc_job(filter_branch):
    job = {
        "name": "upload_docs",
        "context": "org-member",
        "python_version": "3.8",
        "requires": ["build_docs", ],
    }

    if filter_branch:
        job["filters"] = gen_filter_branch_tree(filter_branch)
    return [{"upload_docs": job}]


def docstring_parameters_sync_job(filter_branch):
    job = {
        "name": "docstring_parameters_sync",
        "python_version": "3.8",
        "requires": ["binary_linux_wheel_py3.8_cpu", ],
    }

    if filter_branch:
        job["filters"] = gen_filter_branch_tree(filter_branch)
    return [{"docstring_parameters_sync": job}]


def generate_base_workflow(base_workflow_name, python_version, cu_version, filter_branch, os_type, btype):

    d = {
        "name": base_workflow_name,
        "python_version": python_version,
        "cuda_version": cu_version,
    }

    if os_type in ['linux', 'macos']:
        d['requires'] = ['download_third_parties_nix']
    if btype == 'conda':
        d['conda_docker_image'] = f'pytorch/conda-builder:{cu_version.replace("cu1","cuda1")}'
    elif cu_version.startswith('cu'):
        d['wheel_docker_image'] = f'pytorch/manylinux-{cu_version.replace("cu1","cuda1")}'
    elif cu_version.startswith('rocm'):
        d["wheel_docker_image"] = f"pytorch/manylinux-rocm:{cu_version[len('rocm'):]}"

    if filter_branch:
        d["filters"] = gen_filter_branch_tree(filter_branch)

    return {f"binary_{os_type}_{btype}": d}


def gen_filter_branch_tree(*branches):
    return {
        "branches": {
            "only": list(branches),
        },
        "tags": {
            # Using a raw string here to avoid having to escape
            # anything
            "only": r"/v[0-9]+(\.[0-9]+)*-rc[0-9]+/"
        }
    }


def generate_upload_workflow(base_workflow_name, filter_branch, os_type, btype, cu_version):
    d = {
        "name": "{base_workflow_name}_upload".format(base_workflow_name=base_workflow_name),
        "context": "org-member",
        "requires": [base_workflow_name],
    }

    if btype == 'wheel':
        d["subfolder"] = "" if os_type == 'macos' else cu_version + "/"


    if filter_branch:
        d["filters"] = gen_filter_branch_tree(filter_branch)

    return {"binary_{btype}_upload".format(btype=btype): d}


def generate_smoketest_workflow(pydistro, base_workflow_name, filter_branch, python_version, cu_version, os_type):

    required_build_suffix = "_upload"
    required_build_name = base_workflow_name + required_build_suffix

    smoke_suffix = f"smoke_test_{pydistro}".format(pydistro=pydistro)
    d = {
        "name": f"{base_workflow_name}_{smoke_suffix}",
        "requires": [required_build_name],
        "python_version": python_version,
        "cuda_version": cu_version,
    }

    if filter_branch:
        d["filters"] = gen_filter_branch_tree(filter_branch)

    smoke_name = f"smoke_test_{os_type}_{pydistro}"
    if pydistro == "conda" and os_type == "linux" and cu_version != "cpu":
        smoke_name += "_gpu"
    return {smoke_name: d}


def indent(indentation, data_list):
    return ("\n" + " " * indentation).join(yaml.dump(data_list).splitlines())


def unittest_workflows(indentation=6):
    jobs = []
    jobs += build_download_job(None)
    for os_type in ["linux", "windows", "macos"]:
        for device_type in ["cpu", "gpu"]:
            if os_type == "macos" and device_type == "gpu":
                continue

            for i, python_version in enumerate(PYTHON_VERSIONS):
                job = {
                    "name": f"unittest_{os_type}_{device_type}_py{python_version}",
                    "python_version": python_version,
                    "cuda_version": 'cpu' if device_type == "cpu" else "cu113",
                }

                if os_type != "windows":
                    job['requires'] = ['download_third_parties_nix']

                jobs.append({f"unittest_{os_type}_{device_type}": job})

                if i == 0 and os_type == "linux" and device_type == "cpu":
                    jobs.append({
                        "stylecheck": {
                            "name": f"stylecheck_py{python_version}",
                            "python_version": python_version,
                            "cuda_version": "cpu",
                        }
                    })
    return indent(indentation, jobs)


if __name__ == "__main__":
    d = os.path.dirname(__file__)
    env = jinja2.Environment(
        loader=jinja2.FileSystemLoader(d),
        lstrip_blocks=True,
        autoescape=select_autoescape(enabled_extensions=('html', 'xml')),
    )

    with open(os.path.join(d, 'config.yml'), 'w') as f:
        f.write(env.get_template('config.yml.in').render(
            build_workflows=build_workflows,
            unittest_workflows=unittest_workflows,
        ))
        f.write("\n")

# PROJECT: huggingface_neuralcoref FILE: bin/cythonize.py
#!/usr/bin/env python
""" cythonize.py

Cythonize pyx files into C++ files as needed.

Usage: cythonize.py [root]

Checks pyx files to see if they have been changed relative to their
corresponding C++ files. If they have, then runs cython on these files to
recreate the C++ files.

Additionally, checks pxd files and setup.py if they have been changed. If
they have, rebuilds everything.

Change detection based on file hashes stored in JSON format.

For now, this script should be run by developers when changing Cython files
and the resulting C++ files checked in, so that end-users (and Python-only
developers) do not get the Cython dependencies.

Based upon:

https://raw.github.com/dagss/private-scipy-refactor/cythonize/cythonize.py
https://raw.githubusercontent.com/numpy/numpy/master/tools/cythonize.py

Note: this script does not check any of the dependent C++ libraries.
"""

import os
import sys
import json
import hashlib
import subprocess
import argparse


HASH_FILE = "cythonize.json"


def process_pyx(fromfile, tofile):
    print("Processing %s" % fromfile)
    try:
        from Cython.Compiler.Version import version as cython_version
        from distutils.version import LooseVersion

        if LooseVersion(cython_version) < LooseVersion("0.19"):
            raise Exception("Require Cython >= 0.19")

    except ImportError:
        pass

    flags = ["--fast-fail"]
    if tofile.endswith(".cpp"):
        flags += ["--cplus"]

    try:
        try:
            r = subprocess.call(
                ["cython"] + flags + ["-o", tofile, fromfile], env=os.environ
            )  # See Issue #791
            if r != 0:
                raise Exception("Cython failed")
        except OSError:
            # There are ways of installing Cython that don't result in a cython
            # executable on the path, see gh-2397.
            r = subprocess.call(
                [
                    sys.executable,
                    "-c",
                    "import sys; from Cython.Compiler.Main import "
                    "setuptools_main as main; sys.exit(main())",
                ]
                + flags
                + ["-o", tofile, fromfile]
            )
            if r != 0:
                raise Exception("Cython failed")
    except OSError:
        raise OSError("Cython needs to be installed")


def preserve_cwd(path, func, *args):
    orig_cwd = os.getcwd()
    try:
        os.chdir(path)
        func(*args)
    finally:
        os.chdir(orig_cwd)


def load_hashes(filename):
    try:
        return json.load(open(filename))
    except (ValueError, IOError):
        return {}


def save_hashes(hash_db, filename):
    with open(filename, "w") as f:
        f.write(json.dumps(hash_db))


def get_hash(path):
    return hashlib.md5(open(path, "rb").read()).hexdigest()


def hash_changed(base, path, db):
    full_path = os.path.normpath(os.path.join(base, path))
    return not get_hash(full_path) == db.get(full_path)


def hash_add(base, path, db):
    full_path = os.path.normpath(os.path.join(base, path))
    db[full_path] = get_hash(full_path)


def process(base, filename, db):
    root, ext = os.path.splitext(filename)
    if ext in [".pyx", ".cpp"]:
        if hash_changed(base, filename, db) or not os.path.isfile(
            os.path.join(base, root + ".cpp")
        ):
            preserve_cwd(base, process_pyx, root + ".pyx", root + ".cpp")
            hash_add(base, root + ".cpp", db)
            hash_add(base, root + ".pyx", db)


def check_changes(root, db):
    res = False
    new_db = {}

    setup_filename = "setup.py"
    hash_add(".", setup_filename, new_db)
    if hash_changed(".", setup_filename, db):
        res = True

    for base, _, files in os.walk(root):
        for filename in files:
            if filename.endswith(".pxd"):
                hash_add(base, filename, new_db)
                if hash_changed(base, filename, db):
                    res = True

    if res:
        db.clear()
        db.update(new_db)
    return res


def run(root):
    db = load_hashes(HASH_FILE)

    try:
        check_changes(root, db)
        for base, _, files in os.walk(root):
            for filename in files:
                process(base, filename, db)
    finally:
        save_hashes(db, HASH_FILE)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Cythonize pyx files into C++ files as needed"
    )
    parser.add_argument("root", help="root directory")
    args = parser.parse_args()
    run(args.root)

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/tests/__init__.py

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/tests/test_neuralcoref.py
import spacy
from ..__init__ import add_to_pipe


def test_add_pipe():
    nlp = spacy.lang.en.English()
    add_to_pipe(nlp)
    assert "neuralcoref" in nlp.pipe_names

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/__init__.py
import os
import tarfile
import logging

# Filter Cython warnings that would force everybody to re-compile from source (like https://github.com/numpy/numpy/pull/432).
import warnings

warnings.filterwarnings("ignore", message="spacy.strings.StringStore size changed")

from neuralcoref.neuralcoref import NeuralCoref
from neuralcoref.file_utils import (
    NEURALCOREF_MODEL_URL,
    NEURALCOREF_MODEL_PATH,
    NEURALCOREF_CACHE,
    cached_path,
)

__all__ = ["NeuralCoref", "add_to_pipe"]
__version__ = "4.1.0"

logger = logging.getLogger(__name__)

if os.path.exists(NEURALCOREF_MODEL_PATH) and os.path.exists(
    os.path.join(NEURALCOREF_MODEL_PATH, "cfg")
):
    logger.info(f"Loading model from {NEURALCOREF_MODEL_PATH}")
    local_model = cached_path(NEURALCOREF_MODEL_PATH)
else:
    if not os.path.exists(NEURALCOREF_MODEL_PATH):
        os.makedirs(NEURALCOREF_MODEL_PATH, exist_ok=True)
    logger.info(f"Getting model from {NEURALCOREF_MODEL_URL} or cache")
    downloaded_model = cached_path(NEURALCOREF_MODEL_URL)

    logger.info(
        f"extracting archive file {downloaded_model} to dir {NEURALCOREF_MODEL_PATH}"
    )
    with tarfile.open(downloaded_model, "r:gz") as archive:
        archive.extractall(NEURALCOREF_CACHE)


def add_to_pipe(nlp, **kwargs):
    coref = NeuralCoref(nlp.vocab, **kwargs)
    nlp.add_pipe(coref, name="neuralcoref")
    return nlp

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/algorithm.py
# cython: profile=True
# cython: infer_types=True
"""Coref resolution"""

import os
import spacy
import numpy as np

from neuralcoref.train.utils import PACKAGE_DIRECTORY, SIZE_SINGLE_IN
from neuralcoref.train.compat import unicode_
from neuralcoref.train.document import Document, MENTION_TYPE, NO_COREF_LIST

#######################
##### UTILITIES #######

MAX_FOLLOW_UP = 50

#######################
###### CLASSES ########


class Model(object):
    """
    Coreference neural model
    """

    def __init__(self, model_path):
        weights, biases = [], []
        for file in sorted(os.listdir(model_path)):
            if file.startswith("single_mention_weights"):
                w = np.load(os.path.join(model_path, file))
                weights.append(w)
            if file.startswith("single_mention_bias"):
                w = np.load(os.path.join(model_path, file))
                biases.append(w)
        self.single_mention_model = list(zip(weights, biases))
        weights, biases = [], []
        for file in sorted(os.listdir(model_path)):
            if file.startswith("pair_mentions_weights"):
                w = np.load(os.path.join(model_path, file))
                weights.append(w)
            if file.startswith("pair_mentions_bias"):
                w = np.load(os.path.join(model_path, file))
                biases.append(w)
        self.pair_mentions_model = list(zip(weights, biases))

    def _score(self, features, layers):
        for weights, bias in layers:
            # print("features", features.shape)
            features = np.matmul(weights, features) + bias
            if weights.shape[0] > 1:
                features = np.maximum(features, 0)  # ReLU
        return np.sum(features, axis=0)

    def get_multiple_single_score(self, first_layer_input):
        return self._score(first_layer_input, self.single_mention_model)

    def get_multiple_pair_score(self, first_layer_input):
        return self._score(first_layer_input, self.pair_mentions_model)


class Coref(object):
    """
    Main coreference resolution algorithm
    """

    def __init__(
        self,
        nlp=None,
        greedyness=0.5,
        max_dist=50,
        max_dist_match=500,
        conll=None,
        blacklist=True,
        debug=False,
    ):
        self.greedyness = greedyness
        self.max_dist = max_dist
        self.max_dist_match = max_dist_match
        self.debug = debug
        model_path = os.path.join(
            PACKAGE_DIRECTORY, "weights/conll/" if conll is not None else "weights/"
        )
        model_path = os.path.join(PACKAGE_DIRECTORY, "weights/")
        print("Loading neuralcoref model from", model_path)
        self.coref_model = Model(model_path)
        if nlp is None:
            print("Loading spacy model")
            try:
                spacy.info("en_core_web_sm")
                model = "en_core_web_sm"
            except IOError:
                print("No spacy 2 model detected, using spacy1 'en' model")
                spacy.info("en")
                model = "en"
            nlp = spacy.load(model)
        self.data = Document(
            nlp, conll=conll, blacklist=blacklist, model_path=model_path
        )
        self.clusters = {}
        self.mention_to_cluster = []
        self.mentions_single_scores = {}
        self.mentions_pairs_scores = {}

    ###################################
    #### ENTITY CLUSTERS FUNCTIONS ####
    ###################################

    def _prepare_clusters(self):
        """
        Clean up and prepare one cluster for each mention
        """
        self.mention_to_cluster = list(range(len(self.data.mentions)))
        self.clusters = dict((i, [i]) for i in self.mention_to_cluster)
        self.mentions_single_scores = {}
        self.mentions_pairs_scores = {}
        for mention in self.mention_to_cluster:
            self.mentions_single_scores[mention] = None
            self.mentions_pairs_scores[mention] = {}

    def _merge_coreference_clusters(self, ant_idx, mention_idx):
        """
        Merge two clusters together
        """
        if self.mention_to_cluster[ant_idx] == self.mention_to_cluster[mention_idx]:
            return

        remove_id = self.mention_to_cluster[ant_idx]
        keep_id = self.mention_to_cluster[mention_idx]
        for idx in self.clusters[remove_id]:
            self.mention_to_cluster[idx] = keep_id
            self.clusters[keep_id].append(idx)

        del self.clusters[remove_id]

    def remove_singletons_clusters(self):
        remove_id = []
        for key, mentions in self.clusters.items():
            if len(mentions) == 1:
                remove_id.append(key)
                self.mention_to_cluster[key] = None
        for rem in remove_id:
            del self.clusters[rem]

    def display_clusters(self):
        """
        Print clusters informations
        """
        print(self.clusters)
        for key, mentions in self.clusters.items():
            print(
                "cluster",
                key,
                "(",
                ", ".join(unicode_(self.data[m]) for m in mentions),
                ")",
            )

    ###################################
    ####### MAIN COREF FUNCTIONS ######
    ###################################

    def run_coref_on_mentions(self, mentions):
        """
        Run the coreference model on a mentions list
        """
        best_ant = {}
        best_score = {}
        n_ant = 0
        inp = np.empty((SIZE_SINGLE_IN, len(mentions)))
        for i, mention_idx in enumerate(mentions):
            mention = self.data[mention_idx]
            print("mention.embedding", mention.embedding.shape)
            inp[: len(mention.embedding), i] = mention.embedding
            inp[: len(mention.embedding), i] = mention.features
            inp[: len(mention.embedding), i] = self.data.genre
        score = self.coref_model.get_multiple_single_score(inp.T)
        for mention_idx, s in zip(mentions, score):
            self.mentions_single_scores[mention_idx] = s
            best_score[mention_idx] = s - 50 * (self.greedyness - 0.5)

        for mention_idx, ant_list in self.data.get_candidate_pairs(
            mentions, self.max_dist, self.max_dist_match
        ):
            if len(ant_list) == 0:
                continue
            inp_l = []
            for ant_idx in ant_list:
                mention = self.data[mention_idx]
                antecedent = self.data[ant_idx]
                feats_, pwf = self.data.get_pair_mentions_features(antecedent, mention)
                inp_l.append(pwf)
            inp = np.stack(inp_l, axis=0)
            score = self.coref_model.get_multiple_pair_score(inp.T)
            for ant_idx, s in zip(ant_list, score):
                self.mentions_pairs_scores[mention_idx][ant_idx] = s
                if s > best_score[mention_idx]:
                    best_score[mention_idx] = s
                    best_ant[mention_idx] = ant_idx
            if mention_idx in best_ant:
                n_ant += 1
                self._merge_coreference_clusters(best_ant[mention_idx], mention_idx)
        return (n_ant, best_ant)

    def run_coref_on_utterances(
        self, last_utterances_added=False, follow_chains=True, debug=False
    ):
        """ Run the coreference model on some utterances

        Arg:
            last_utterances_added: run the coreference model over the last utterances added to the data
            follow_chains: follow coreference chains over previous utterances
        """
        if debug:
            print("== run_coref_on_utterances == start")
        self._prepare_clusters()
        if debug:
            self.display_clusters()
        mentions = list(
            self.data.get_candidate_mentions(
                last_utterances_added=last_utterances_added
            )
        )
        n_ant, antecedents = self.run_coref_on_mentions(mentions)
        mentions = antecedents.values()
        if follow_chains and last_utterances_added and n_ant > 0:
            i = 0
            while i < MAX_FOLLOW_UP:
                i += 1
                n_ant, antecedents = self.run_coref_on_mentions(mentions)
                mentions = antecedents.values()
                if n_ant == 0:
                    break
        if debug:
            self.display_clusters()
        if debug:
            print("== run_coref_on_utterances == end")

    def one_shot_coref(
        self,
        utterances,
        utterances_speakers_id=None,
        context=None,
        context_speakers_id=None,
        speakers_names=None,
    ):
        """ Clear history, load a list of utterances and an optional context and run the coreference model on them

        Arg:
        - `utterances` : iterator or list of string corresponding to successive utterances (in a dialogue) or sentences.
            Can be a single string for non-dialogue text.
        - `utterances_speakers_id=None` : iterator or list of speaker id for each utterance (in the case of a dialogue).
            - if not provided, assume two speakers speaking alternatively.
            - if utterances and utterances_speaker are not of the same length padded with None
        - `context=None` : iterator or list of string corresponding to additionnal utterances/sentences sent prior to `utterances`. Coreferences are not computed for the mentions identified in `context`. The mentions in `context` are only used as possible antecedents to mentions in `uterrance`. Reduce the computations when we are only interested in resolving coreference in the last sentences/utterances.
        - `context_speakers_id=None` : same as `utterances_speakers_id` for `context`. 
        - `speakers_names=None` : dictionnary of list of acceptable speaker names (strings) for speaker_id in `utterances_speakers_id` and `context_speakers_id`
        Return:
            clusters of entities with coreference resolved
        """
        self.data.set_utterances(context, context_speakers_id, speakers_names)
        self.continuous_coref(utterances, utterances_speakers_id, speakers_names)
        return self.get_clusters()

    def continuous_coref(
        self, utterances, utterances_speakers_id=None, speakers_names=None
    ):
        """
        Only resolve coreferences for the mentions in the utterances
        (but use the mentions in previously loaded utterances as possible antecedents)
        Arg:
            utterances : iterator or list of string corresponding to successive utterances
            utterances_speaker : iterator or list of speaker id for each utterance.
                If not provided, assume two speakers speaking alternatively.
                if utterances and utterances_speaker are not of the same length padded with None
            speakers_names : dictionnary of list of acceptable speaker names for each speaker id
        Return:
            clusters of entities with coreference resolved
        """
        self.data.add_utterances(utterances, utterances_speakers_id, speakers_names)
        self.run_coref_on_utterances(last_utterances_added=True, follow_chains=True)
        return self.get_clusters()

    ###################################
    ###### INFORMATION RETRIEVAL ######
    ###################################

    def get_utterances(self, last_utterances_added=True):
        """ Retrieve the list of parsed uterrances"""
        if last_utterances_added and len(self.data.last_utterances_loaded):
            return [
                self.data.utterances[idx] for idx in self.data.last_utterances_loaded
            ]
        else:
            return self.data.utterances

    def get_resolved_utterances(self, last_utterances_added=True, blacklist=True):
        """ Return a list of utterrances text where the """
        coreferences = self.get_most_representative(last_utterances_added, blacklist)
        resolved_utterances = []
        for utt in self.get_utterances(last_utterances_added=last_utterances_added):
            resolved_utt = ""
            in_coref = None
            for token in utt:
                if in_coref is None:
                    for coref_original, coref_replace in coreferences.items():
                        if coref_original[0] == token:
                            in_coref = coref_original
                            resolved_utt += coref_replace.text.lower()
                            break
                    if in_coref is None:
                        resolved_utt += token.text_with_ws
                if in_coref is not None and token == in_coref[-1]:
                    resolved_utt += (
                        " " if token.whitespace_ and resolved_utt[-1] is not " " else ""
                    )
                    in_coref = None
            resolved_utterances.append(resolved_utt)
        return resolved_utterances

    def get_mentions(self):
        """ Retrieve the list of mentions"""
        return self.data.mentions

    def get_scores(self):
        """ Retrieve scores for single mentions and pair of mentions"""
        return {
            "single_scores": self.mentions_single_scores,
            "pair_scores": self.mentions_pairs_scores,
        }

    def get_clusters(self, remove_singletons=False, blacklist=False):
        """ Retrieve cleaned clusters"""
        clusters = self.clusters
        mention_to_cluster = self.mention_to_cluster
        remove_id = []
        if blacklist:
            for key, mentions in clusters.items():
                cleaned_list = []
                for mention_idx in mentions:
                    mention = self.data.mentions[mention_idx]
                    if mention.lower_ not in NO_COREF_LIST:
                        cleaned_list.append(mention_idx)
                clusters[key] = cleaned_list
            # Also clean up keys so we can build coref chains in self.get_most_representative
            added = {}
            for key, mentions in clusters.items():
                if self.data.mentions[key].lower_ in NO_COREF_LIST:
                    remove_id.append(key)
                    mention_to_cluster[key] = None
                    if mentions:
                        added[mentions[0]] = mentions
            for rem in remove_id:
                del clusters[rem]
            clusters.update(added)

        if remove_singletons:
            remove_id = []
            for key, mentions in clusters.items():
                if len(mentions) == 1:
                    remove_id.append(key)
                    mention_to_cluster[key] = None
            for rem in remove_id:
                del clusters[rem]

        return clusters, mention_to_cluster

    def get_most_representative(self, last_utterances_added=True, blacklist=True):
        """
        Find a most representative mention for each cluster

        Return:
            Dictionnary of {original_mention: most_representative_resolved_mention, ...}
        """
        clusters, _ = self.get_clusters(remove_singletons=True, blacklist=blacklist)
        coreferences = {}
        for key in self.data.get_candidate_mentions(
            last_utterances_added=last_utterances_added
        ):
            if self.mention_to_cluster[key] is None:
                continue
            mentions = clusters.get(self.mention_to_cluster[key], None)
            if mentions is None:
                continue
            representative = self.data.mentions[key]
            for mention_idx in mentions[1:]:
                mention = self.data.mentions[mention_idx]
                if mention.mention_type is not representative.mention_type:
                    if mention.mention_type == MENTION_TYPE["PROPER"] or (
                        mention.mention_type == MENTION_TYPE["NOMINAL"]
                        and representative.mention_type == MENTION_TYPE["PRONOMINAL"]
                    ):
                        coreferences[self.data.mentions[key]] = mention
                        representative = mention

        return coreferences

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/conllparser.py
"""Conll parser"""

import re
import argparse
import time
import os
import io
import pickle

import spacy

import numpy as np

from tqdm import tqdm

from neuralcoref.train.compat import unicode_
from neuralcoref.train.document import (
    Mention,
    Document,
    Speaker,
    EmbeddingExtractor,
    MISSING_WORD,
    extract_mentions_spans,
)
from neuralcoref.train.utils import parallel_process

PACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))
REMOVED_CHAR = ["/", "%", "*"]
NORMALIZE_DICT = {
    "/.": ".",
    "/?": "?",
    "-LRB-": "(",
    "-RRB-": ")",
    "-LCB-": "{",
    "-RCB-": "}",
    "-LSB-": "[",
    "-RSB-": "]",
}

CONLL_GENRES = {"bc": 0, "bn": 1, "mz": 2, "nw": 3, "pt": 4, "tc": 5, "wb": 6}

FEATURES_NAMES = [
    "mentions_features",  # 0
    "mentions_labels",  # 1
    "mentions_pairs_length",  # 2
    "mentions_pairs_start_index",  # 3
    "mentions_spans",  # 4
    "mentions_words",  # 5
    "pairs_ant_index",  # 6
    "pairs_features",  # 7
    "pairs_labels",  # 8
    "locations",  # 9
    "conll_tokens",  # 10
    "spacy_lookup",  # 11
    "doc",  # 12
]

MISSED_MENTIONS_FILE = os.path.join(
    PACKAGE_DIRECTORY, "test_mentions_identification.txt"
)
SENTENCES_PATH = os.path.join(PACKAGE_DIRECTORY, "test_sentences.txt")

###################
### UTILITIES #####


def clean_token(token):
    cleaned_token = token
    if cleaned_token in NORMALIZE_DICT:
        cleaned_token = NORMALIZE_DICT[cleaned_token]
    if cleaned_token not in REMOVED_CHAR:
        for char in REMOVED_CHAR:
            cleaned_token = cleaned_token.replace(char, "")
    if len(cleaned_token) == 0:
        cleaned_token = ","
    return cleaned_token


def mention_words_idx(embed_extractor, mention, debug=False):
    # index of the word in the tuned embeddings no need for normalizing,
    # it is already performed in set_mentions_features()
    # We take them in the tuned vocabulary which is a smaller voc tailored from conll
    words = []
    for _, w in sorted(mention.words_embeddings_.items()):
        if w not in embed_extractor.tun_idx:
            if debug:
                print(
                    "No matching tokens in tuned voc for word ",
                    w,
                    "surrounding or inside mention",
                    mention,
                )
            words.append(MISSING_WORD)
        else:
            words.append(w)
    return [embed_extractor.tun_idx[w] for w in words]


def check_numpy_array(feature, array, n_mentions_list, compressed=True):
    for n_mentions in n_mentions_list:
        if feature == FEATURES_NAMES[0]:
            assert array.shape[0] == len(n_mentions)
            if compressed:
                assert np.array_equiv(
                    array[:, 3], np.array([len(n_mentions)] * len(n_mentions))
                )
                assert np.max(array[:, 2]) == len(n_mentions) - 1
                assert np.min(array[:, 2]) == 0
        elif feature == FEATURES_NAMES[1]:
            assert array.shape[0] == len(n_mentions)
        elif feature == FEATURES_NAMES[2]:
            assert array.shape[0] == len(n_mentions)
            assert np.array_equiv(array[:, 0], np.array(list(range(len(n_mentions)))))
        elif feature == FEATURES_NAMES[3]:
            assert array.shape[0] == len(n_mentions)
            assert np.array_equiv(
                array[:, 0], np.array([p * (p - 1) / 2 for p in range(len(n_mentions))])
            )
        elif feature == FEATURES_NAMES[4]:
            assert array.shape[0] == len(n_mentions)
        elif feature == FEATURES_NAMES[5]:
            assert array.shape[0] == len(n_mentions)
        elif feature == FEATURES_NAMES[6]:
            assert array.shape[0] == len(n_mentions) * (len(n_mentions) - 1) / 2
            assert np.max(array) == len(n_mentions) - 2
        elif feature == FEATURES_NAMES[7]:
            if compressed:
                assert array.shape[0] == len(n_mentions) * (len(n_mentions) - 1) / 2
                assert np.max(array[:, 7]) == len(n_mentions) - 2
                assert np.min(array[:, 7]) == 0
        elif feature == FEATURES_NAMES[8]:
            assert array.shape[0] == len(n_mentions) * (len(n_mentions) - 1) / 2


###############################################################################################
### PARALLEL FCT (has to be at top-level of the module to be pickled for multiprocessing) #####
def load_file(full_name, debug=False):
    """
    load a *._conll file
    Input: full_name: path to the file
    Output: list of tuples for each conll doc in the file, where the tuple contains:
        (utts_text ([str]): list of the utterances in the document
         utts_tokens ([[str]]): list of the tokens (conll words) in the document
         utts_corefs: list of coref objects (dicts) with the following properties:
            coref['label']: id of the coreference cluster,
            coref['start']: start index (index of first token in the utterance),
            coref['end': end index (index of last token in the utterance).
         utts_speakers ([str]): list of the speaker associated to each utterances in the document
         name (str): name of the document
         part (str): part of the document
        )
    """
    docs = []
    with io.open(full_name, "rt", encoding="utf-8", errors="strict") as f:
        lines = list(f)  # .readlines()
        utts_text = []
        utts_tokens = []
        utts_corefs = []
        utts_speakers = []
        tokens = []
        corefs = []
        index = 0
        speaker = ""
        name = ""
        part = ""
        for li, line in enumerate(lines):
            cols = line.split()
            if debug:
                print("line", li, "cols:", cols)
            # End of utterance
            if len(cols) == 0:
                if tokens:
                    if debug:
                        print("End of utterance")
                    utts_text.append("".join(t + " " for t in tokens))
                    utts_tokens.append(tokens)
                    utts_speakers.append(speaker)
                    utts_corefs.append(corefs)
                    tokens = []
                    corefs = []
                    index = 0
                    speaker = ""
                    continue
            # End of doc
            elif len(cols) == 2:
                if debug:
                    print("End of doc")
                if cols[0] == "#end":
                    if debug:
                        print("Saving doc")
                    docs.append(
                        (utts_text, utts_tokens, utts_corefs, utts_speakers, name, part)
                    )
                    utts_text = []
                    utts_tokens = []
                    utts_corefs = []
                    utts_speakers = []
                else:
                    raise ValueError("Error on end line " + line)
            # New doc
            elif len(cols) == 5:
                if debug:
                    print("New doc")
                if cols[0] == "#begin":
                    name = re.match(r"\((.*)\);", cols[2]).group(1)
                    try:
                        part = cols[4]
                    except ValueError:
                        print("Error parsing document part " + line)
                    if debug:
                        print("New doc", name, part, name[:2])
                    tokens = []
                    corefs = []
                    index = 0
                else:
                    raise ValueError("Error on begin line " + line)
            # Inside utterance
            elif len(cols) > 7:
                if debug:
                    print("Inside utterance")
                assert cols[0] == name and int(cols[1]) == int(part), (
                    "Doc name or part error " + line
                )
                assert int(cols[2]) == index, "Index error on " + line
                if speaker:
                    assert cols[9] == speaker, "Speaker changed in " + line + speaker
                else:
                    speaker = cols[9]
                    if debug:
                        print("speaker", speaker)
                if cols[-1] != "-":
                    coref_expr = cols[-1].split("|")
                    if debug:
                        print("coref_expr", coref_expr)
                    if not coref_expr:
                        raise ValueError("Coref expression empty " + line)
                    for tok in coref_expr:
                        if debug:
                            print("coref tok", tok)
                        try:
                            match = re.match(r"^(\(?)(\d+)(\)?)$", tok)
                        except:
                            print("error getting coreferences for line " + line)
                        assert match is not None, (
                            "Error parsing coref " + tok + " in " + line
                        )
                        num = match.group(2)
                        assert num is not "", (
                            "Error parsing coref " + tok + " in " + line
                        )
                        if match.group(1) == "(":
                            if debug:
                                print("New coref", num)
                            corefs.append({"label": num, "start": index, "end": None})
                        if match.group(3) == ")":
                            j = None
                            for i in range(len(corefs) - 1, -1, -1):
                                if debug:
                                    print("i", i)
                                if (
                                    corefs[i]["label"] == num
                                    and corefs[i]["end"] is None
                                ):
                                    j = i
                                    break
                            assert j is not None, "coref closing error " + line
                            if debug:
                                print("End coref", num)
                            corefs[j]["end"] = index
                tokens.append(clean_token(cols[3]))
                index += 1
            else:
                raise ValueError("Line not standard " + line)
    return docs


def set_feats(doc):
    doc.set_mentions_features()


def get_feats(doc, i):
    return doc.get_feature_array(doc_id=i)


def gather_feats(gathering_array, array, feat_name, pairs_ant_index, pairs_start_index):
    if gathering_array is None:
        gathering_array = array
    else:
        if feat_name == FEATURES_NAMES[6]:
            array = [a + pairs_ant_index for a in array]
        elif feat_name == FEATURES_NAMES[3]:
            array = [a + pairs_start_index for a in array]
        gathering_array += array
    return feat_name, gathering_array


def read_file(full_name):
    doc = ""
    with io.open(full_name, "rt", encoding="utf-8", errors="strict") as f:
        doc = f.read()
    return doc


###################
### ConllDoc #####


class ConllDoc(Document):
    def __init__(self, name, part, *args, **kwargs):
        self.name = name
        self.part = part
        self.feature_matrix = {}
        self.conll_tokens = []
        self.conll_lookup = []
        self.gold_corefs = []
        self.missed_gold = []
        super(ConllDoc, self).__init__(*args, **kwargs)

    def get_conll_spacy_lookup(self, conll_tokens, spacy_tokens, debug=False):
        """
        Compute a look up table between spacy tokens (from spacy tokenizer)
        and conll pre-tokenized tokens
        Output: list[conll_index] => list of associated spacy tokens (assume spacy tokenizer has a finer granularity)
        """
        lookup = []
        c_iter = (t for t in conll_tokens)
        s_iter = enumerate(t for t in spacy_tokens)
        i, s_tok = next(s_iter)
        for c_tok in c_iter:
            # if debug: print("conll", c_tok, "spacy", s_tok, "index", i)
            c_lookup = []
            while i is not None and len(c_tok) and c_tok.startswith(s_tok.text):
                c_lookup.append(i)
                c_tok = c_tok[len(s_tok) :]
                i, s_tok = next(s_iter, (None, None))
                if debug and len(c_tok):
                    print("eating token: conll", c_tok, "spacy", s_tok, "index", i)
            assert len(c_lookup), "Unmatched conll and spacy tokens"
            lookup.append(c_lookup)
        return lookup

    def add_conll_utterance(
        self, parsed, tokens, corefs, speaker_id, use_gold_mentions, debug=False
    ):
        conll_lookup = self.get_conll_spacy_lookup(tokens, parsed)
        self.conll_tokens.append(tokens)
        self.conll_lookup.append(conll_lookup)
        # Convert conll tokens coref index in spacy tokens indexes
        identified_gold = [False] * len(corefs)
        for coref in corefs:
            missing_values = [key for key in ['label', 'start', 'end', ] if coref.get(key, None) is None]
            if missing_values:
                found_values = {key: coref[key] for key in ['label', 'start', 'end'] if coref.get(key, None) is not None}
                raise Exception(f"Coref {self.name} with fields {found_values} has empty values for the keys {missing_values}.")

            coref["start"] = conll_lookup[coref["start"]][0]
            coref["end"] = conll_lookup[coref["end"]][-1]

        if speaker_id not in self.speakers:
            speaker_name = speaker_id.split("_")
            if debug:
                print("New speaker: ", speaker_id, "name: ", speaker_name)
            self.speakers[speaker_id] = Speaker(speaker_id, speaker_name)
        if use_gold_mentions:
            for coref in corefs:
                # print("coref['label']", coref['label'])
                # print("coref text",parsed[coref['start']:coref['end']+1])
                mention = Mention(
                    parsed[coref["start"] : coref["end"] + 1],
                    len(self.mentions),
                    len(self.utterances),
                    self.n_sents,
                    speaker=self.speakers[speaker_id],
                    gold_label=coref["label"],
                )
                self.mentions.append(mention)
                # print("mention: ", mention, "label", mention.gold_label)
        else:
            mentions_spans = extract_mentions_spans(
                doc=parsed, blacklist=self.blacklist
            )
            self._process_mentions(
                mentions_spans,
                len(self.utterances),
                self.n_sents,
                self.speakers[speaker_id],
            )

            # Assign a gold label to mentions which have one
            if debug:
                print("Check corefs", corefs)
            for i, coref in enumerate(corefs):
                for m in self.mentions:
                    if m.utterance_index != len(self.utterances):
                        continue
                    # if debug: print("Checking mention", m, m.utterance_index, m.start, m.end)
                    if coref["start"] == m.start and coref["end"] == m.end - 1:
                        m.gold_label = coref["label"]
                        identified_gold[i] = True
                        # if debug: print("Gold mention found:", m, coref['label'])
            for found, coref in zip(identified_gold, corefs):
                if not found:
                    self.missed_gold.append(
                        [
                            self.name,
                            self.part,
                            str(len(self.utterances)),
                            parsed.text,
                            parsed[coref["start"] : coref["end"] + 1].text,
                        ]
                    )
                    if debug:
                        print(
                            " gold mention not in predicted mentions",
                            coref,
                            parsed[coref["start"] : coref["end"] + 1],
                        )
        self.utterances.append(parsed)
        self.gold_corefs.append(corefs)
        self.utterances_speaker.append(self.speakers[speaker_id])
        self.n_sents += len(list(parsed.sents))

    def get_single_mention_features_conll(self, mention, compressed=True):
        """ Compressed or not single mention features"""
        if not compressed:
            _, features = self.get_single_mention_features(mention)
            return features[np.newaxis, :]
        feat_l = [
            mention.features_["01_MentionType"],
            mention.features_["02_MentionLength"],
            mention.index,
            len(self.mentions),
            mention.features_["04_IsMentionNested"],
            self.genre_,
        ]
        return feat_l

    def get_pair_mentions_features_conll(self, m1, m2, compressed=True):
        """ Compressed or not single mention features"""
        if not compressed:
            _, features = self.get_pair_mentions_features(m1, m2)
            return features[np.newaxis, :]
        features_, _ = self.get_pair_mentions_features(m1, m2)
        feat_l = [
            features_["00_SameSpeaker"],
            features_["01_AntMatchMentionSpeaker"],
            features_["02_MentionMatchSpeaker"],
            features_["03_HeadsAgree"],
            features_["04_ExactStringMatch"],
            features_["05_RelaxedStringMatch"],
            features_["06_SentenceDistance"],
            features_["07_MentionDistance"],
            features_["08_Overlapping"],
        ]
        return feat_l

    def get_feature_array(self, doc_id, feature=None, compressed=True, debug=False):
        """
        Prepare feature array:
            mentions_spans: (N, S)
            mentions_words: (N, W)
            mentions_features: (N, Fs)
            mentions_labels: (N, 1)
            mentions_pairs_start_index: (N, 1) index of beggining of pair list in pair_labels
            mentions_pairs_length: (N, 1) number of pairs (i.e. nb of antecedents) for each mention
            pairs_features: (P, Fp)
            pairs_labels: (P, 1)
            pairs_ant_idx: (P, 1) => indexes of antecedents mention for each pair (mention index in doc)
        """
        if not self.mentions:
            if debug:
                print("No mention in this doc !")
            return {}
        if debug:
            print(" features matrices")
        mentions_spans = []
        mentions_words = []
        mentions_features = []
        pairs_ant_idx = []
        pairs_features = []
        pairs_labels = []
        mentions_labels = []
        mentions_pairs_start = []
        mentions_pairs_length = []
        mentions_location = []
        n_mentions = 0
        total_pairs = 0
        if debug:
            print("mentions", self.mentions, str([m.gold_label for m in self.mentions]))
        for mention_idx, antecedents_idx in list(
            self.get_candidate_pairs(max_distance=None, max_distance_with_match=None)
        ):
            n_mentions += 1
            mention = self.mentions[mention_idx]
            mentions_spans.append(mention.spans_embeddings)
            w_idx = mention_words_idx(self.embed_extractor, mention)
            if w_idx is None:
                print("error in", self.name, self.part, mention.utterance_index)
            mentions_words.append(w_idx)
            mentions_features.append(
                self.get_single_mention_features_conll(mention, compressed)
            )
            mentions_location.append(
                [
                    mention.start,
                    mention.end,
                    mention.utterance_index,
                    mention_idx,
                    doc_id,
                ]
            )
            ants = [self.mentions[ant_idx] for ant_idx in antecedents_idx]
            no_antecedent = (
                not any(ant.gold_label == mention.gold_label for ant in ants)
                or mention.gold_label is None
            )
            if antecedents_idx:
                pairs_ant_idx += [idx for idx in antecedents_idx]
                pairs_features += [
                    self.get_pair_mentions_features_conll(ant, mention, compressed)
                    for ant in ants
                ]
                ant_labels = (
                    [0 for ant in ants]
                    if no_antecedent
                    else [
                        1 if ant.gold_label == mention.gold_label else 0 for ant in ants
                    ]
                )
                pairs_labels += ant_labels
            mentions_labels.append(1 if no_antecedent else 0)
            mentions_pairs_start.append(total_pairs)
            total_pairs += len(ants)
            mentions_pairs_length.append(len(ants))

        out_dict = {
            FEATURES_NAMES[0]: mentions_features,
            FEATURES_NAMES[1]: mentions_labels,
            FEATURES_NAMES[2]: mentions_pairs_length,
            FEATURES_NAMES[3]: mentions_pairs_start,
            FEATURES_NAMES[4]: mentions_spans,
            FEATURES_NAMES[5]: mentions_words,
            FEATURES_NAMES[6]: pairs_ant_idx if pairs_ant_idx else None,
            FEATURES_NAMES[7]: pairs_features if pairs_features else None,
            FEATURES_NAMES[8]: pairs_labels if pairs_labels else None,
            FEATURES_NAMES[9]: [mentions_location],
            FEATURES_NAMES[10]: [self.conll_tokens],
            FEATURES_NAMES[11]: [self.conll_lookup],
            FEATURES_NAMES[12]: [
                {
                    "name": self.name,
                    "part": self.part,
                    "utterances": list(str(u) for u in self.utterances),
                    "mentions": list(str(m) for m in self.mentions),
                }
            ],
        }
        if debug:
            print(" Summary")
            for k, v in out_dict.items():
                print(k, len(v))
        return n_mentions, total_pairs, out_dict


###################
### ConllCorpus #####
class ConllCorpus(object):
    def __init__(
        self,
        n_jobs=4,
        embed_path=PACKAGE_DIRECTORY + "/weights/",
        gold_mentions=False,
        blacklist=False,
    ):
        self.n_jobs = n_jobs
        self.features = {}
        self.utts_text = []
        self.utts_tokens = []
        self.utts_corefs = []
        self.utts_speakers = []
        self.utts_doc_idx = []
        self.docs_names = []
        self.docs = []
        if embed_path is not None:
            self.embed_extractor = EmbeddingExtractor(embed_path)
        self.trainable_embed = []
        self.trainable_voc = []
        self.gold_mentions = gold_mentions
        self.blacklist = blacklist

    def check_words_in_embeddings_voc(self, embedding, tuned=True, debug=False):
        print(" Checking if words are in embedding voc")
        if tuned:
            embed_voc = embedding.tun_idx
        else:
            embed_voc = embedding.stat_idx
        missing_words = []
        missing_words_sents = []
        missing_words_doc = []
        for doc in self.docs:
            # if debug: print("Checking doc", doc.name, doc.part)
            for sent in doc.utterances:
                # if debug: print(sent.text)
                for word in sent:
                    w = embedding.normalize_word(word)
                    # if debug: print(w)
                    if w not in embed_voc:
                        missing_words.append(w)
                        missing_words_sents.append(sent.text)
                        missing_words_doc.append(doc.name + doc.part)
                        if debug:
                            out_str = (
                                "No matching tokens in tuned voc for "
                                + w
                                + " in sentence "
                                + sent.text
                                + " in doc "
                                + doc.name
                                + doc.part
                            )
                            print(out_str)
        return missing_words, missing_words_sents, missing_words_doc

    def test_sentences_words(self, save_file, debug=False):
        print(" Saving sentence list")
        with io.open(save_file, "w", encoding="utf-8") as f:
            if debug:
                print("Sentences saved in", save_file)
            for doc in self.docs:
                out_str = "#begin document (" + doc.name + "); part " + doc.part + "\n"
                f.write(out_str)
                for sent in doc.utterances:
                    f.write(sent.text + "\n")
                out_str = "#end document\n\n"
                f.write(out_str)

    def save_sentences(self, save_file, debug=False):
        print(" Saving sentence list")
        with io.open(save_file, "w", encoding="utf-8") as f:
            if debug:
                print("Sentences saved in", save_file)
            for doc in self.docs:
                out_str = "#begin document (" + doc.name + "); part " + doc.part + "\n"
                f.write(out_str)
                for sent in doc.utterances:
                    f.write(sent.text + "\n")
                out_str = "#end document\n\n"
                f.write(out_str)

    def build_key_file(self, data_path, key_file, debug=False):
        print(" Building key file from corpus")
        print("Saving in", key_file)
        # Create a pool of processes. By default, one is created for each CPU in your machine.
        with io.open(key_file, "w", encoding="utf-8") as kf:
            if debug:
                print("Key file saved in", key_file)
            for dirpath, _, filenames in os.walk(data_path):
                print("In", dirpath)
                file_list = [
                    os.path.join(dirpath, f)
                    for f in filenames
                    if f.endswith(".v4_auto_conll") or f.endswith(".v4_gold_conll")
                ]
                cleaned_file_list = []
                for f in file_list:
                    fn = f.split(".")
                    if fn[1] == "v4_auto_conll":
                        gold = fn[0] + "." + "v4_gold_conll"
                        if gold not in file_list:
                            cleaned_file_list.append(f)
                    else:
                        cleaned_file_list.append(f)
                # self.load_file(file_list[0])
                doc_list = parallel_process(cleaned_file_list, read_file)
                for doc in doc_list:
                    kf.write(doc)

    def list_undetected_mentions(self, data_path, save_file, debug=True):
        self.read_corpus(data_path)
        print(" Listing undetected mentions")
        with io.open(save_file, "w", encoding="utf-8") as out_file:
            for doc in tqdm(self.docs):
                for name, part, utt_i, utt, coref in doc.missed_gold:
                    out_str = name + "\t" + part + "\t" + utt_i + '\t"' + utt + '"\n'
                    out_str += coref + "\n"
                    out_file.write(out_str)
                    if debug:
                        print(out_str)

    def read_corpus(self, data_path, model=None, debug=False):
        print(" Reading files")
        for dirpath, _, filenames in os.walk(data_path):
            print("In", dirpath, os.path.abspath(dirpath))
            file_list = [
                os.path.join(dirpath, f)
                for f in filenames
                if f.endswith(".v4_auto_conll") or f.endswith(".v4_gold_conll")
            ]
            cleaned_file_list = []
            for f in file_list:
                fn = f.split(".")
                if fn[1] == "v4_auto_conll":
                    gold = fn[0] + "." + "v4_gold_conll"
                    if gold not in file_list:
                        cleaned_file_list.append(f)
                else:
                    cleaned_file_list.append(f)
            doc_list = parallel_process(cleaned_file_list, load_file)
            for docs in doc_list:  # executor.map(self.load_file, cleaned_file_list):
                for (
                    utts_text,
                    utt_tokens,
                    utts_corefs,
                    utts_speakers,
                    name,
                    part,
                ) in docs:
                    if debug:
                        print("Imported", name)
                        print("utts_text", utts_text)
                        print("utt_tokens", utt_tokens)
                        print("utts_corefs", utts_corefs)
                        print("utts_speakers", utts_speakers)
                        print("name, part", name, part)
                    self.utts_text += utts_text
                    self.utts_tokens += utt_tokens
                    self.utts_corefs += utts_corefs
                    self.utts_speakers += utts_speakers
                    self.utts_doc_idx += [len(self.docs_names)] * len(utts_text)
                    self.docs_names.append((name, part))
        print("utts_text size", len(self.utts_text))
        print("utts_tokens size", len(self.utts_tokens))
        print("utts_corefs size", len(self.utts_corefs))
        print("utts_speakers size", len(self.utts_speakers))
        print("utts_doc_idx size", len(self.utts_doc_idx))
        print(" Building docs")
        for name, part in self.docs_names:
            self.docs.append(
                ConllDoc(
                    name=name,
                    part=part,
                    nlp=None,
                    blacklist=self.blacklist,
                    consider_speakers=True,
                    embedding_extractor=self.embed_extractor,
                    conll=CONLL_GENRES[name[:2]],
                )
            )
        print(" Loading spacy model")

        if model is None:
            model_options = ["en_core_web_lg", "en_core_web_md", "en_core_web_sm", "en"]
            for model_option in model_options:
                if not model:
                    try:
                        spacy.info(model_option)
                        model = model_option
                        print("Loading model", model_option)
                    except:
                        print("Could not detect model", model_option)
            if not model:
                print("Could not detect any suitable English model")
                return
        else:
            spacy.info(model)
            print("Loading model", model)
        nlp = spacy.load(model)
        print(
            " Parsing utterances and filling docs with use_gold_mentions="
            + (str(bool(self.gold_mentions)))
        )
        doc_iter = (s for s in self.utts_text)
        for utt_tuple in tqdm(
            zip(
                nlp.pipe(doc_iter),
                self.utts_tokens,
                self.utts_corefs,
                self.utts_speakers,
                self.utts_doc_idx,
            )
        ):
            spacy_tokens, conll_tokens, corefs, speaker, doc_id = utt_tuple
            if debug:
                print(unicode_(self.docs_names[doc_id]), "-", spacy_tokens)
            doc = spacy_tokens
            if debug:
                out_str = (
                    "utterance "
                    + unicode_(doc)
                    + " corefs "
                    + unicode_(corefs)
                    + " speaker "
                    + unicode_(speaker)
                    + "doc_id"
                    + unicode_(doc_id)
                )
                print(out_str.encode("utf-8"))
            self.docs[doc_id].add_conll_utterance(
                doc, conll_tokens, corefs, speaker, use_gold_mentions=self.gold_mentions
            )

    def build_and_gather_multiple_arrays(self, save_path):
        print(f" Extracting mentions features with {self.n_jobs} job(s)")
        parallel_process(self.docs, set_feats, n_jobs=self.n_jobs)

        print(f" Building and gathering array with {self.n_jobs} job(s)")
        arr = [{"doc": doc, "i": i} for i, doc in enumerate(self.docs)]
        arrays_dicts = parallel_process(
            arr, get_feats, use_kwargs=True, n_jobs=self.n_jobs
        )
        gathering_dict = dict((feat, None) for feat in FEATURES_NAMES)
        n_mentions_list = []
        pairs_ant_index = 0
        pairs_start_index = 0
        for npaidx in tqdm(range(len(arrays_dicts))):
            try:
                n, p, arrays_dict = arrays_dicts[npaidx]
            except:
                # empty array dict, cannot extract the dict values for this doc
                continue

            for f in FEATURES_NAMES:
                if gathering_dict[f] is None:
                    gathering_dict[f] = arrays_dict[f]
                else:
                    if f == FEATURES_NAMES[6]:
                        array = [a + pairs_ant_index for a in arrays_dict[f]]
                    elif f == FEATURES_NAMES[3]:
                        array = [a + pairs_start_index for a in arrays_dict[f]]
                    else:
                        array = arrays_dict[f]
                    gathering_dict[f] += array
            pairs_ant_index += n
            pairs_start_index += p
            n_mentions_list.append(n)

        for feature in FEATURES_NAMES[:9]:
            feature_data = gathering_dict[feature]
            if not feature_data:
                print("No data for", feature)
                continue
            print("Building numpy array for", feature, "length", len(feature_data))
            if feature != "mentions_spans":
                array = np.array(feature_data)
                if array.ndim == 1:
                    array = np.expand_dims(array, axis=1)
            else:
                array = np.stack(feature_data)
            # check_numpy_array(feature, array, n_mentions_list)
            print("Saving numpy", feature, "size", array.shape)
            np.save(save_path + feature, array)
        for feature in FEATURES_NAMES[9:]:
            feature_data = gathering_dict[feature]
            if feature_data:
                print("Saving pickle", feature, "size", len(feature_data))
                with open(save_path + feature + ".bin", "wb") as fp:
                    pickle.dump(feature_data, fp)

    def save_vocabulary(self, save_path, debug=False):
        def _vocabulary_to_file(path, vocabulary):
            print(" Saving vocabulary")
            with io.open(path, "w", encoding="utf-8") as f:
                if debug:
                    print(f"voc saved in {path}, length: {len(vocabulary)}")
                for w in tunable_voc:
                    f.write(w + "\n")

        print(" Building tunable vocabulary matrix from static vocabulary")
        tunable_voc = self.embed_extractor.tun_voc
        _vocabulary_to_file(
            path=save_path + "tuned_word_vocabulary.txt", vocabulary=tunable_voc
        )

        static_voc = self.embed_extractor.stat_voc
        _vocabulary_to_file(
            path=save_path + "static_word_vocabulary.txt", vocabulary=static_voc
        )

        tuned_word_embeddings = np.vstack(
            [self.embed_extractor.get_stat_word(w)[1] for w in tunable_voc]
        )
        print("Saving tunable voc, size:", tuned_word_embeddings.shape)
        np.save(save_path + "tuned_word_embeddings", tuned_word_embeddings)

        static_word_embeddings = np.vstack(
            [self.embed_extractor.static_embeddings[w] for w in static_voc]
        )
        print("Saving static voc, size:", static_word_embeddings.shape)
        np.save(save_path + "static_word_embeddings", static_word_embeddings)


if __name__ == "__main__":
    DIR_PATH = os.path.dirname(os.path.realpath(__file__))
    parser = argparse.ArgumentParser(
        description="Training the neural coreference model"
    )
    parser.add_argument(
        "--function",
        type=str,
        default="all",
        help='Function ("all", "key", "parse", "find_undetected")',
    )
    parser.add_argument(
        "--path", type=str, default=DIR_PATH + "/data/", help="Path to the dataset"
    )
    parser.add_argument(
        "--key", type=str, help="Path to an optional key file for scoring"
    )
    parser.add_argument(
        "--n_jobs", type=int, default=1, help="Number of parallel jobs (default 1)"
    )
    parser.add_argument(
        "--gold_mentions",
        type=int,
        default=0,
        help="Use gold mentions (1) or not (0, default)",
    )
    parser.add_argument(
        "--blacklist", type=int, default=0, help="Use blacklist (1) or not (0, default)"
    )
    parser.add_argument("--spacy_model", type=str, default=None, help="model name")
    args = parser.parse_args()
    if args.key is None:
        args.key = args.path + "/key.txt"
    CORPUS = ConllCorpus(
        n_jobs=args.n_jobs, gold_mentions=args.gold_mentions, blacklist=args.blacklist
    )
    if args.function == "parse" or args.function == "all":
        SAVE_DIR = args.path + "/numpy/"
        if not os.path.exists(SAVE_DIR):
            os.makedirs(SAVE_DIR)
        else:
            if os.listdir(SAVE_DIR):
                print("There are already data in", SAVE_DIR)
                print("Erasing")
                for file in os.listdir(SAVE_DIR):
                    print(file)
                    os.remove(SAVE_DIR + file)
        start_time = time.time()
        CORPUS.read_corpus(args.path, model=args.spacy_model)
        print("=> read_corpus time elapsed", time.time() - start_time)
        if not CORPUS.docs:
            print("Could not parse any valid docs")
        else:
            start_time2 = time.time()
            CORPUS.build_and_gather_multiple_arrays(SAVE_DIR)
            print(
                "=> build_and_gather_multiple_arrays time elapsed",
                time.time() - start_time2,
            )
            start_time2 = time.time()
            CORPUS.save_vocabulary(SAVE_DIR)
            print("=> save_vocabulary time elapsed", time.time() - start_time2)
            print("=> total time elapsed", time.time() - start_time)
    if args.function == "key" or args.function == "all":
        CORPUS.build_key_file(args.path, args.key)
    if args.function == "find_undetected":
        CORPUS.list_undetected_mentions(
            args.path, args.path + "/undetected_mentions.txt"
        )

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/compat.py
import sys

is_windows = sys.platform.startswith("win")
is_linux = sys.platform.startswith("linux")
is_osx = sys.platform == "darwin"


# Python 3 is default, Python 2 is not supported anymore
unicode_ = str
bytes_ = bytes
string_types = (bytes, str)
chr_ = chr


def unicode_to_bytes(s, encoding="utf8", errors="strict"):
    return s.encode(encoding=encoding, errors=errors)


def bytes_to_unicode(b, encoding="utf8", errors="strict"):
    return b.decode(encoding=encoding, errors=errors)

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/__init__.py

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/model.py
"""Conll training algorithm"""

import os
import numpy as np

import torch
import torch.nn as nn
import torch.utils.data


class Model(nn.Module):
    def __init__(
        self, vocab_size, embedding_dim, H1, H2, H3, D_pair_in, D_single_in, dropout=0.5
    ):
        super(Model, self).__init__()
        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)
        self.drop = nn.Dropout(dropout)
        self.pair_top = nn.Sequential(
            nn.Linear(D_pair_in, H1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(H1, H2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(H2, H3),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(H3, 1),
            nn.Linear(1, 1),
        )
        self.single_top = nn.Sequential(
            nn.Linear(D_single_in, H1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(H1, H2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(H2, H3),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(H3, 1),
            nn.Linear(1, 1),
        )
        self.init_weights()

    def init_weights(self):
        w = (param.data for name, param in self.named_parameters() if "weight" in name)
        b = (param.data for name, param in self.named_parameters() if "bias" in name)
        nn.init.uniform_(self.word_embeds.weight.data, a=-0.5, b=0.5)
        for t in w:
            nn.init.xavier_uniform_(t)
        for t in b:
            nn.init.constant_(t, 0)

    def load_embeddings(self, preloaded_weights):
        self.word_embeds.weight = nn.Parameter(preloaded_weights)

    def load_weights(self, weights_path):
        print("Loading weights")
        single_layers_weights, single_layers_biases = [], []
        for f in sorted(os.listdir(weights_path)):
            if f.startswith("single_mention_weights"):
                single_layers_weights.append(np.load(os.path.join(weights_path, f)))
            if f.startswith("single_mention_bias"):
                single_layers_biases.append(np.load(os.path.join(weights_path, f)))
        top_single_linear = (
            layer for layer in self.single_top if isinstance(layer, nn.Linear)
        )
        for w, b, layer in zip(
            single_layers_weights, single_layers_biases, top_single_linear
        ):
            layer.weight = nn.Parameter(torch.from_numpy(w).float())
            layer.bias = nn.Parameter(torch.from_numpy(b).float().squeeze())
        pair_layers_weights, pair_layers_biases = [], []
        for f in sorted(os.listdir(weights_path)):
            if f.startswith("pair_mentions_weights"):
                pair_layers_weights.append(np.load(os.path.join(weights_path, f)))
            if f.startswith("pair_mentions_bias"):
                pair_layers_biases.append(np.load(os.path.join(weights_path, f)))
        top_pair_linear = (
            layer for layer in self.pair_top if isinstance(layer, nn.Linear)
        )
        for w, b, layer in zip(
            pair_layers_weights, pair_layers_biases, top_pair_linear
        ):
            layer.weight = nn.Parameter(torch.from_numpy(w).float())
            layer.bias = nn.Parameter(torch.from_numpy(b).float().squeeze())

    def forward(self, inputs, concat_axis=1):
        pairs = len(inputs) == 8
        if pairs:
            spans, words, single_features, ant_spans, ant_words, ana_spans, ana_words, pair_features = (
                inputs
            )
        else:
            spans, words, single_features = inputs
        words = words.type(torch.LongTensor)
        if torch.cuda.is_available():
            words = words.cuda()
        embed_words = self.drop(self.word_embeds(words).view(words.size()[0], -1))
        single_input = torch.cat([spans, embed_words, single_features], 1)
        single_scores = self.single_top(single_input)
        if pairs:
            batchsize, pairs_num, _ = ana_spans.size()
            ant_words_long = ant_words.view(batchsize, -1).type(torch.LongTensor)
            ana_words_long = ana_words.view(batchsize, -1).type(torch.LongTensor)
            if torch.cuda.is_available():
                ant_words_long = ant_words_long.cuda()
                ana_words_long = ana_words_long.cuda()
            ant_embed_words = self.drop(
                self.word_embeds(ant_words_long).view(batchsize, pairs_num, -1)
            )
            ana_embed_words = self.drop(
                self.word_embeds(ana_words_long).view(batchsize, pairs_num, -1)
            )
            pair_input = torch.cat(
                [ant_spans, ant_embed_words, ana_spans, ana_embed_words, pair_features],
                2,
            )
            pair_scores = self.pair_top(pair_input).squeeze(dim=2)
            total_scores = torch.cat([pair_scores, single_scores], concat_axis)
        return total_scores if pairs else single_scores

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/dataset.py
"""Conll training algorithm"""

import os
import io
import numpy as np

import torch
import torch.utils.data

from torch.utils.data.sampler import Sampler
from torch.utils.data import Dataset

from neuralcoref.train.utils import (
    encode_distance,
    BATCH_SIZE_PATH,
    SIZE_FP,
    SIZE_FP_COMPRESSED,
    SIZE_FS,
    SIZE_FS_COMPRESSED,
    SIZE_GENRE,
    SIZE_PAIR_IN,
    SIZE_SINGLE_IN,
)
from neuralcoref.train.conllparser import FEATURES_NAMES


def load_embeddings_from_file(name):
    print("loading", name + "_embeddings.npy")
    embed = torch.from_numpy(np.load(name + "_embeddings.npy")).float()
    print(embed.size())
    print("loading", name + "_vocabulary.txt")
    with io.open(name + "_vocabulary.txt", "r", encoding="utf-8") as f:
        voc = [line.strip() for line in f]
    return embed, voc


class _DictionaryDataLoader(object):
    def __init__(self, dict_object, order):
        self.dict_object = dict_object
        self.order = order

    def __len__(self):
        return len(self.dict_object[self.order[0]])

    def __getitem__(self, idx):
        if isinstance(idx, slice):
            data = []
            for i in range(
                idx.start, idx.stop, idx.step if idx.step is not None else 1
            ):
                temp_data = []
                for key in self.order:
                    temp_data.append(self.dict_object[key][i])
                data.append(temp_data)

        else:
            data = []
            for key in self.order:
                data.append(self.dict_object[key][idx])

        return data


class NCDataset(Dataset):
    def __init__(self, data_path, params, no_targets=False):
        print(" Loading Dataset at", data_path)
        self.costs = params.costs
        self.no_targets = no_targets
        # Load files
        datas = {}
        if not os.listdir(data_path):
            raise ValueError("Empty data_path")
        numpy_files_found = False
        print("Reading ", end="")
        for file_name in os.listdir(data_path):
            if not ".npy" in file_name:
                continue
            numpy_files_found = True
            print(file_name, end=", ")
            datas[file_name.split(".")[0]] = np.load(
                data_path + file_name, mmap_mode="r" if params.lazy else None
            )
        if not numpy_files_found:
            raise ValueError(f"Can't find numpy files in {data_path}")

        # Gather arrays in two lists of tuples for mention and pairs
        if not params.lazy:
            self.mentions = list(
                zip(
                    *(
                        arr
                        for key, arr in sorted(datas.items())
                        if key.startswith("mentions")
                    )
                )
            )
            self.pairs = list(
                zip(
                    *(
                        arr
                        for key, arr in sorted(datas.items())
                        if key.startswith("pairs")
                    )
                )
            )
        else:
            self.mentions = _DictionaryDataLoader(
                datas,
                order=(
                    "mentions_features",
                    "mentions_labels",
                    "mentions_pairs_length",
                    "mentions_pairs_start_index",
                    "mentions_spans",
                    "mentions_words",
                ),
            )
            self.pairs = _DictionaryDataLoader(
                datas, order=("pairs_ant_index", "pairs_features", "pairs_labels")
            )

        self.mentions_pair_length = datas[FEATURES_NAMES[2]]
        assert [arr.shape[0] for arr in self.mentions[0]] == [
            6,
            1,
            1,
            1,
            250,
            8,
        ]  # Cf order of FEATURES_NAMES in conllparser.py
        assert [arr.shape[0] for arr in self.pairs[0]] == [
            1,
            9,
            1,
        ]  # Cf order of FEATURES_NAMES in conllparser.py

    def __len__(self):
        return len(self.mentions)

    def __getitem__(self, mention_idx, debug=False):
        """
        Return:
            Definitions:
                P is the number of antecedent per mention (number of pairs for the mention)
                S = 250 is the size of the span vector (averaged word embeddings)
                W = 8 is the number of words in a mention (tuned embeddings)
                Fp = 70 is the number of features for a pair of mention
                Fs = 24 is the number of features of a single mention

            if there are some pairs:
                inputs = (spans, words, features, ant_spans, ant_words, ana_spans, ana_words, pairs_features)
                targets = (labels, costs, true_ants, false_ants)
            else:
                inputs = (spans, words, features)
                targets = (labels, costs, true_ants)

            inputs: Tuple of
                spans => (S,)
                words => (W,)
                features => (Fs,)
                + if there are potential antecedents (P > 0):
                    ant_spans => (P, S) or nothing if no pairs
                    ant_words => (P, W) or nothing if no pairs
                    ana_spans => (P, S) or nothing if no pairs
                    ana_words => (P, W) or nothing if no pairs
                    pair_features => (P, Fp) or nothing if no pairs

            targets: Tuple of
                labels => (P+1,)
                costs => (P+1,)
                true_ant => (P+1,)
                + if there are potential antecedents (P > 0):
                    false_ant => (P+1,)

        """
        features_raw, label, pairs_length, pairs_start_index, spans, words = self.mentions[
            mention_idx
        ]
        pairs_start_index = pairs_start_index.item()
        pairs_length = pairs_length.item()

        # Build features array (float) from raw features (int)
        assert features_raw.shape[0] == SIZE_FS_COMPRESSED
        features = np.zeros((SIZE_FS,))
        features[features_raw[0]] = 1
        features[4:15] = encode_distance(features_raw[1])
        features[15] = features_raw[2].astype(float) / features_raw[3].astype(float)
        features[16] = features_raw[4]
        features[features_raw[5] + 17] = 1

        if pairs_length == 0:
            spans = torch.from_numpy(spans).float()
            words = torch.from_numpy(words)
            features = torch.from_numpy(features).float()
            inputs = (spans, words, features)
            if self.no_targets:
                return inputs
            true_ant = torch.zeros(1).long()  # zeros = indices of true ant
            costs = torch.from_numpy((1 - label) * self.costs["FN"]).float()
            label = torch.from_numpy(label).float()
            targets = (label, costs, true_ant)
            if debug:
                print("inputs shapes: ", [a.size() for a in inputs])
                print("targets shapes: ", [a.size() for a in targets])
            return inputs, targets

        start = pairs_start_index
        end = pairs_start_index + pairs_length
        pairs = self.pairs[start:end]
        assert len(pairs) == pairs_length
        assert (
            len(pairs[0]) == 3
        )  # pair[i] = (pairs_ant_index, pairs_features, pairs_labels)
        pairs_ant_index, pairs_features_raw, pairs_labels = list(zip(*pairs))

        pairs_features_raw = np.stack(pairs_features_raw)
        pairs_labels = np.squeeze(np.stack(pairs_labels), axis=1)

        # Build pair features array (float) from raw features (int)
        assert pairs_features_raw[0, :].shape[0] == SIZE_FP_COMPRESSED
        pairs_features = np.zeros((len(pairs_ant_index), SIZE_FP))
        pairs_features[:, 0:6] = pairs_features_raw[:, 0:6]
        pairs_features[:, 6:17] = encode_distance(pairs_features_raw[:, 6])
        pairs_features[:, 17:28] = encode_distance(pairs_features_raw[:, 7])
        pairs_features[:, 28] = pairs_features_raw[:, 8]
        # prepare antecent features
        ant_features_raw = np.concatenate(
            [self.mentions[idx.item()][0][np.newaxis, :] for idx in pairs_ant_index]
        )
        ant_features = np.zeros((pairs_length, SIZE_FS - SIZE_GENRE))
        ant_features[:, ant_features_raw[:, 0]] = 1
        ant_features[:, 4:15] = encode_distance(ant_features_raw[:, 1])
        ant_features[:, 15] = ant_features_raw[:, 2].astype(float) / ant_features_raw[
            :, 3
        ].astype(float)
        ant_features[:, 16] = ant_features_raw[:, 4]
        pairs_features[:, 29:46] = ant_features
        # Here we keep the genre
        ana_features = np.tile(features, (pairs_length, 1))
        pairs_features[:, 46:] = ana_features

        ant_spans = np.concatenate(
            [self.mentions[idx.item()][4][np.newaxis, :] for idx in pairs_ant_index]
        )
        ant_words = np.concatenate(
            [self.mentions[idx.item()][5][np.newaxis, :] for idx in pairs_ant_index]
        )
        ana_spans = np.tile(spans, (pairs_length, 1))
        ana_words = np.tile(words, (pairs_length, 1))
        ant_spans = torch.from_numpy(ant_spans).float()
        ant_words = torch.from_numpy(ant_words)
        ana_spans = torch.from_numpy(ana_spans).float()
        ana_words = torch.from_numpy(ana_words)
        pairs_features = torch.from_numpy(pairs_features).float()

        labels_stack = np.concatenate((pairs_labels, label), axis=0)
        assert labels_stack.shape == (pairs_length + 1,)
        labels = torch.from_numpy(labels_stack).float()

        spans = torch.from_numpy(spans).float()
        words = torch.from_numpy(words)
        features = torch.from_numpy(features).float()

        inputs = (
            spans,
            words,
            features,
            ant_spans,
            ant_words,
            ana_spans,
            ana_words,
            pairs_features,
        )

        if self.no_targets:
            return inputs

        if label == 0:
            costs = np.concatenate(
                (self.costs["WL"] * (1 - pairs_labels), [self.costs["FN"]])
            )  # Inverse labels: 1=>0, 0=>1
        else:
            costs = np.concatenate((self.costs["FL"] * np.ones_like(pairs_labels), [0]))
        assert costs.shape == (pairs_length + 1,)
        costs = torch.from_numpy(costs).float()

        true_ants_unpad = np.flatnonzero(labels_stack)
        if len(true_ants_unpad) == 0:
            raise ValueError("Error: no True antecedent for mention")
        true_ants = np.pad(
            true_ants_unpad, (0, len(pairs_labels) + 1 - len(true_ants_unpad)), "edge"
        )
        assert true_ants.shape == (pairs_length + 1,)
        true_ants = torch.from_numpy(true_ants).long()

        false_ants_unpad = np.flatnonzero(1 - labels_stack)
        assert len(false_ants_unpad) != 0
        false_ants = np.pad(
            false_ants_unpad, (0, len(pairs_labels) + 1 - len(false_ants_unpad)), "edge"
        )
        assert false_ants.shape == (pairs_length + 1,)
        false_ants = torch.from_numpy(false_ants).long()

        targets = (labels, costs, true_ants, false_ants)
        if debug:
            print("Mention", mention_idx)
            print("inputs shapes: ", [a.size() for a in inputs])
            print("targets shapes: ", [a.size() for a in targets])
        return inputs, targets


class NCBatchSampler(Sampler):
    """A Batch sampler to group mentions in batches with close number of pairs to be padded together
    """

    def __init__(
        self, mentions_pairs_length, batchsize=600, shuffle=False, debug=False
    ):
        """ Create and feed batches of mentions having close number of antecedents
            The batch are padded and collated by the padder_collate function

        # Arguments:
            mentions_pairs_length array of shape (N, 1): list/array of the number of pairs for each mention
            batchsize: Number of pairs of each batch will be capped at this
        """
        self.shuffle = shuffle
        num_mentions = len(mentions_pairs_length)
        mentions_lengths = np.concatenate(
            [
                mentions_pairs_length,
                np.arange(0, num_mentions, 1, dtype=int)[:, np.newaxis],
            ],
            axis=1,
        )
        sorted_lengths = mentions_lengths[mentions_lengths[:, 0].argsort()]
        print("Preparing batches ")

        self.batches = []
        self.batches_pairs = []
        self.batches_size = []
        batch = []
        n_pairs = []
        num = 0
        for length, mention_idx in sorted_lengths:
            if num > batchsize or (
                num == len(batch) and length != 0
            ):  # We keep the no_pairs batches pure
                if debug:
                    print(
                        "Added batch number",
                        len(self.batches),
                        "with",
                        len(batch),
                        "mentions and",
                        num,
                        "pairs",
                    )
                self.batches.append(batch)
                self.batches_size.append(
                    num
                )  # We don't count the max 7 additional mentions that are repeated
                self.batches_pairs.append(n_pairs)

                # Start a new batch
                batch = [mention_idx]
                n_pairs = [length]
                num = (
                    length + 1
                )  # +1 since we also have the single mention to add to the number of pairs
            else:
                num += length + 1
                batch.append(mention_idx)
                n_pairs.append(length)

        # Complete and store the last batch
        if debug:
            print(
                "Added batch number",
                len(self.batches),
                "with",
                len(batch),
                "mentions and",
                num,
                "pairs",
            )
        self.batches.append(batch)
        self.batches_size.append(num)
        self.batches_pairs.append(n_pairs)
        self.n_pairs = sum(sum(p) for p in self.batches_pairs)
        self.n_mentions = sum(len(b) for b in self.batches)
        self.n_batches = len(self.batches)
        self.pairs_per_batch = float(self.n_pairs) / self.n_batches
        self.mentions_per_batch = float(self.n_mentions) / self.n_batches
        print(
            "Dataset has:",
            self.n_batches,
            "batches,",
            self.n_mentions,
            "mentions,",
            self.n_pairs,
            "pairs",
        )

    def get_batch_info(self):
        return self.batches, self.batches_pairs

    def save_batch_sizes(self, save_file=BATCH_SIZE_PATH, debug=False):
        print(" Saving sizes of batches")
        with io.open(save_file, "w", encoding="utf-8") as f:
            if debug:
                print("Batch sizes saved in", save_file)
            for batch, size in zip(self.batches, self.batches_size):
                out_str = str(len(batch)) + "\t" + str(size) + "\n"
                f.write(out_str)

    def __iter__(self):
        if self.shuffle:
            np.random.shuffle(self.batches)
        for batch in self.batches:
            yield batch

    def __len__(self):
        return self.n_batches


def padder_collate(batch, debug=False):
    """ Puts each data field into a tensor with outer dimension batch size
        Pad variable length input tensors and add a weight tensor to the target
    """
    transposed_inputs = tuple(zip(*batch))
    if len(transposed_inputs) == 2:
        inputs, targets = transposed_inputs
        transposed_inputs = tuple(zip(*inputs))
        transposed_targets = tuple(zip(*targets))
    else:
        transposed_targets = None

    max_pairs = (
        max(len(t) for t in transposed_inputs[3]) if len(transposed_inputs) == 8 else 0
    )  # Get max nb of pairs (batch are sorted by nb of pairs)
    if max_pairs > 0:
        out_inputs = []
        out_targets = []
        for t_inp in transposed_inputs:
            if len(t_inp[0].shape) == 2:
                out_inputs.append(
                    torch.stack(
                        [
                            torch.cat([t, t.new(max_pairs - len(t), len(t[0])).zero_()])
                            if len(t) != max_pairs
                            else t
                            for t in t_inp
                        ],
                        0,
                    )
                )
            else:
                out_inputs.append(torch.stack(t_inp, 0))
        if transposed_targets is not None:
            for i, t_targ in enumerate(
                transposed_targets
            ):  # 0:labels, 1:costs, 2:true_ants, 3:false_ants
                if i == 2 or i == 3:
                    if debug:
                        print("collate before", t_targ)
                    # shift the antecedent index associated to single anaphores (last)
                    t_targ = tuple(
                        t.masked_fill_(torch.eq(t, len(t) - 1), max_pairs)
                        for t in t_targ
                    )
                    if debug:
                        print("collate after", t_targ)
                out_targets.append(
                    torch.stack(
                        [
                            torch.cat(
                                [
                                    t[:-1] if len(t) > 2 else t.new(1).fill_(t[0]),
                                    t.new(max_pairs + 1 - len(t)).fill_(t[0]),
                                    t.new(1).fill_(t[-1]),
                                ]
                            )
                            if len(t) != max_pairs + 1
                            else t
                            for t in t_targ
                        ],
                        0,
                    )
                )

            t_costs = transposed_targets[
                1
            ]  # We build the weights from the costs to have a float Tensor
            out_targets.append(
                torch.stack(
                    [
                        torch.cat(
                            [
                                t.new(len(t) - 1).fill_(1),
                                t.new(max_pairs + 1 - len(t)).zero_(),
                                t.new(1).fill_(1),
                            ]
                        )
                        if len(t) != max_pairs + 1
                        else t.new(max_pairs + 1).fill_(1)
                        for t in t_costs
                    ],
                    0,
                )
            )
        else:
            # Remark this mask is the inverse of the weights in the above target (used for evaluation masking)
            t_base = transposed_inputs[3]
            out_targets = torch.stack(
                [
                    torch.cat(
                        [
                            t.new(len(t) - 1).zero_().bool(),
                            t.new(max_pairs + 1 - len(t)).fill_(1).bool(),
                            t.new(1).zero_().bool(),
                        ]
                    )
                    if len(t) != max_pairs + 1
                    else t.new(max_pairs + 1).zero_().bool()
                    for t in t_base
                ],
                0,
            )
    else:
        out_inputs = [torch.stack(t_inp, 0) for t_inp in transposed_inputs]
        if transposed_targets is not None:
            out_targets = [torch.stack(t_targ, 0) for t_targ in transposed_targets]
            out_targets.append(out_targets[1].new(len(out_targets[1]), 1).fill_(1))
        else:
            out_targets = out_inputs[0].new(len(out_inputs[0]), 1).zero_().bool()
    return (out_inputs, out_targets)

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/utils.py
"""Utils"""


from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import numpy as np
from tqdm import tqdm

PACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))
BATCH_SIZE_PATH = os.path.join(
    PACKAGE_DIRECTORY, "test_batch_size.txt"
)  # fernandes.txt")#

SIZE_SPAN = 250  # size of the span vector (averaged word embeddings)
SIZE_WORD = 8  # number of words in a mention (tuned embeddings)
SIZE_EMBEDDING = 50  # size of the words embeddings
SIZE_FP = 70  # number of features for a pair of mention
SIZE_FP_COMPRESSED = (
    9
)  # size of the features for a pair of mentions as stored in numpy arrays
SIZE_FS = 24  # number of features of a single mention
SIZE_FS_COMPRESSED = 6  # size of the features for a mention as stored in numpy arrays
SIZE_GENRE = 7  # Size of the genre one-hot array
SIZE_MENTION_EMBEDDING = (
    SIZE_SPAN + SIZE_WORD * SIZE_EMBEDDING
)  # A mention embeddings (span + words vectors)
SIZE_SNGL_FEATS = SIZE_FS - SIZE_GENRE
SIZE_PAIR_FEATS = SIZE_FP - SIZE_GENRE
SIZE_SNGL_IN_NO_GENRE = SIZE_MENTION_EMBEDDING + SIZE_SNGL_FEATS
SIZE_PAIR_IN_NO_GENRE = 2 * SIZE_MENTION_EMBEDDING + SIZE_PAIR_FEATS

SIZE_PAIR_IN = (
    2 * SIZE_MENTION_EMBEDDING + SIZE_FP
)  # Input to the mentions pair neural network
SIZE_SINGLE_IN = (
    SIZE_MENTION_EMBEDDING + SIZE_FS
)  # Input to the single mention neural network

DISTANCE_BINS = list(range(5)) + [5] * 3 + [6] * 8 + [7] * 16 + [8] * 32
BINS_NUM = float(len(DISTANCE_BINS))
MAX_BINS = DISTANCE_BINS[-1] + 1


def encode_distance(x):
    """ Encode an integer or an array of integers as a (bined) one-hot numpy array """

    def _encode_distance(d):
        """ Encode an integer as a (bined) one-hot numpy array """
        dist_vect = np.zeros((11,), dtype="float32")
        if d < 64:
            dist_vect[DISTANCE_BINS[d]] = 1
        else:
            dist_vect[9] = 1
        dist_vect[10] = min(float(d), BINS_NUM) / BINS_NUM
        return dist_vect

    if isinstance(x, np.ndarray):
        arr_l = [_encode_distance(y)[np.newaxis, :] for y in x]
        out_arr = np.concatenate(arr_l)
    else:
        out_arr = _encode_distance(x)
    return out_arr


def parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=10):
    """
        A parallel version of the map function with a progress bar. 

        Args:
            array (array-like): An array to iterate over.
            function (function): A python function to apply to the elements of array
            n_jobs (int, default=16): The number of cores to use
            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of 
                keyword arguments to function 
            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. 
                Useful for catching bugs
        Returns:
            [function(array[0]), function(array[1]), ...]
    """
    # We run the first few iterations serially to catch bugs
    if front_num > 0:
        front = [
            function(**a) if use_kwargs else function(a) for a in array[:front_num]
        ]
    else:
        front = []
    # If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.
    if n_jobs == 1:
        return front + [
            function(**a) if use_kwargs else function(a)
            for a in tqdm(array[front_num:])
        ]
    # Assemble the workers
    with ThreadPoolExecutor(max_workers=n_jobs) as pool:
        # Pass the elements of array into function
        if use_kwargs:
            futures = [pool.submit(function, **a) for a in array[front_num:]]
        else:
            futures = [pool.submit(function, a) for a in array[front_num:]]
        kwargs = {
            "total": len(futures),
            "unit": "it",
            "unit_scale": True,
            "leave": True,
        }
        # #Print out the progress as tasks complete
        # for _ in tqdm(as_completed(futures), **kwargs):
        #     pass
    out = []
    # Get the results from the futures.
    for future in futures:  # tqdm(futures):
        try:
            out.append(future.result())
        except Exception as e:
            out.append(e)
    return front + out

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/document.py
"""data models and pre-processing for the coref algorithm"""

import re
import io
from six import string_types, integer_types
from spacy.tokens import Span, Token

from neuralcoref.train.compat import unicode_
from neuralcoref.train.utils import encode_distance, parallel_process

try:
    from itertools import izip_longest as zip_longest
except ImportError:  # will be 3.x series
    from itertools import zip_longest

import spacy
import numpy as np

#########################
####### UTILITIES #######
#########################

NO_COREF_LIST = ["i", "me", "my", "you", "your"]

MENTION_TYPE = {"PRONOMINAL": 0, "NOMINAL": 1, "PROPER": 2, "LIST": 3}
MENTION_LABEL = {0: "PRONOMINAL", 1: "NOMINAL", 2: "PROPER", 3: "LIST"}

PROPERS_TAGS = ["NN", "NNS", "NNP", "NNPS"]
ACCEPTED_ENTS = [
    "PERSON",
    "NORP",
    "FACILITY",
    "ORG",
    "GPE",
    "LOC",
    "PRODUCT",
    "EVENT",
    "WORK_OF_ART",
    "LANGUAGE",
]
WHITESPACE_PATTERN = r"\s+|_+"
UNKNOWN_WORD = "*UNK*"
MISSING_WORD = "<missing>"
MAX_ITER = 100

#########################
## MENTION EXTRACTION ###
#########################


def extract_mentions_spans(doc, blacklist, debug=False):
    """
    Extract potential mentions from a spacy parsed Doc
    """
    if debug:
        print("===== doc ====:", doc)
    for c in doc:
        if debug:
            print(
                " span search:",
                c,
                "head:",
                c.head,
                "tag:",
                c.tag_,
                "pos:",
                c.pos_,
                "dep:",
                c.dep_,
            )
    # Named entities
    mentions_spans = list(ent for ent in doc.ents if ent.label_ in ACCEPTED_ENTS)

    if debug:
        print("==-- ents:", list(((ent, ent.label_) for ent in mentions_spans)))
    for spans in parallel_process(
        [{"doc": doc, "span": sent, "blacklist": blacklist} for sent in doc.sents],
        _extract_from_sent,
        use_kwargs=True,
        front_num=0,
    ):
        mentions_spans = mentions_spans + spans
    spans_set = set()
    cleaned_mentions_spans = []
    for spans in mentions_spans:
        if spans.end > spans.start and (spans.start, spans.end) not in spans_set:
            cleaned_mentions_spans.append(spans)
            spans_set.add((spans.start, spans.end))

    return cleaned_mentions_spans


def _extract_from_sent(doc, span, blacklist=True, debug=False):
    """
    Extract Pronouns and Noun phrases mentions from a spacy Span
    """
    keep_tags = re.compile(r"N.*|PRP.*|DT|IN")
    leave_dep = ["det", "compound", "appos"]
    keep_dep = ["nsubj", "dobj", "iobj", "pobj"]
    nsubj_or_dep = ["nsubj", "dep"]
    conj_or_prep = ["conj", "prep"]
    remove_pos = ["CCONJ", "SCONJ", "INTJ", "ADP"]
    lower_not_end = ["'s", ",", ".", "!", "?", ":", ";"]

    # Utility to remove bad endings
    def cleanup_endings(left, right, token):
        minchild_idx = min(left + [token.i]) if left else token.i
        maxchild_idx = max(right + [token.i]) if right else token.i
        # Clean up endings and begginging
        while maxchild_idx >= minchild_idx and (
            doc[maxchild_idx].pos_ in remove_pos
            or doc[maxchild_idx].lower_ in lower_not_end
        ):
            if debug:
                print(
                    "Removing last token",
                    doc[maxchild_idx].lower_,
                    doc[maxchild_idx].tag_,
                )
            maxchild_idx -= (
                1
            )  # We don't want mentions finishing with 's or conjunctions/punctuation
        while minchild_idx <= maxchild_idx and (
            doc[minchild_idx].pos_ in remove_pos
            or doc[minchild_idx].lower_ in lower_not_end
        ):
            if debug:
                print(
                    "Removing first token",
                    doc[minchild_idx].lower_,
                    doc[minchild_idx].tag_,
                )
            minchild_idx += (
                1
            )  # We don't want mentions starting with 's or conjunctions/punctuation
        return minchild_idx, maxchild_idx + 1

    mentions_spans = []
    for token in span:
        if debug:
            print(
                " tok:",
                token,
                "tok.tag_:",
                token.tag_,
                "tok.pos_:",
                token.pos_,
                "tok.dep_:",
                token.dep_,
            )

        if blacklist and token.lower_ in NO_COREF_LIST:
            if debug:
                print("token in no_coref_list")
            continue
        if (
            not keep_tags.match(token.tag_) or token.dep_ in leave_dep
        ) and not token.dep_ in keep_dep:
            if debug:
                print("not pronoun or no right dependency")
            continue

        # pronoun
        if re.match(r"PRP.*", token.tag_):
            if debug:
                print("PRP")
            endIdx = token.i + 1

            span = doc[token.i : endIdx]
            if debug:
                print("==-- PRP store:", span)
            mentions_spans.append(span)

            # when pronoun is a part of conjunction (e.g., you and I)
            if token.n_rights > 0 or token.n_lefts > 0:
                span = doc[token.left_edge.i : token.right_edge.i + 1]
                if debug:
                    print("==-- in conj store:", span)
                mentions_spans.append(span)
            continue

        # Add NP mention
        if debug:
            print("NP or IN:", token.lower_)
            if token.tag_ == "IN":
                print("IN tag")
        # Take care of 's
        if token.lower_ == "'s":
            if debug:
                print("'s detected")
            h = token.head
            j = 0
            while h.head.i != h.i and j < MAX_ITER:
                if debug:
                    print("token head:", h, h.dep_, "head:", h.head)
                    print(id(h.head), id(h))
                if h.dep_ == "nsubj":
                    minchild_idx = min(
                        (
                            c.left_edge.i
                            for c in doc
                            if c.head.i == h.head.i and c.dep_ in nsubj_or_dep
                        ),
                        default=token.i,
                    )
                    maxchild_idx = max(
                        (
                            c.right_edge.i
                            for c in doc
                            if c.head.i == h.head.i and c.dep_ in nsubj_or_dep
                        ),
                        default=token.i,
                    )
                    if debug:
                        print("'s', i1:", doc[minchild_idx], " i2:", doc[maxchild_idx])
                    span = doc[minchild_idx : maxchild_idx + 1]
                    if debug:
                        print("==-- 's' store:", span)
                    mentions_spans.append(span)
                    break
                h = h.head
                j += 1
            assert j != MAX_ITER
            continue

        # clean up
        for c in doc:
            if debug and c.head.i == token.i:
                print(" token in span:", c, "- head & dep:", c.head, c.dep_)
        left = list(c.left_edge.i for c in doc if c.head.i == token.i)
        right = list(c.right_edge.i for c in doc if c.head.i == token.i)
        if (
            token.tag_ == "IN"
            and token.dep_ == "mark"
            and len(left) == 0
            and len(right) == 0
        ):
            left = list(c.left_edge.i for c in doc if c.head.i == token.head.i)
            right = list(c.right_edge.i for c in doc if c.head.i == token.head.i)
        if debug:
            print("left side:", left)
            print("right side:", right)
            minchild_idx = min(left) if left else token.i
            maxchild_idx = max(right) if right else token.i
            print("full span:", doc[minchild_idx : maxchild_idx + 1])
        start, end = cleanup_endings(left, right, token)
        if start == end:
            continue
        if doc[start].lower_ == "'s":
            continue  # we probably already have stored this mention
        span = doc[start:end]
        if debug:
            print("cleaned endings span:", doc[start:end])
            print("==-- full span store:", span)
        mentions_spans.append(span)
        if debug and token.tag_ == "IN":
            print("IN tag")
        if any(tok.dep_ in conj_or_prep for tok in span):
            if debug:
                print("Conjunction found, storing first element separately")
            for c in doc:
                if c.head.i == token.i and c.dep_ not in conj_or_prep:
                    if debug:
                        print("left no conj:", c, "dep & edge:", c.dep_, c.left_edge)
                    if debug:
                        print("right no conj:", c, "dep & edge:", c.dep_, c.right_edge)
            left_no_conj = list(
                c.left_edge.i
                for c in doc
                if c.head.i == token.i and c.dep_ not in conj_or_prep
            )
            right_no_conj = list(
                c.right_edge.i
                for c in doc
                if c.head.i == token.i and c.dep_ not in conj_or_prep
            )
            if debug:
                print("left side no conj:", [doc[i] for i in left_no_conj])
            if debug:
                print("right side no conj:", [doc[i] for i in right_no_conj])
            start, end = cleanup_endings(left_no_conj, right_no_conj, token)
            if start == end:
                continue
            span = doc[start:end]
            if debug:
                print("==-- full span store:", span)
            mentions_spans.append(span)
    if debug:
        print("mentions_spans inside", mentions_spans)
    return mentions_spans


#########################
####### CLASSES #########


class Mention(spacy.tokens.Span):
    """
    A mention (possible anaphor) inherite from spacy Span class with additional informations
    """

    def __new__(
        cls,
        span,
        mention_index,
        utterance_index,
        utterance_start_sent,
        speaker=None,
        gold_label=None,
        *args,
        **kwargs,
    ):
        # We need to override __new__ see http://cython.readthedocs.io/en/latest/src/userguide/special_methods.html
        obj = spacy.tokens.Span.__new__(
            cls, span.doc, span.start, span.end, *args, **kwargs
        )
        return obj

    def __init__(
        self,
        span,
        mention_index,
        utterance_index,
        utterances_start_sent,
        speaker=None,
        gold_label=None,
    ):
        """
        Arguments:
            span (spaCy Span): the spaCy span from which creating the Mention object
            mention_index (int): index of the Mention in the Document
            utterance_index (int): index of the utterance of the Mention in the Document
            utterances_start_sent (int): index of the first sentence of the utterance of the Mention in the Document
                (an utterance can comprise several sentences)
            speaker (Speaker): the speaker of the mention
            gold_label (anything): a gold label associated to the Mention (for training)
        """
        self.index = mention_index
        self.utterance_index = utterance_index
        self.utterances_sent = utterances_start_sent + self._get_doc_sent_number()
        self.speaker = speaker
        self.gold_label = gold_label
        self.spans_embeddings = None
        self.words_embeddings = None
        self.features = None

        self.features_ = None
        self.spans_embeddings_ = None
        self.words_embeddings_ = None

        self.mention_type = self._get_type()
        self.propers = set(self.content_words)
        self.entity_label = self._get_entity_label()
        self.in_entities = self._get_in_entities()

    def _get_entity_label(self):
        """ Label of a detected named entity the Mention is nested in if any"""
        for ent in self.doc.ents:
            if ent.start <= self.start and self.end <= ent.end:
                return ent.label
        return None

    def _get_in_entities(self):
        """ Is the Mention nested in a detected named entity"""
        return self.entity_label is not None

    def _get_type(self):
        """ Find the type of the Span """
        conj = ["CC", ","]
        prp = ["PRP", "PRP$"]
        proper = ["NNP", "NNPS"]
        if any(t.tag_ in conj and t.ent_type_ not in ACCEPTED_ENTS for t in self):
            mention_type = MENTION_TYPE["LIST"]
        elif self.root.tag_ in prp:
            mention_type = MENTION_TYPE["PRONOMINAL"]
        elif self.root.ent_type_ in ACCEPTED_ENTS or self.root.tag_ in proper:
            mention_type = MENTION_TYPE["PROPER"]
        else:
            mention_type = MENTION_TYPE["NOMINAL"]
        return mention_type

    def _get_doc_sent_number(self):
        """ Index of the sentence of the Mention in the current utterance"""
        for i, s in enumerate(self.doc.sents):
            if s == self.sent:
                return i
        return None

    @property
    def content_words(self):
        """ Returns an iterator of nouns/proper nouns in the Mention """
        return (tok.lower_ for tok in self if tok.tag_ in PROPERS_TAGS)

    @property
    def embedding(self):
        return np.concatenate([self.spans_embeddings, self.words_embeddings], axis=0)

    def heads_agree(self, mention2):
        """ Does the root of the Mention match the root of another Mention/Span"""
        # we allow same-type NEs to not match perfectly,
        # but rather one could be included in the other, e.g., "George" -> "George Bush"
        if (
            self.in_entities
            and mention2.in_entities
            and self.entity_label == mention2.entity_label
            and (
                self.root.lower_ in mention2.lower_
                or mention2.root.lower_ in self.lower_
            )
        ):
            return True
        return self.root.lower_ == mention2.root.lower_

    def exact_match(self, mention2):
        """ Does the Mention lowercase text matches another Mention/Span lowercase text"""
        return self.lower_ == mention2.lower_

    def relaxed_match(self, mention2):
        """ Does the nouns/proper nous in the Mention match another Mention/Span nouns/propers"""
        return not self.propers.isdisjoint(mention2.propers)

    def speaker_match_mention(self, mention2):
        # To take care of sentences like 'Larry said, "San Francisco is a city."': (id(Larry), id(San Francisco))
        # if document.speakerPairs.contains(new Pair<>(mention.mentionID, ant.mentionID)):
        #    return True
        if self.speaker is not None:
            return self.speaker.speaker_matches_mention(mention2, strict_match=False)
        return False


class Speaker(object):
    """
    A speaker with its names, list of mentions and matching test functions
    """

    def __init__(self, speaker_id, speaker_names=None):
        self.mentions = []
        self.speaker_id = speaker_id
        if speaker_names is None:
            self.speaker_names = [unicode_(speaker_id)]
        elif isinstance(speaker_names, string_types):
            self.speaker_names = [speaker_names]
        elif len(speaker_names) > 1:
            self.speaker_names = speaker_names
        else:
            self.speaker_names = unicode_(speaker_names)
        self.speaker_tokens = [
            tok.lower()
            for s in self.speaker_names
            for tok in re.split(WHITESPACE_PATTERN, s)
        ]

    def __str__(self):
        return f"{self.speaker_id} <names> {self.speaker_names}"

    def add_mention(self, mention):
        """ Add a Mention of the Speaker"""
        self.mentions.append(mention)

    def contain_mention(self, mention):
        """ Does the Speaker contains a Mention"""
        return mention in self.mentions

    def contain_string(self, string):
        """ Does the Speaker names contains a string"""
        return any(
            re.sub(WHITESPACE_PATTERN, "", string.lower())
            == re.sub(WHITESPACE_PATTERN, "", s.lower())
            for s in self.speaker_names
        )

    def contain_token(self, token):
        """ Does the Speaker names contains a token (word)"""
        return any(token.lower() == tok.lower() for tok in self.speaker_tokens)

    def speaker_matches_mention(self, mention, strict_match=False):
        """ Does a Mention matches the speaker names"""
        # Got info about this speaker
        if self.contain_mention(mention):
            return True
        if strict_match:
            if self.contain_string(mention):
                self.mentions.append(mention)
                return True
        else:
            # test each token in the speaker names
            if not mention.root.tag_.startswith("NNP"):
                return False
            if self.contain_token(mention.root.lower_):
                self.mentions.append(mention)
                return True
        return False


class EmbeddingExtractor:
    """
    Compute words embedding features for mentions
    """

    def __init__(self, pretrained_model_path):
        _, self.static_embeddings, self.stat_idx, self.stat_voc = self.load_embeddings_from_file(
            pretrained_model_path + "static_word"
        )
        _, self.tuned_embeddings, self.tun_idx, self.tun_voc = self.load_embeddings_from_file(
            pretrained_model_path + "tuned_word"
        )
        self.fallback = self.static_embeddings.get(UNKNOWN_WORD)

        self.shape = self.static_embeddings[UNKNOWN_WORD].shape
        shape2 = self.tuned_embeddings[UNKNOWN_WORD].shape
        assert self.shape == shape2

    @staticmethod
    def load_embeddings_from_file(name):
        print("Loading embeddings from", name)
        embeddings = {}
        voc_to_idx = {}
        idx_to_voc = []
        mat = np.load(name + "_embeddings.npy")
        average_mean = np.average(mat, axis=0, weights=np.sum(mat, axis=1))
        with io.open(name + "_vocabulary.txt", "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                embeddings[line.strip()] = mat[i, :]
                voc_to_idx[line.strip()] = i
                idx_to_voc.append(line.strip())
        return average_mean, embeddings, voc_to_idx, idx_to_voc

    @staticmethod
    def normalize_word(w):
        if w is None:
            return MISSING_WORD
        return re.sub(r"\d", "0", w.lower_)

    def get_document_embedding(self, utterances_list):
        """ Embedding for the document """
        #    We could also use this: embed_vector = np.copy(self.average_mean)#np.zeros(self.shape)
        #    return embed_vector
        embed_vector = np.zeros(self.shape)
        for utt in utterances_list:
            _, utt_embed = self.get_average_embedding(utt)
            embed_vector += utt_embed
        return embed_vector / max(len(utterances_list), 1)

    def get_stat_word(self, word):
        if word in self.static_embeddings:
            return word, self.static_embeddings.get(word)
        else:
            return UNKNOWN_WORD, self.fallback

    def get_word_embedding(self, word, static=False):
        """ Embedding for a single word (tuned if possible, otherwise static) """
        norm_word = self.normalize_word(word)
        if static:
            return self.get_stat_word(norm_word)
        else:
            if norm_word in self.tuned_embeddings:
                return norm_word, self.tuned_embeddings.get(norm_word)
            else:
                return self.get_stat_word(norm_word)

    def get_word_in_sentence(self, word_idx, sentence):
        """ Embedding for a word in a sentence """
        if word_idx < sentence.start or word_idx >= sentence.end:
            return self.get_word_embedding(None)
        return self.get_word_embedding(sentence.doc[word_idx])

    def get_average_embedding(self, token_list):
        """ Embedding for a list of words """
        embed_vector = np.zeros(
            self.shape
        )  # We could also use np.copy(self.average_mean)
        word_list = []
        for tok in token_list:
            if tok.lower_ not in [".", "!", "?"]:
                word, embed = self.get_word_embedding(tok, static=True)
                embed_vector += embed
                word_list.append(word)
        return word_list, (embed_vector / max(len(word_list), 1))

    def get_mention_embeddings(self, mention, doc_embedding):
        """ Get span (averaged) and word (single) embeddings of a mention """
        st = mention.sent
        mention_lefts = mention.doc[max(mention.start - 5, st.start) : mention.start]
        mention_rights = mention.doc[mention.end : min(mention.end + 5, st.end)]
        head = mention.root.head
        spans = [
            self.get_average_embedding(mention),
            self.get_average_embedding(mention_lefts),
            self.get_average_embedding(mention_rights),
            self.get_average_embedding(st),
            (unicode_(doc_embedding[0:8]) + "...", doc_embedding),
        ]
        words = [
            self.get_word_embedding(mention.root),
            self.get_word_embedding(mention[0]),
            self.get_word_embedding(mention[-1]),
            self.get_word_in_sentence(mention.start - 1, st),
            self.get_word_in_sentence(mention.end, st),
            self.get_word_in_sentence(mention.start - 2, st),
            self.get_word_in_sentence(mention.end + 1, st),
            self.get_word_embedding(head),
        ]
        spans_embeddings_ = {
            "00_Mention": spans[0][0],
            "01_MentionLeft": spans[1][0],
            "02_MentionRight": spans[2][0],
            "03_Sentence": spans[3][0],
            "04_Doc": spans[4][0],
        }
        words_embeddings_ = {
            "00_MentionHead": words[0][0],
            "01_MentionFirstWord": words[1][0],
            "02_MentionLastWord": words[2][0],
            "03_PreviousWord": words[3][0],
            "04_NextWord": words[4][0],
            "05_SecondPreviousWord": words[5][0],
            "06_SecondNextWord": words[6][0],
            "07_MentionRootHead": words[7][0],
        }
        return (
            spans_embeddings_,
            words_embeddings_,
            np.concatenate([em[1] for em in spans], axis=0),
            np.concatenate([em[1] for em in words], axis=0),
        )


class Document(object):
    """
    Main data class: encapsulate list of utterances, mentions and speakers
    Process utterances to extract mentions and pre-compute mentions features
    """

    def __init__(
        self,
        nlp,
        utterances=None,
        utterances_speaker=None,
        speakers_names=None,
        blacklist=False,
        consider_speakers=False,
        model_path=None,
        embedding_extractor=None,
        conll=None,
        debug=False,
    ):
        """
        Arguments:
            nlp (spaCy Language Class): A spaCy Language Class for processing the text input
            utterances: utterance(s) to load already see self.add_utterances()
            utterances_speaker: speaker(s) of utterance(s) to load already see self.add_utterances()
            speakers_names: speaker(s) of utterance(s) to load already see self.add_utterances()
            blacklist (boolean): use a list of term for which coreference is not preformed
            consider_speakers (boolean): consider speakers informations
            pretrained_model_path (string): Path to a folder with pretrained word embeddings
            embedding_extractor (EmbeddingExtractor): Use a pre-loaded word embeddings extractor
            conll (string): If training on coNLL data: identifier of the document type
            debug (boolean): print debug informations
        """
        self.nlp = nlp
        self.blacklist = blacklist
        self.utterances = []
        self.utterances_speaker = []
        self.last_utterances_loaded = []
        self.mentions = []
        self.speakers = {}
        self.n_sents = 0
        self.debug = debug
        self.consider_speakers = consider_speakers or conll is not None

        self.genre_, self.genre = self.set_genre(conll)

        if model_path is not None and embedding_extractor is None:
            self.embed_extractor = EmbeddingExtractor(model_path)
        elif embedding_extractor is not None:
            self.embed_extractor = embedding_extractor
        else:
            self.embed_extractor = None

        if utterances:
            self.add_utterances(utterances, utterances_speaker, speakers_names)

    def set_genre(self, conll):
        if conll is not None:
            genre = np.zeros((7,))
            genre[conll] = 1
        else:
            genre = np.array(0, ndmin=1, copy=False)
        return conll, genre

    def __str__(self):
        formatted = "\n ".join(
            unicode_(i) + " " + unicode_(s)
            for i, s in zip(self.utterances, self.utterances_speaker)
        )
        mentions = "\n ".join(
            unicode_(i) + " " + unicode_(i.speaker) for i in self.mentions
        )
        return f"<utterances, speakers> \n {formatted}\n<mentions> \n {mentions}"

    def __len__(self):
        """ Return the number of mentions (not utterances) since it is what we really care about """
        return len(self.mentions)

    def __getitem__(self, key):
        """ Return a specific mention (not utterance) """
        return self.mentions[key]

    def __iter__(self):
        """ Iterate over mentions (not utterances) """
        for mention in self.mentions:
            yield mention

    #######################################
    ###### UTERANCE LOADING FUNCTIONS #####
    #######################################

    def set_utterances(self, utterances, utterances_speaker=None, speakers_names=None):
        self.utterances = []
        self.utterances_speaker = []
        self.mentions = []
        self.speakers = {}
        self.n_sents = 0
        if utterances:
            self.add_utterances(utterances, utterances_speaker, speakers_names)

    def add_utterances(self, utterances, utterances_speaker=None, speakers_names=None):
        """
        Add utterances to the utterance list and build mention list for these utterances

        Arg:
            utterances : iterator or list of string corresponding to successive utterances
            utterances_speaker : iterator or list of speaker id for each utterance.
                If not provided, assume two speakers speaking alternatively.
                if utterances and utterances_speaker are not of the same length padded with None
            speakers_names : dictionnary of list of acceptable speaker names for each speaker id
        Return:
            List of indexes of added utterances in the docs
        """
        if self.debug:
            print("Adding utterances", utterances)
        if isinstance(utterances, string_types):
            utterances = [utterances]
        if utterances_speaker is None:
            if self.debug:
                print("No utterance speaker indication")
            a = -1
            if self.utterances_speaker and isinstance(
                self.utterances_speaker[-1].speaker_id, integer_types
            ):
                a = self.utterances_speaker[-1].speaker_id
            utterances_speaker = ((i + a + 1) % 2 for i in range(len(utterances)))
        utterances_index = []
        utt_start = len(self.utterances)
        docs = list(self.nlp.pipe(utterances))
        m_spans = list(
            extract_mentions_spans(doc, blacklist=self.blacklist) for doc in docs
        )
        for utt_index, (doc, m_spans, speaker_id) in enumerate(
            zip_longest(docs, m_spans, utterances_speaker)
        ):
            if speaker_id not in self.speakers:
                speaker_name = (
                    speakers_names.get(speaker_id, None) if speakers_names else None
                )
                self.speakers[speaker_id] = Speaker(speaker_id, speaker_name)
            self._process_mentions(
                m_spans, utt_start + utt_index, self.n_sents, self.speakers[speaker_id]
            )
            utterances_index.append(utt_start + utt_index)
            self.utterances.append(doc)
            self.utterances_speaker.append(self.speakers[speaker_id])
            self.n_sents += len(list(doc.sents))

        self.set_mentions_features()
        self.last_utterances_loaded = utterances_index

    ###################################
    ## FEATURES MENTIONS EXTRACTION ###
    ###################################

    def _process_mentions(self, mentions_spans, utterance_index, n_sents, speaker):
        """
        Process mentions in a spacy doc (an utterance)
        """
        processed_spans = sorted(
            (m for m in mentions_spans), key=lambda m: (m.root.i, m.start)
        )
        n_mentions = len(self.mentions)
        for mention_index, span in enumerate(processed_spans):
            self.mentions.append(
                Mention(
                    span, mention_index + n_mentions, utterance_index, n_sents, speaker
                )
            )

    def set_mentions_features(self):
        """
        Compute features for the extracted mentions
        """
        doc_embedding = (
            self.embed_extractor.get_document_embedding(self.utterances)
            if self.embed_extractor is not None
            else None
        )
        for mention in self.mentions:
            one_hot_type = np.zeros((4,))
            one_hot_type[mention.mention_type] = 1
            features_ = {
                "01_MentionType": mention.mention_type,
                "02_MentionLength": len(mention) - 1,
                "03_MentionNormLocation": (mention.index) / len(self.mentions),
                "04_IsMentionNested": 1
                if any(
                    (
                        m is not mention
                        and m.utterances_sent == mention.utterances_sent
                        and m.start <= mention.start
                        and mention.end <= m.end
                    )
                    for m in self.mentions
                )
                else 0,
            }
            features = np.concatenate(
                [
                    one_hot_type,
                    encode_distance(features_["02_MentionLength"]),
                    np.array(features_["03_MentionNormLocation"], ndmin=1, copy=False),
                    np.array(features_["04_IsMentionNested"], ndmin=1, copy=False),
                ],
                axis=0,
            )
            (
                spans_embeddings_,
                words_embeddings_,
                spans_embeddings,
                words_embeddings,
            ) = self.embed_extractor.get_mention_embeddings(mention, doc_embedding)
            mention.features_ = features_
            mention.features = features
            mention.spans_embeddings = spans_embeddings
            mention.spans_embeddings_ = spans_embeddings_
            mention.words_embeddings = words_embeddings
            mention.words_embeddings_ = words_embeddings_

    def get_single_mention_features(self, mention):
        """ Features for anaphoricity test (single mention features + genre if conll)"""
        features_ = mention.features_
        features_["DocGenre"] = self.genre_
        return (features_, np.concatenate([mention.features, self.genre], axis=0))

    def get_pair_mentions_features(self, m1, m2):
        """ Features for pair of mentions (same speakers, speaker mentioned, string match)"""
        features_ = {
            "00_SameSpeaker": 1
            if self.consider_speakers and m1.speaker == m2.speaker
            else 0,
            "01_AntMatchMentionSpeaker": 1
            if self.consider_speakers and m2.speaker_match_mention(m1)
            else 0,
            "02_MentionMatchSpeaker": 1
            if self.consider_speakers and m1.speaker_match_mention(m2)
            else 0,
            "03_HeadsAgree": 1 if m1.heads_agree(m2) else 0,
            "04_ExactStringMatch": 1 if m1.exact_match(m2) else 0,
            "05_RelaxedStringMatch": 1 if m1.relaxed_match(m2) else 0,
            "06_SentenceDistance": m2.utterances_sent - m1.utterances_sent,
            "07_MentionDistance": m2.index - m1.index - 1,
            "08_Overlapping": 1
            if (m1.utterances_sent == m2.utterances_sent and m1.end > m2.start)
            else 0,
            "09_M1Features": m1.features_,
            "10_M2Features": m2.features_,
            "11_DocGenre": self.genre_,
        }
        pairwise_features = [
            np.array(
                [
                    features_["00_SameSpeaker"],
                    features_["01_AntMatchMentionSpeaker"],
                    features_["02_MentionMatchSpeaker"],
                    features_["03_HeadsAgree"],
                    features_["04_ExactStringMatch"],
                    features_["05_RelaxedStringMatch"],
                ]
            ),
            encode_distance(features_["06_SentenceDistance"]),
            encode_distance(features_["07_MentionDistance"]),
            np.array(features_["08_Overlapping"], ndmin=1),
            m1.features,
            m2.features,
            self.genre,
        ]
        return (features_, np.concatenate(pairwise_features, axis=0))

    ###################################
    ###### ITERATOR OVER MENTIONS #####
    ###################################

    def get_candidate_mentions(self, last_utterances_added=False):
        """
        Return iterator over indexes of mentions in a list of utterances if specified
        """
        if last_utterances_added:
            for i, mention in enumerate(self.mentions):
                if self.debug:
                    print("", i, mention, "utterance index", mention.utterance_index)
                if mention.utterance_index in self.last_utterances_loaded:
                    yield i
        else:
            iterator = range(len(self.mentions))
            for i in iterator:
                yield i

    def get_candidate_pairs(
        self, mentions=None, max_distance=50, max_distance_with_match=500, debug=False
    ):
        """
        Yield tuples of mentions, dictionnary of candidate antecedents for the mention

        Arg:
            mentions: an iterator over mention indexes (as returned by get_candidate_mentions)
            max_mention_distance : max distance between a mention and its antecedent
            max_mention_distance_string_match : max distance between a mention and
                its antecedent when there is a proper noun match
        """
        if mentions is None:
            mentions = range(len(self.mentions))
        if debug:
            print("get_candidate_pairs: mentions", mentions)

        if max_distance_with_match is not None:
            word_to_mentions = {}
            for i in mentions:
                for tok in self.mentions[i].content_words:
                    if not tok in word_to_mentions:
                        word_to_mentions[tok] = [i]
                    else:
                        word_to_mentions[tok].append(i)

        for i in mentions:
            antecedents = (
                set(range(i))
                if max_distance is None
                else set(range(max(0, i - max_distance), i))
            )
            if debug:
                print("antecedents", antecedents)
            if max_distance_with_match is not None:
                for tok in self.mentions[i].content_words:
                    with_string_match = word_to_mentions.get(tok, None)
                    for match_idx in with_string_match:
                        if match_idx < i and match_idx >= i - max_distance_with_match:
                            antecedents.add(match_idx)
            yield i, antecedents


def mention_detection_debug(sentence):
    print(" Loading spacy model")
    try:
        spacy.info("en_core_web_sm")
        model = "en_core_web_sm"
    except IOError:
        print("No spacy 2 model detected, using spacy1 'en' model")
        spacy.info("en")
        model = "en"
    nlp = spacy.load(model)
    doc = nlp(sentence.decode("utf-8"))
    mentions = extract_mentions_spans(doc, blacklist=False, debug=True)
    for mention in mentions:
        print(mention)


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        sent = sys.argv[1]
        mention_detection_debug(sent)
    else:
        mention_detection_debug("My sister has a dog. She loves him.")

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/learn.py
"""Conll training algorithm"""

import os
import time
import argparse
import socket
from datetime import datetime
import numpy as np

import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.optim import RMSprop
from torch.utils.data import DataLoader
from tensorboardX import SummaryWriter

from neuralcoref.train.model import Model
from neuralcoref.train.dataset import (
    NCDataset,
    NCBatchSampler,
    load_embeddings_from_file,
    padder_collate,
    SIZE_PAIR_IN,
    SIZE_SINGLE_IN,
)
from neuralcoref.train.utils import SIZE_EMBEDDING
from neuralcoref.train.evaluator import ConllEvaluator

PACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))
STAGES = ["allpairs", "toppairs", "ranking"]


def clipped_sigmoid(inputs):
    epsilon = 1.0e-7
    return torch.sigmoid(inputs).clamp(epsilon, 1.0 - epsilon)


def get_all_pairs_loss(n):
    def all_pair_loss(scores, targets):
        """ All pairs and single mentions probabilistic loss
        """
        labels = targets[0]
        weights = targets[4].data if len(targets) == 5 else None
        loss_op = nn.BCEWithLogitsLoss(weight=weights, reduction="sum")
        loss = loss_op(scores, labels)
        return loss / n

    return all_pair_loss


def get_top_pair_loss(n):
    def top_pair_loss(scores, targets, debug=False):
        """ Top pairs (best true and best mistaken) and single mention probabilistic loss
        """
        true_ants = targets[2]
        false_ants = targets[3] if len(targets) == 5 else None
        s_scores = clipped_sigmoid(scores)
        true_pairs = torch.gather(s_scores, 1, true_ants)
        top_true, top_true_arg = torch.log(true_pairs).max(
            dim=1
        )  # max(log(p)), p=sigmoid(s)
        if debug:
            print("true_pairs", true_pairs.data)
            print("top_true", top_true.data)
            print("top_true_arg", top_true_arg.data)
        out_score = torch.sum(top_true).neg()
        if (
            false_ants is not None
        ):  # We have no false antecedents when there are no pairs
            false_pairs = torch.gather(s_scores, 1, false_ants)
            top_false, _ = torch.log(1 - false_pairs).min(
                dim=1
            )  # min(log(1-p)), p=sigmoid(s)
            out_score = out_score + torch.sum(top_false).neg()
        return out_score / n

    return top_pair_loss


def get_ranking_loss(n):
    def ranking_loss(scores, targets):
        """ Slack-rescaled max margin loss
        """
        costs = targets[1]
        true_ants = targets[2]
        weights = targets[4] if len(targets) == 5 else None
        true_ant_score = torch.gather(scores, 1, true_ants)
        top_true, _ = true_ant_score.max(dim=1)
        tmp_loss = scores.add(1).add(
            top_true.unsqueeze(1).neg()
        )  # 1 + scores - top_true
        if weights is not None:
            tmp_loss = tmp_loss.mul(weights)
        tmp_loss = tmp_loss.mul(costs)
        loss, _ = tmp_loss.max(dim=1)
        out_score = torch.sum(loss)
        return out_score / n

    return ranking_loss


def decrease_lr(optim_func, factor=0.1, min_lrs=0, eps=0, verbose=True):
    for i, param_group in enumerate(optim_func.param_groups):
        old_lr = float(param_group["lr"])
        new_lr = max(old_lr * factor, min_lrs)
        if old_lr - new_lr > eps:
            param_group["lr"] = new_lr
            if verbose:
                print(f"Reducing learning rate" " of group {i} to {new_lr:.4e}.")
    return new_lr


def load_model(model, path):
    print(" Reloading model from", path)
    model.load_state_dict(
        torch.load(path)
        if args.cuda
        else torch.load(path, map_location=lambda storage, loc: storage)
    )


def run_model(args):
    print(
        "Training for",
        args.all_pairs_epoch,
        args.top_pairs_epoch,
        args.ranking_epoch,
        "epochs",
    )
    # Tensorboard server
    writer = SummaryWriter()

    # Load datasets and embeddings
    embed_path = args.weights if args.weights is not None else args.train
    tensor_embeddings, voc = load_embeddings_from_file(embed_path + "tuned_word")
    dataset = NCDataset(args.train, args)
    eval_dataset = NCDataset(args.eval, args)
    print("Vocabulary:", len(voc))

    # Construct model
    print(" Build model")
    model = Model(
        len(voc),
        SIZE_EMBEDDING,
        args.h1,
        args.h2,
        args.h3,
        SIZE_PAIR_IN,
        SIZE_SINGLE_IN,
    )
    model.load_embeddings(tensor_embeddings)
    if args.cuda:
        model.cuda()
    if args.weights is not None:
        print(" Loading pre-trained weights")
        model.load_weights(args.weights)
    if args.checkpoint_file is not None:
        print(" Loading model from", args.checkpoint_file)
        model.load_state_dict(
            torch.load(args.checkpoint_file)
            if args.cuda
            else torch.load(
                args.checkpoint_file, map_location=lambda storage, loc: storage
            )
        )

    print(" Loading conll evaluator")
    eval_evaluator = ConllEvaluator(
        model, eval_dataset, args.eval, args.evalkey, embed_path, args
    )
    train_evaluator = ConllEvaluator(
        model, dataset, args.train, args.trainkey, embed_path, args
    )
    print(" Testing evaluator and getting first eval score")
    eval_evaluator.test_model()
    start_time = time.time()
    eval_evaluator.build_test_file()
    score, f1_conll, ident = eval_evaluator.get_score()
    elapsed = time.time() - start_time
    print(f"|| s/evaluation {elapsed:5.2f}")
    writer.add_scalar("eval/" + "F1_conll", f1_conll, 0)

    # Preparing dataloader
    print(" Preparing dataloader")
    print(
        "Dataloader parameters: batchsize",
        args.batchsize,
        "numworkers",
        args.numworkers,
    )
    batch_sampler = NCBatchSampler(
        dataset.mentions_pair_length, shuffle=True, batchsize=args.batchsize
    )
    dataloader = DataLoader(
        dataset,
        collate_fn=padder_collate,
        batch_sampler=batch_sampler,
        num_workers=args.numworkers,
        pin_memory=args.cuda,
    )
    mentions_idx, n_pairs = batch_sampler.get_batch_info()

    print(" Start training")
    g_step = 0
    start_from = (
        args.startstep
        if args.startstep is not None and args.startstage is not None
        else 0
    )

    def run_epochs(
        start_epoch, end_epoch, loss_func, optim_func, save_name, lr, g_step, debug=None
    ):
        best_model_path = args.save_path + "best_model" + save_name
        start_time_all = time.time()
        best_f1_conll = 0
        lower_eval = 0
        for epoch in range(start_epoch, end_epoch):
            """ Run an epoch """
            print(f" {save_name} Epoch {epoch:d}")
            model.train()
            start_time_log = time.time()
            start_time_epoch = time.time()
            epoch_loss = 0
            for batch_i, (m_idx, n_pairs_l, batch) in enumerate(
                zip(mentions_idx, n_pairs, dataloader)
            ):
                if debug is not None and (debug == -1 or debug in m_idx):
                    l = list(dataset.flat_m_loc[m][2:] for m in m_idx)
                    print(
                        " Batch",
                        batch_i,
                        "m_idx:",
                        "|".join(str(i) for i in m_idx),
                        "mentions:",
                        "|".join(dataset.docs[d]["mentions"][i] for u, i, d in l),
                    )
                    print("Batch n_pairs:", "|".join(str(p) for p in n_pairs_l))
                inputs, targets = batch
                inputs = tuple(Variable(inp, requires_grad=False) for inp in inputs)
                targets = tuple(Variable(tar, requires_grad=False) for tar in targets)
                if args.cuda:
                    inputs = tuple(i.cuda() for i in inputs)
                    targets = tuple(t.cuda() for t in targets)
                scores = model(inputs)
                if debug is not None and (debug == -1 or debug in m_idx):
                    print(
                        "Scores:\n"
                        + "\n".join(
                            "|".join(str(s) for s in s_l)
                            for s_l in scores.data.cpu().numpy()
                        )
                    )
                    print(
                        "Labels:\n"
                        + "\n".join(
                            "|".join(str(s) for s in s_l)
                            for s_l in targets[0].data.cpu().numpy()
                        )
                    )
                loss = loss_func(scores, targets)
                if debug is not None and (debug == -1 or debug in m_idx):
                    print("Loss", loss.item())
                # Zero gradients, perform a backward pass, and update the weights.
                optim_func.zero_grad()
                loss.backward()
                epoch_loss += loss.item()
                optim_func.step()
                writer.add_scalar("train/" + save_name + "_loss", loss.item(), g_step)
                writer.add_scalar("meta/" + "lr", lr, g_step)
                writer.add_scalar("meta/" + "stage", STAGES.index(save_name), g_step)
                g_step += 1
                if batch_i % args.log_interval == 0 and batch_i > 0:
                    elapsed = time.time() - start_time_log
                    lr = optim_func.param_groups[0]["lr"]
                    ea = elapsed * 1000 / args.log_interval
                    li = loss.item()
                    print(
                        f"| epoch {epoch:3d} | {batch_i:5d}/{len(dataloader):5d} batches | "
                        f"lr {lr:.2e} | ms/batch {ea:5.2f} | "
                        f"loss {li:.2e}"
                    )
                    start_time_log = time.time()
            elapsed_all = time.time() - start_time_all
            elapsed_epoch = time.time() - start_time_epoch
            ep = elapsed_epoch / 60
            ea = (
                elapsed_all
                / 3600
                * float(end_epoch - epoch)
                / float(epoch - start_epoch + 1)
            )
            print(
                f"|| min/epoch {ep:5.2f} | est. remaining time (h) {ea:5.2f} | loss {epoch_loss:.2e}"
            )
            writer.add_scalar("epoch/" + "loss", epoch_loss, g_step)
            if epoch % args.conll_train_interval == 0:
                start_time = time.time()
                train_evaluator.build_test_file()
                score, f1_conll, ident = train_evaluator.get_score()
                elapsed = time.time() - start_time
                ep = elapsed_epoch / 60
                print(f"|| min/train evaluation {ep:5.2f} | F1_conll {f1_conll:5.2f}")
                writer.add_scalar("epoch/" + "F1_conll", f1_conll, g_step)
            if epoch % args.conll_eval_interval == 0:
                start_time = time.time()
                eval_evaluator.build_test_file()
                score, f1_conll, ident = eval_evaluator.get_score()
                elapsed = time.time() - start_time
                ep = elapsed_epoch / 60
                print(f"|| min/evaluation {ep:5.2f}")
                writer.add_scalar("eval/" + "F1_conll", f1_conll, g_step)
                g_step += 1
                save_path = args.save_path + save_name + "_" + str(epoch)
                torch.save(model.state_dict(), save_path)
                if f1_conll > best_f1_conll:
                    best_f1_conll = f1_conll
                    torch.save(model.state_dict(), best_model_path)
                    lower_eval = 0
                elif args.on_eval_decrease != "nothing":
                    print("Evaluation metric decreases")
                    lower_eval += 1
                    if lower_eval >= args.patience:
                        if (
                            args.on_eval_decrease == "divide_lr"
                            or args.on_eval_decrease == "divide_then_next"
                        ):
                            print("reload best model and decrease lr")
                            load_model(model, best_model_path)
                            lr = decrease_lr(optim_func)
                        if args.on_eval_decrease == "next_stage" or lr <= args.min_lr:
                            print("Switch to next stage")
                            break
        # Save last step
        start_time = time.time()
        eval_evaluator.build_test_file()
        score, f1_conll, ident = eval_evaluator.get_score()
        elapsed = time.time() - start_time
        ep = elapsed / 60
        print(f"|| min/evaluation {ep:5.2f}")
        writer.add_scalar("eval/" + "F1_conll", f1_conll, g_step)
        g_step += 1
        save_path = args.save_path + save_name + "_" + str(epoch)
        torch.save(model.state_dict(), save_path)
        load_model(model, best_model_path)
        return g_step

    if args.startstage is None or args.startstage == "allpairs":
        optimizer = RMSprop(
            model.parameters(), lr=args.all_pairs_lr, weight_decay=args.all_pairs_l2
        )
        loss_func = get_all_pairs_loss(batch_sampler.pairs_per_batch)
        g_step = run_epochs(
            start_from,
            args.all_pairs_epoch,
            loss_func,
            optimizer,
            "allpairs",
            args.all_pairs_lr,
            g_step,
        )
        start_from = 0

    if args.startstage is None or args.startstage in ["allpairs", "toppairs"]:
        optimizer = RMSprop(
            model.parameters(), lr=args.top_pairs_lr, weight_decay=args.top_pairs_l2
        )
        loss_func = get_top_pair_loss(10 * batch_sampler.mentions_per_batch)
        g_step = run_epochs(
            start_from,
            args.top_pairs_epoch,
            loss_func,
            optimizer,
            "toppairs",
            args.top_pairs_lr,
            g_step,
        )
        start_from = 0

    if args.startstage is None or args.startstage in [
        "ranking",
        "allpairs",
        "toppairs",
    ]:
        optimizer = RMSprop(
            model.parameters(), lr=args.ranking_lr, weight_decay=args.ranking_l2
        )
        loss_func = get_ranking_loss(batch_sampler.mentions_per_batch)
        g_step = run_epochs(
            start_from,
            args.ranking_epoch,
            loss_func,
            optimizer,
            "ranking",
            args.ranking_lr,
            g_step,
        )


if __name__ == "__main__":
    DIR_PATH = os.path.dirname(os.path.realpath(__file__))
    parser = argparse.ArgumentParser(
        description="Training the neural coreference model"
    )
    parser.add_argument(
        "--train",
        type=str,
        default=DIR_PATH + "/data/",
        help="Path to the train dataset",
    )
    parser.add_argument(
        "--eval", type=str, default=DIR_PATH + "/data/", help="Path to the eval dataset"
    )
    parser.add_argument(
        "--evalkey", type=str, help="Path to an optional key file for scoring"
    )
    parser.add_argument(
        "--weights",
        type=str,
        help="Path to pre-trained weights (if you only want to test the scoring for e.g.)",
    )
    parser.add_argument(
        "--batchsize",
        type=int,
        default=20000,
        help="Size of a batch in total number of pairs",
    )
    parser.add_argument(
        "--numworkers",
        type=int,
        default=8,
        help="Number of workers for loading batches",
    )
    parser.add_argument(
        "--startstage",
        type=str,
        help='Start from a specific stage ("allpairs", "toppairs", "ranking")',
    )
    parser.add_argument("--startstep", type=int, help="Start from a specific step")
    parser.add_argument(
        "--checkpoint_file",
        type=str,
        help="Start from a previously saved checkpoint file",
    )
    parser.add_argument(
        "--log_interval", type=int, default=10, help="test every X mini-batches"
    )
    parser.add_argument(
        "--conll_eval_interval",
        type=int,
        default=10,
        help="evaluate eval F1 conll every X epochs",
    )
    parser.add_argument(
        "--conll_train_interval",
        type=int,
        default=20,
        help="evaluate train F1 conll every X epochs",
    )
    parser.add_argument("--seed", type=int, default=1111, help="random seed")
    parser.add_argument("--costfn", type=float, default=0.8, help="cost of false new")
    parser.add_argument("--costfl", type=float, default=0.4, help="cost of false link")
    parser.add_argument("--costwl", type=float, default=1.0, help="cost of wrong link")
    parser.add_argument(
        "--h1", type=int, default=1000, help="number of hidden unit on layer 1"
    )
    parser.add_argument(
        "--h2", type=int, default=500, help="number of hidden unit on layer 2"
    )
    parser.add_argument(
        "--h3", type=int, default=500, help="number of hidden unit on layer 3"
    )
    parser.add_argument(
        "--all_pairs_epoch",
        type=int,
        default=200,
        help="number of epochs for all-pairs pre-training",
    )
    parser.add_argument(
        "--top_pairs_epoch",
        type=int,
        default=200,
        help="number of epochs for top-pairs pre-training",
    )
    parser.add_argument(
        "--ranking_epoch",
        type=int,
        default=200,
        help="number of epochs for ranking training",
    )
    parser.add_argument(
        "--all_pairs_lr",
        type=float,
        default=2e-4,
        help="all pairs pre-training learning rate",
    )
    parser.add_argument(
        "--top_pairs_lr",
        type=float,
        default=2e-4,
        help="top pairs pre-training learning rate",
    )
    parser.add_argument(
        "--ranking_lr", type=float, default=2e-6, help="ranking training learning rate"
    )
    parser.add_argument(
        "--all_pairs_l2",
        type=float,
        default=1e-6,
        help="all pairs pre-training l2 regularization",
    )
    parser.add_argument(
        "--top_pairs_l2",
        type=float,
        default=1e-5,
        help="top pairs pre-training l2 regularization",
    )
    parser.add_argument(
        "--ranking_l2",
        type=float,
        default=1e-5,
        help="ranking training l2 regularization",
    )
    parser.add_argument(
        "--patience",
        type=int,
        default=3,
        help="patience (epochs) before considering evaluation has decreased",
    )
    parser.add_argument("--min_lr", type=float, default=2e-8, help="min learning rate")
    parser.add_argument(
        "--on_eval_decrease",
        type=str,
        default="nothing",
        help='What to do when evaluation decreases ("nothing", "divide_lr", "next_stage", "divide_then_next")',
    )
    parser.add_argument(
        "--lazy",
        type=int,
        default=1,
        choices=(0, 1),
        help="Use lazy loading (1, default) or not (0) while loading the npy files",
    )
    args = parser.parse_args()
    args.costs = {"FN": args.costfn, "FL": args.costfl, "WL": args.costwl}
    args.lazy = bool(args.lazy)
    current_time = datetime.now().strftime("%b%d_%H-%M-%S")
    args.save_path = os.path.join(
        PACKAGE_DIRECTORY,
        "checkpoints",
        current_time + "_" + socket.gethostname() + "_",
    )

    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    args.cuda = torch.cuda.is_available()
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

    args.evalkey = args.evalkey if args.evalkey is not None else args.eval + "/key.txt"
    args.trainkey = args.train + "/key.txt"
    args.train = args.train + "/numpy/"
    args.eval = args.eval + "/numpy/"
    print(args)
    run_model(args)

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/train/evaluator.py
"""Conll Evaluation - Scoring"""

import os
import subprocess
import io
import pickle

import torch
from torch.utils.data import DataLoader

from neuralcoref.train.conllparser import FEATURES_NAMES
from neuralcoref.train.dataset import NCBatchSampler, padder_collate
from neuralcoref.train.compat import unicode_

PACKAGE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))

OUT_PATH = os.path.join(PACKAGE_DIRECTORY, "test_corefs.txt")  # fernandes.txt")#
ALL_MENTIONS_PATH = os.path.join(PACKAGE_DIRECTORY, "test_mentions.txt")
# KEY_PATH = os.path.join(PACKAGE_DIRECTORY, "conll-2012-test-test-key.txt")
SCORING_SCRIPT = os.path.join(PACKAGE_DIRECTORY, "scorer_wrapper.pl")

METRICS = ["muc", "bcub", "ceafm", "ceafe", "blanc"]
CONLL_METRICS = ["muc", "bcub", "ceafe"]


class ConllEvaluator(object):
    def __init__(self, model, dataset, test_data_path, test_key_file, embed_path, args):
        """ Evaluate the pytorch model that is currently being build
            We take the embedding vocabulary currently being trained
        """
        self.test_key_file = test_key_file
        self.cuda = args.cuda
        self.model = model
        batch_sampler = NCBatchSampler(
            dataset.mentions_pair_length, batchsize=args.batchsize, shuffle=False
        )
        self.dataloader = DataLoader(
            dataset,
            collate_fn=padder_collate,
            batch_sampler=batch_sampler,
            num_workers=args.numworkers,
            pin_memory=args.cuda,
        )
        self.mentions_idx, self.n_pairs = batch_sampler.get_batch_info()
        self.load_meta(test_data_path)

    def load_meta(self, test_data_path):
        # Load meta files
        datas = {}
        if not os.listdir(test_data_path):
            raise ValueError("Empty test_data_path")
        bin_files_found = False
        print("Reading ", end="")
        for file_name in os.listdir(test_data_path):
            if ".bin" not in file_name:
                continue
            bin_files_found = True
            print(file_name, end=", ")
            with open(test_data_path + file_name, "rb") as f:
                datas[file_name.split(".")[0]] = pickle.load(f)
        if not bin_files_found:
            raise ValueError(f"Can't find bin files in {test_data_path}")
        print("Done")
        self.m_loc = datas[FEATURES_NAMES[9]]
        self.tokens = datas[FEATURES_NAMES[10]]
        self.lookup = datas[FEATURES_NAMES[11]]
        self.docs = datas[FEATURES_NAMES[12]]
        self.flat_m_idx = list(
            (doc_i, m_i) for doc_i, l in enumerate(self.m_loc) for m_i in range(len(l))
        )

    ###########################
    #### CLUSTER FUNCTIONS ####
    ###########################

    def _prepare_clusters(self):
        """
        Clean up and prepare one cluster for each mention
        """
        self.mention_to_cluster = list(
            list(range(len(doc_mentions))) for doc_mentions in self.m_loc
        )
        self.clusters = list(
            dict((i, [i]) for i in doc_mentions)
            for doc_mentions in self.mention_to_cluster
        )

    def _merge_coreference_clusters(self, ant_flat_idx, mention_flat_idx):
        """
        Merge two clusters together
        """
        doc_idx, ant_idx = self.flat_m_idx[ant_flat_idx]
        doc_idx2, mention_idx = self.flat_m_idx[mention_flat_idx]
        assert doc_idx2 == doc_idx
        if (
            self.mention_to_cluster[doc_idx][ant_idx]
            == self.mention_to_cluster[doc_idx][mention_idx]
        ):
            return
        remove_id = self.mention_to_cluster[doc_idx][ant_idx]
        keep_id = self.mention_to_cluster[doc_idx][mention_idx]
        for idx in self.clusters[doc_idx][remove_id]:
            self.mention_to_cluster[doc_idx][idx] = keep_id
            self.clusters[doc_idx][keep_id].append(idx)
        del self.clusters[doc_idx][remove_id]

    def remove_singletons_clusters(self, debug=False):
        for doc_idx in range(len(self.docs)):
            remove_id = []
            kept = False
            for key, mentions in self.clusters[doc_idx].items():
                if len(mentions) == 1:
                    remove_id.append(key)
                    self.mention_to_cluster[doc_idx][key] = None
                else:
                    kept = True
                    if debug:
                        l = list(self.m_loc[doc_idx][m][3] for m in mentions)
                        print("Cluster found", key)
                        print(
                            "Corefs:",
                            "|".join(
                                str(self.docs[doc_idx]["mentions"][m_idx])
                                + " ("
                                + str(m_idx)
                                + ")"
                                for m_idx in l
                            ),
                        )
            if not kept and debug:
                print(" No coreference found")
            for rem in remove_id:
                del self.clusters[doc_idx][rem]

    def display_clusters(self, doc_idx=None):
        """
        Print clusters informations
        """
        doc_it = range(len(self.docs)) if doc_idx is None else [doc_idx]
        for d_i in doc_it:
            print(
                "Clusters in doc:",
                doc_it,
                self.docs[d_i]["name"],
                self.docs[d_i]["part"],
            )
            print(self.clusters[d_i])
            for key, mentions in self.clusters[d_i].items():
                l = list(self.m_loc[d_i][m][3] for m in mentions)
                print(
                    "cluster",
                    key,
                    "(",
                    ", ".join(self.docs[d_i]["mentions"][m_idx] for m_idx in l),
                    ")",
                )

    ########################
    #### MAIN FUNCTIONS ####
    ########################
    def get_max_score(self, batch, debug=False):
        inputs, mask = batch
        if self.cuda:
            inputs = tuple(i.cuda() for i in inputs)
            mask = mask.cuda()
        self.model.eval()
        with torch.no_grad():
            scores = self.model(inputs, concat_axis=1)
            scores.masked_fill_(mask, -float("Inf"))
            _, max_idx = scores.max(
                dim=1
            )  # We may want to weight the single score with coref.greedyness
        if debug:
            print("Max_idx", max_idx)
        return scores.cpu().numpy(), max_idx.cpu().numpy()

    def test_model(self):
        print(" Test evaluator / print all mentions")
        self.build_test_file(out_path=ALL_MENTIONS_PATH, print_all_mentions=True)
        self.get_score(file_path=ALL_MENTIONS_PATH)

    def build_test_file(
        self,
        out_path=OUT_PATH,
        remove_singleton=True,
        print_all_mentions=False,
        debug=None,
    ):
        """ Build a test file to supply to the coreference scoring perl script
        """
        print(" Building test file")
        self._prepare_clusters()
        self.dataloader.dataset.no_targets = True
        if not print_all_mentions:
            print(" Build coreference clusters")
            for sample_batched, mentions_idx, n_pairs_l in zip(
                self.dataloader, self.mentions_idx, self.n_pairs
            ):
                scores, max_i = self.get_max_score(sample_batched)
                for m_idx, ind, n_pairs in zip(mentions_idx, max_i, n_pairs_l):
                    if (
                        ind < n_pairs
                    ):  # the single score is not the highest, we have a match !
                        prev_idx = m_idx - n_pairs + ind
                        if debug is not None and (
                            debug == -1 or debug == prev_idx or debug == m_idx
                        ):
                            m1_doc, m1_idx = self.flat_m_idx[m_idx]
                            m1 = self.docs[m1_doc]["mentions"][m1_idx]
                            m2_doc, m2_idx = self.flat_m_idx[prev_idx]
                            m2 = self.docs[m2_doc]["mentions"][m2_idx]
                            print(
                                "We have a match between:",
                                m1,
                                "(" + str(m1_idx) + ")",
                                "and:",
                                m2,
                                "(" + str(m2_idx) + ")",
                            )
                        self._merge_coreference_clusters(prev_idx, m_idx)
            if remove_singleton:
                self.remove_singletons_clusters()
        self.dataloader.dataset.no_targets = False

        print(" Construct test file")
        out_str = ""
        for doc, d_tokens, d_lookup, d_m_loc, d_m_to_c in zip(
            self.docs, self.tokens, self.lookup, self.m_loc, self.mention_to_cluster
        ):
            out_str += (
                "#begin document (" + doc["name"] + "); part " + doc["part"] + "\n"
            )
            for utt_idx, (c_tokens, c_lookup) in enumerate(zip(d_tokens, d_lookup)):
                for i, (token, lookup) in enumerate(zip(c_tokens, c_lookup)):
                    out_coref = ""
                    for m_str, mention, mention_cluster in zip(
                        doc["mentions"], d_m_loc, d_m_to_c
                    ):
                        m_start, m_end, m_utt, m_idx, m_doc = mention
                        if mention_cluster is None:
                            pass
                        elif m_utt == utt_idx:
                            if m_start in lookup:
                                out_coref += "|" if out_coref else ""
                                out_coref += "(" + unicode_(mention_cluster)
                                if (m_end - 1) in lookup:
                                    out_coref += ")"
                                else:
                                    out_coref += ""
                            elif (m_end - 1) in lookup:
                                out_coref += "|" if out_coref else ""
                                out_coref += unicode_(mention_cluster) + ")"
                    out_line = (
                        doc["name"]
                        + " "
                        + doc["part"]
                        + " "
                        + unicode_(i)
                        + " "
                        + token
                        + " "
                    )
                    out_line += "-" if len(out_coref) == 0 else out_coref
                    out_str += out_line + "\n"
                out_str += "\n"
            out_str += "#end document\n"

        # Write test file
        print("Writing in", out_path)
        with io.open(out_path, "w", encoding="utf-8") as out_file:
            out_file.write(out_str)

    def get_score(self, file_path=OUT_PATH, debug=False):
        """ Call the coreference scoring perl script on the created test file
        """
        print(" Computing score")
        score = {}
        ident = None
        for metric_name in CONLL_METRICS:
            if debug:
                print("Computing metric:", metric_name)
            try:
                scorer_out = subprocess.check_output(
                    [
                        "perl",
                        SCORING_SCRIPT,
                        metric_name,
                        self.test_key_file,
                        file_path,
                    ],
                    stderr=subprocess.STDOUT,
                    encoding="utf-8",
                )
            except subprocess.CalledProcessError as err:
                print("Error during the scoring")
                print(err)
                print(err.output)
                raise
            if debug:
                print("scorer_out", scorer_out)
            value, ident = scorer_out.split("\n")[-2], scorer_out.split("\n")[-1]
            if debug:
                print("value", value, "identification", ident)
            NR, DR, NP, DP = [float(x) for x in value.split(" ")]
            ident_NR, ident_DR, ident_NP, ident_DP = [
                float(x) for x in ident.split(" ")
            ]
            precision = NP / DP if DP else 0
            recall = NR / DR if DR else 0
            F1 = (
                2 * precision * recall / (precision + recall)
                if precision + recall > 0
                else 0
            )
            ident_precision = ident_NP / ident_DP if ident_DP else 0
            ident_recall = ident_NR / ident_DR if ident_DR else 0
            ident_F1 = (
                2 * ident_precision * ident_recall / (ident_precision + ident_recall)
                if ident_precision + ident_recall > 0
                else 0
            )
            score[metric_name] = (precision, recall, F1)
            ident = (
                ident_precision,
                ident_recall,
                ident_F1,
                ident_NR,
                ident_DR,
                ident_NP,
                ident_DP,
            )
        F1_conll = sum([score[metric][2] for metric in CONLL_METRICS]) / len(
            CONLL_METRICS
        )
        print(
            "Mention identification recall",
            ident[1],
            "<= Detected mentions",
            ident[3],
            "True mentions",
            ident[4],
        )
        print("Scores", score)
        print("F1_conll", F1_conll)
        return score, F1_conll, ident

# PROJECT: huggingface_neuralcoref FILE: neuralcoref/file_utils.py
"""
Utilities for working with the local dataset cache.
This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp
Copyright by the AllenNLP authors.
"""

import json
import logging
import os
import shutil
import tempfile
from functools import wraps
from hashlib import sha256
from io import open

import boto3
import requests
from botocore.exceptions import ClientError
from tqdm import tqdm

try:
    from urllib.parse import urlparse
except ImportError:
    from urlparse import urlparse

NEURALCOREF_CACHE = os.getenv(
    "NEURALCOREF_CACHE", os.path.join(os.path.expanduser("~"), ".neuralcoref_cache")
)

NEURALCOREF_MODEL_URL = (
    "https://s3.amazonaws.com/models.huggingface.co/neuralcoref/neuralcoref.tar.gz"
)
NEURALCOREF_MODEL_PATH = os.path.join(str(NEURALCOREF_CACHE), "neuralcoref")

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name


def url_to_filename(url, etag=None):
    """
    Convert `url` into a hashed filename in a repeatable way.
    If `etag` is specified, append its hash to the url's, delimited
    by a period.
    """
    url_bytes = url.encode("utf-8")
    url_hash = sha256(url_bytes)
    filename = url_hash.hexdigest()

    if etag:
        etag_bytes = etag.encode("utf-8")
        etag_hash = sha256(etag_bytes)
        filename += "." + etag_hash.hexdigest()

    return filename


def filename_to_url(filename, cache_dir=None):
    """
    Return the url and etag (which may be ``None``) stored for `filename`.
    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.
    """
    if cache_dir is None:
        cache_dir = NEURALCOREF_CACHE

    cache_path = os.path.join(cache_dir, filename)
    if not os.path.exists(cache_path):
        raise EnvironmentError(f"file {cache_path} not found")

    meta_path = cache_path + ".json"
    if not os.path.exists(meta_path):
        raise EnvironmentError(f"file {meta_path} not found")

    with open(meta_path, encoding="utf-8") as meta_file:
        metadata = json.load(meta_file)
    url = metadata["url"]
    etag = metadata["etag"]

    return url, etag


def cached_path(url_or_filename, cache_dir=None):
    """
    Given something that might be a URL (or might be a local path),
    determine which. If it's a URL, download the file and cache it, and
    return the path to the cached file. If it's already a local path,
    make sure the file exists and then return the path.
    """
    if cache_dir is None:
        cache_dir = NEURALCOREF_CACHE

    parsed = urlparse(url_or_filename)

    if parsed.scheme in ("http", "https", "s3"):
        # URL, so get it from the cache (downloading if necessary)
        return get_from_cache(url_or_filename, cache_dir)
    elif os.path.exists(url_or_filename):
        # File, and it exists.
        return url_or_filename
    elif parsed.scheme == "":
        # File, but it doesn't exist.
        raise EnvironmentError(f"file {url_or_filename} not found")
    else:
        # Something unknown
        raise ValueError(
            f"unable to parse {url_or_filename} as a URL or as a local path"
        )


def split_s3_path(url):
    """Split a full s3 path into the bucket name and path."""
    parsed = urlparse(url)
    if not parsed.netloc or not parsed.path:
        raise ValueError(f"bad s3 path {url}")
    bucket_name = parsed.netloc
    s3_path = parsed.path
    # Remove '/' at beginning of path.
    if s3_path.startswith("/"):
        s3_path = s3_path[1:]
    return bucket_name, s3_path


def s3_request(func):
    """
    Wrapper function for s3 requests in order to create more helpful error
    messages.
    """

    @wraps(func)
    def wrapper(url, *args, **kwargs):
        try:
            return func(url, *args, **kwargs)
        except ClientError as exc:
            if int(exc.response["Error"]["Code"]) == 404:
                raise EnvironmentError(f"file {url} not found")
            else:
                raise

    return wrapper


@s3_request
def s3_etag(url):
    """Check ETag on S3 object."""
    s3_resource = boto3.resource("s3")
    bucket_name, s3_path = split_s3_path(url)
    s3_object = s3_resource.Object(bucket_name, s3_path)
    return s3_object.e_tag


@s3_request
def s3_get(url, temp_file):
    """Pull a file directly from S3."""
    s3_resource = boto3.resource("s3")
    bucket_name, s3_path = split_s3_path(url)
    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)


def http_get(url, temp_file):
    req = requests.get(url, stream=True)
    content_length = req.headers.get("Content-Length")
    total = int(content_length) if content_length is not None else None
    progress = tqdm(unit="B", total=total)
    for chunk in req.iter_content(chunk_size=1024):
        if chunk:  # filter out keep-alive new chunks
            progress.update(len(chunk))
            temp_file.write(chunk)
    progress.close()


def get_from_cache(url, cache_dir=None):
    """
    Given a URL, look for the corresponding dataset in the local cache.
    If it's not there, download it. Then return the path to the cached file.
    """
    if cache_dir is None:
        cache_dir = NEURALCOREF_CACHE

    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)

    # Get eTag to add to filename, if it exists.
    if url.startswith("s3://"):
        etag = s3_etag(url)
    else:
        response = requests.head(url, allow_redirects=True)
        if response.status_code != 200:
            raise IOError(
                f"HEAD request failed for url {url} with status code {response.status_code}"
            )
        etag = response.headers.get("ETag")

    filename = url_to_filename(url, etag)

    # get cache path to put the file
    cache_path = os.path.join(cache_dir, filename)

    if not os.path.exists(cache_path):
        # Download to temporary file, then copy to cache dir once finished.
        # Otherwise you get corrupt cache entries if the download gets interrupted.
        with tempfile.NamedTemporaryFile() as temp_file:
            logger.info("%s not found in cache, downloading to %s", url, temp_file.name)

            # GET file object
            if url.startswith("s3://"):
                s3_get(url, temp_file)
            else:
                http_get(url, temp_file)

            # we are copying the file before closing it, so flush to avoid truncation
            temp_file.flush()
            # shutil.copyfileobj() starts at the current position, so go to the start
            temp_file.seek(0)

            logger.info("copying %s to cache at %s", temp_file.name, cache_path)
            with open(cache_path, "wb") as cache_file:
                shutil.copyfileobj(temp_file, cache_file)

            logger.info("creating metadata file for %s", cache_path)
            meta = {"url": url, "etag": etag}
            meta_path = cache_path + ".json"
            with open(meta_path, "w", encoding="utf-8") as meta_file:
                json.dump(meta, meta_file)

            logger.info("removing temp file %s", temp_file.name)

    return cache_path


def read_set_from_file(filename):
    """
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    """
    collection = set()
    with open(filename, "r", encoding="utf-8") as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection


def get_file_extension(path, dot=True, lower=True):
    ext = os.path.splitext(path)[1]
    ext = ext if dot else ext[1:]
    return ext.lower() if lower else ext

# PROJECT: huggingface_neuralcoref FILE: setup.py
#!/usr/bin/env python
"""
Simple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py

To create the package for pypi.

1. Change the version in __init__.py and setup.py.

2. Commit these changes with the message: "Release: VERSION"

3. Add a tag in git to mark the release: "git tag VERSION -m'Adds tag VERSION for pypi' "
   Push the tag to git: git push --tags origin master

4. Build both the sources and the wheel. Do not change anything in setup.py between
   creating the wheel and the source distribution (obviously).

   For the wheel, run: "python setup.py bdist_wheel" in the top level allennlp directory.
   (this will build a wheel for the python version you use to build it - make sure you use python 3.x).

   For the sources, run: "python setup.py sdist"
   You should now have a /dist directory with both .whl and .tar.gz source versions of allennlp.

5. Check that everything looks correct by uploading the package to the pypi test server:

   twine upload dist/* -r pypitest
   (pypi suggest using twine as other methods upload files via plaintext.)

   Check that you can install it in a virtualenv by running:
   pip install -i https://testpypi.python.org/pypi neuralcoref

6. Upload the final version to actual pypi:
   twine upload dist/* -r pypi

7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.

"""
import os
import subprocess
import sys
import contextlib
from distutils.command.build_ext import build_ext
from distutils.sysconfig import get_python_inc
import distutils.util
from distutils import ccompiler, msvccompiler
from setuptools import Extension, setup, find_packages

def is_new_osx():
    """Check whether we're on OSX >= 10.10"""
    name = distutils.util.get_platform()
    if sys.platform != "darwin":
        return False
    elif name.startswith("macosx-10"):
        minor_version = int(name.split("-")[1].split(".")[1])
        if minor_version >= 7:
            return True
        else:
            return False
    else:
        return False


PACKAGE_DATA = {'': ['*.pyx', '*.pxd'],
                '': ['*.h'],}


PACKAGES = find_packages()


MOD_NAMES = ['neuralcoref.neuralcoref']



COMPILE_OPTIONS = {
    "msvc": ["/Ox", "/EHsc"],
    "mingw32": ["-O2", "-Wno-strict-prototypes", "-Wno-unused-function"],
    "other": ["-O2", "-Wno-strict-prototypes", "-Wno-unused-function"],
}


LINK_OPTIONS = {"msvc": [], "mingw32": [], "other": []}


if is_new_osx():
    # On Mac, use libc++ because Apple deprecated use of
    # libstdc
    COMPILE_OPTIONS["other"].append("-stdlib=libc++")
    LINK_OPTIONS["other"].append("-lc++")
    # g++ (used by unix compiler on mac) links to libstdc++ as a default lib.
    # See: https://stackoverflow.com/questions/1653047/avoid-linking-to-libstdc
    LINK_OPTIONS["other"].append("-nodefaultlibs")


USE_OPENMP_DEFAULT = "0" if sys.platform != "darwin" else None
if os.environ.get("USE_OPENMP", USE_OPENMP_DEFAULT) == "1":
    if sys.platform == "darwin":
        COMPILE_OPTIONS["other"].append("-fopenmp")
        LINK_OPTIONS["other"].append("-fopenmp")
        PACKAGE_DATA["spacy.platform.darwin.lib"] = ["*.dylib"]
        PACKAGES.append("spacy.platform.darwin.lib")

    elif sys.platform == "win32":
        COMPILE_OPTIONS["msvc"].append("/openmp")

    else:
        COMPILE_OPTIONS["other"].append("-fopenmp")
        LINK_OPTIONS["other"].append("-fopenmp")


# By subclassing build_extensions we have the actual compiler that will be used which is really known only after finalize_options
# http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used
class build_ext_options:
    def build_options(self):
        for e in self.extensions:
            e.extra_compile_args += COMPILE_OPTIONS.get(
                self.compiler.compiler_type, COMPILE_OPTIONS["other"]
            )
        for e in self.extensions:
            e.extra_link_args += LINK_OPTIONS.get(
                self.compiler.compiler_type, LINK_OPTIONS["other"]
            )


class build_ext_subclass(build_ext, build_ext_options):
    def build_extensions(self):
        build_ext_options.build_options(self)
        build_ext.build_extensions(self)


# def is_installed(requirement):
#     try:
#         pkg_resources.require(requirement)
#     except pkg_resources.ResolutionError:
#         return False
#     else:
#         return True

# if not is_installed('numpy>=1.11.0') or not is_installed('spacy>=2.1.0'):
#     print(textwrap.dedent("""
#             Error: requirements needs to be installed first.
#             You can install them via:
#             $ pip install -r requirements.txt
#             """), file=sys.stderr)
#     exit(1)

@contextlib.contextmanager
def chdir(new_dir):
    old_dir = os.getcwd()
    try:
        os.chdir(new_dir)
        sys.path.insert(0, new_dir)
        yield
    finally:
        del sys.path[0]
        os.chdir(old_dir)


def generate_cython(root, source):
    print('Cythonizing sources')
    p = subprocess.call([sys.executable,
                         os.path.join(root, 'bin', 'cythonize.py'),
                         source], env=os.environ)
    if p != 0:
        raise RuntimeError('Running cythonize failed')


def is_source_release(path):
    return os.path.exists(os.path.join(path, 'PKG-INFO'))


def setup_package():
    root = os.path.abspath(os.path.dirname(__file__))
    with chdir(root):
        if not is_source_release(root):
            generate_cython(root, 'neuralcoref')

        include_dirs = [
            get_python_inc(plat_specific=True),
            os.path.join(root, 'include')]

        if (ccompiler.new_compiler().compiler_type == 'msvc'
            and msvccompiler.get_build_version() == 9):
            include_dirs.append(os.path.join(root, 'include', 'msvc9'))

        ext_modules = []
        for mod_name in MOD_NAMES:
            mod_path = mod_name.replace('.', '/') + '.cpp'
            extra_link_args = []
            # ???
            # Imported from patch from @mikepb
            # See Issue #267. Running blind here...
            if sys.platform == 'darwin':
                dylib_path = ['..' for _ in range(mod_name.count('.'))]
                dylib_path = '/'.join(dylib_path)
                dylib_path = '@loader_path/%s/neuralcoref/platform/darwin/lib' % dylib_path
                extra_link_args.append('-Wl,-rpath,%s' % dylib_path)
            ext_modules.append(
                Extension(mod_name, [mod_path],
                    language='c++', include_dirs=include_dirs,
                    extra_link_args=extra_link_args))

        setup(name='neuralcoref',
            version='4.0',
            description="Coreference Resolution in spaCy with Neural Networks",
            url='https://github.com/huggingface/neuralcoref',
            author='Thomas Wolf',
            author_email='thomwolf@gmail.com',
            ext_modules=ext_modules,
            classifiers=[
                'Development Status :: 3 - Alpha',
                'Environment :: Console',
                'Intended Audience :: Developers',
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Operating System :: POSIX :: Linux",
                "Operating System :: MacOS :: MacOS X",
                "Operating System :: Microsoft :: Windows",
                "Programming Language :: Cython",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering",
            ],
            install_requires=[
                "numpy>=1.15.0",
                "boto3",
                "requests>=2.13.0,<3.0.0",
                "spacy>=2.1.0,<3.0.0"],
            setup_requires=['wheel', 'spacy>=2.1.0,<3.0.0'],
            python_requires=">=3.6",
            packages=PACKAGES,
            package_data=PACKAGE_DATA,
            keywords='NLP chatbots coreference resolution',
            license='MIT',
            zip_safe=False,
            platforms='any',
            cmdclass={"build_ext": build_ext_subclass})

if __name__ == '__main__':
    setup_package()

# PROJECT: huggingface_neuralcoref FILE: examples/server.py
#!/usr/bin/env python
"""Coreference resolution server example.
A simple server serving the coreference system.
"""

import json
from wsgiref.simple_server import make_server
import falcon
import spacy
import neuralcoref

# Python 3
unicode_ = str


class AllResource(object):
    def __init__(self):
        self.nlp = spacy.load("en")
        neuralcoref.add_to_pipe(self.nlp)
        print("Server loaded")
        self.response = None

    def on_get(self, req, resp):
        self.response = {}

        text_param = req.get_param_as_list("text")
        print("text: ", text_param)
        if text_param is not None:
            text = ",".join(text_param) if isinstance(text_param, list) else text_param
            text = unicode_(text)
            doc = self.nlp(text)
            if doc._.has_coref:
                mentions = [
                    {
                        "start": mention.start_char,
                        "end": mention.end_char,
                        "text": mention.text,
                        "resolved": cluster.main.text,
                    }
                    for cluster in doc._.coref_clusters
                    for mention in cluster.mentions
                ]
                clusters = list(
                    list(span.text for span in cluster)
                    for cluster in doc._.coref_clusters
                )
                resolved = doc._.coref_resolved
                self.response["mentions"] = mentions
                self.response["clusters"] = clusters
                self.response["resolved"] = resolved

        resp.body = json.dumps(self.response)
        resp.content_type = "application/json"
        resp.append_header("Access-Control-Allow-Origin", "*")
        resp.status = falcon.HTTP_200


if __name__ == "__main__":
    RESSOURCE = AllResource()
    APP = falcon.API()
    APP.add_route("/", RESSOURCE)
    HTTPD = make_server("0.0.0.0", 8000, APP)
    HTTPD.serve_forever()

# PROJECT: NVIDIA_sentiment-discovery FILE: pretrain.py
# transformer_main.py
import argparse
import os
import sys
import time
import math
import random

import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable

from fp16 import FP16_Module, FP16_Optimizer

import data
import model as m
from model import DistributedDataParallel as DDP

from reparameterization import apply_weight_norm, remove_weight_norm
from configure_data import configure_data
from learning_rates import AnnealingLR, WarmupLR, SlantedTriangularLR
from arguments import add_general_args, add_model_args, add_unsupervised_data_args

rnn_model = None

def setup_model_and_optim(args, train_data, tokenizer):
    ntokens = args.data_size
    if args.model.lower() == 'transformer':
        embed_tokens = m.Embedding(ntokens, args.decoder_embed_dim, padding_idx=tokenizer.command_name_map['pad'].Id)
        model = m.TransformerModel(m.DecoderPreprocessor(args, embed_tokens),
                                    m.TransformerDecoder(args, embed_tokens))
    else:
        model = m.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)
        global rnn_model
        rnn_model = model
    LR_Warmer = None
    print('* number of parameters: %d' % sum([p.nelement() for p in model.parameters()]))
    if args.cuda:
        model.cuda()


    optim = None
    if args.load is not None and args.load != '':
        sd = torch.load(args.load, map_location='cpu')
        if args.load_optim:
            #optim_sd = torch.load(os.path.join(os.path.dirname(args.load), 'optim.pt'), map_location='cpu')
            rng = torch.load(os.path.join(os.path.dirname(args.load), 'rng.pt'))
            torch.cuda.set_rng_state(rng[0])
            torch.set_rng_state(rng[1])
        try:
            model.load_state_dict(sd)
        except:
            if hasattr(model, 'rnn'):
                apply_weight_norm(model.rnn, hook_child=False)
            else:
                apply_weight_norm(model, hook_child=False)
            model.load_state_dict(sd)
            remove_weight_norm(model)

    if not args.no_weight_norm:
        if hasattr(model, 'rnn'):
            apply_weight_norm(model.rnn, hook_child=False)
        else:
            apply_weight_norm(model, hook_child=False)

    if optim is None:
        optim_choice = 'Adam' if args.stlr_cut_frac else args.optim
        if args.fp16:
            model = FP16_Module(model)
            optim = eval('torch.optim.'+args.optim)(model.parameters(), lr=args.lr)
            optim = FP16_Optimizer(optim,
                               static_loss_scale=args.loss_scale,
                               dynamic_loss_scale=args.dynamic_loss_scale)
        else:
            optim = eval('torch.optim.'+args.optim)(model.parameters(), lr=args.lr)

    if args.load_optim:
        optim.load_state_dict(optim_sd)

    # add linear learning rate scheduler
    if train_data is not None:
        if args.constant_decay:
            num_iters = args.constant_decay
        else:
            num_iters = args.train_iters * args.epochs

        init_step = -1
        if args.load_optim:
            #TODO: this no longer makes sense given the new data loaders
            init_step = optim_sd['iter']-optim_sd['skipped_iter']
            train_data.batch_sampler.start_iter = (optim_sd['iter'] % len(train_data)) + 1

        warmup_iter = args.warmup * num_iters

        if args.stlr_cut_frac is not None:
            LR = SlantedTriangularLR(optim, cut_frac=args.stlr_cut_frac, num_iters=num_iters)
        else:
            LR = AnnealingLR(optim, start_lr=args.lr, warmup_iter=warmup_iter, num_iters=num_iters, decay_style=args.decay_style)

        if args.warmup != 0:
            LR_Warmer = WarmupLR(optim, warmup_iter, last_iter=init_step)

    # wrap model for distributed training
    if args.world_size > 1:
        model = DDP(model)

    criterion = nn.CrossEntropyLoss(reduce=False)
    return model, optim, LR, LR_Warmer, criterion

###############################################################################
# Training code
###############################################################################

# get_batch subdivides the source data into chunks of length args.seq_length.
# If source is equal to the example output of the data loading example, with
# a seq_length limit of 2, we'd get the following two Variables for i = 0:
#  a g m s   b h n t 
#  b h n t   c i o u 
# Note that despite the name of the function, the subdivison of data is not
# done along the batch dimension (i.e. dimension 1), since that was handled
# by the data loader. The chunks are along dimension 0, corresponding
# to the seq_len dimension in the LSTM. A Variable representing an appropriate
# shard reset mask of the same dimensions is also returned.

def get_batch(data, args):
    reset_mask_batch = data[1].long()
    padding_mask_batch = data[2].float()
    data = data[0].long()
    if args.cuda:
        data = data.cuda()
        reset_mask_batch = reset_mask_batch.cuda()
        padding_mask_batch = padding_mask_batch.cuda()
    text_batch = Variable(data[:,:-1].t().contiguous(), requires_grad=False)
    target_batch = Variable(data[:,1:].t().contiguous(), requires_grad=False)
    reset_mask_batch = Variable(reset_mask_batch[:,:text_batch.size(0)].t().contiguous(), requires_grad=False)
    padding_mask_batch = Variable(padding_mask_batch[:,:text_batch.size(0)].t().contiguous(), requires_grad=False)
    return text_batch, target_batch, reset_mask_batch, padding_mask_batch

def init_hidden(args):
    if rnn_model is not None:
        rnn_model.rnn.init_hidden(args.batch_size)

def evaluate(data_source, model, criterion, args):
    # Turn on evaluation mode which disables dropout.
    model.eval()
    init_hidden(args)
    total_loss = 0
    ntokens = args.data_size
    max_iters = args.eval_iters
    with torch.no_grad():
        data_iter = iter(data_source)
        i = 0
        while i < max_iters:
            batch = next(data_iter)
            data, targets, reset_mask, padding_mask = get_batch(batch, args)

            output, hidden = model(data, reset_mask=reset_mask)
            losses = criterion(output.view(-1, ntokens).contiguous().float(), targets.view(-1).contiguous())
            padding_mask = padding_mask.view(-1)
            portion_unpadded = padding_mask.sum() / padding_mask.size(0)
            loss = portion_unpadded * torch.mean(losses * (padding_mask.view(-1).float()))
            if isinstance(model, DDP):
                torch.distributed.all_reduce(loss.data)
                loss.data /= args.world_size
            total_loss += loss.data.float()
            i+=1
    return (total_loss / max_iters).item()

def train(epoch, model, optim, train_data, LR, LR_Warmer, criterion, args, total_iters=0, skipped_iters=0, elapsed_time=False):
    # Turn on training mode which enables dropout.
    model.train()
    init_hidden(args)
    total_loss = 0
    start_time = time.time()
    t0 = start_time
    ntokens = args.data_size
    curr_loss = 0.
    distributed = isinstance(model, DDP)
    max_iters = args.train_iters
    def log(epoch, i, lr, ms_iter, total_time, loss, scale):
        print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:.2E} | ms/batch {:.3E} | total time {:.3E}\
                  loss {:.2E} | ppl {:8.2f} | loss scale {:8.2f}'.format(
                      epoch, i, max_iters, lr,
                      ms_iter, total_time, loss, math.exp(min(loss, 20)), scale
                  )
        )
    i = 0
    data_iter = iter(train_data)
    while i < max_iters:
        batch = next(data_iter)
        data, targets, reset_mask, padding_mask = get_batch(batch, args)
        optim.zero_grad()
        output, _ = model(data, reset_mask=reset_mask, chkpt_grad=args.chkpt_grad)

        losses = criterion(output.view(-1, ntokens).contiguous().float(), targets.view(-1).contiguous())
        padding_mask = padding_mask.view(-1)
        portion_unpadded = padding_mask.sum() / padding_mask.size(0)
        loss = portion_unpadded * torch.mean(losses * (padding_mask.view(-1).float()))
        total_loss += loss.data.float()

        if args.fp16:
            optim.backward(loss, update_master_grads=False)
        else:
            loss.backward()

        if distributed:
            torch.distributed.all_reduce(loss.data)
            loss.data = loss.data/args.world_size
            model.allreduce_params()

        # clipping gradients helps prevent the exploding gradient problem in RNNs / LSTMs.
        if args.clip > 0:
            if not args.fp16:
                torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
            else:
                optim.clip_master_grads(clip=args.clip)

        if args.fp16:
            optim.update_master_grads()

        optim.step()


        # step learning rate and log training progress
        lr = optim.param_groups[0]['lr']
        if not args.fp16:
            LR.step()
            if args.warmup != 0:
                LR_Warmer.step()
        else:
            # if fp16 optimizer skips gradient step due to explosion do not step lr
            if not optim.overflow:
                LR.step()
                if args.warmup != 0:
                    LR_Warmer.step()
            else:
                skipped_iters += 1

        if ((i+1) % args.log_interval == 0):
            cur_loss = total_loss.item() / args.log_interval
            cur_time = time.time()
            elapsed = cur_time - start_time
            total_elapsed = cur_time - t0 + elapsed_time
            log(epoch, i+1, lr, elapsed * 1000 / args.log_interval, total_elapsed, 
                cur_loss, args.loss_scale if not args.fp16 else optim.loss_scale)
            total_loss = 0
            start_time = cur_time
            sys.stdout.flush()

        # save current model progress. If distributed only save from worker 0
        if args.save_iters and total_iters % (args.save_iters) == 0 and total_iters > 0 and args.rank < 1:
            if args.rank < 1:
                with open(os.path.join(os.path.splitext(args.save)[0], 'e%s.pt'%(str(total_iters),)), 'wb') as f:
                    torch.save(model.state_dict(), f)
                if args.save_optim:
                    with open(os.path.join(os.path.splitext(args.save)[0], 'optim.pt'), 'wb') as f:
                        optim_sd = optim.state_dict()
                        optim_sd['iter'] = total_iters
                        optim_sd['skipped_iter'] = skipped_iters
                        torch.save(optim_sd, f)
                        del optim_sd

                    with open(os.path.join(os.path.splitext(args.save)[0], 'rng.pt'), 'wb') as f:
                        torch.save((torch.cuda.get_rng_state(), torch.get_rng_state()),f)
            if args.cuda:
                torch.cuda.synchronize()
        total_iters += 1
        i+=1
    #final logging
    elapsed_iters = max_iters % args.log_interval
    if elapsed_iters == 0:
        return cur_loss, skipped_iters

    cur_time = time.time()
    elapsed = cur_time - start_time
    total_elapsed = cur_time - t0 + elapsed_time
    cur_loss = total_loss.item() / args.log_interval
    log(epoch, max_iters, lr, elapsed * 1000/ elapsed_iters, total_elapsed,
        cur_loss, args.loss_scale if not args.fp16 else optim.loss_scale)

    return cur_loss, skipped_iters


def main():
    parser = argparse.ArgumentParser(description='PyTorch Sentiment-Discovery Language Modeling')
    parser = add_general_args(parser)
    parser = add_model_args(parser)
    data_config, data_parser = add_unsupervised_data_args(parser)
    args = parser.parse_args()

    torch.backends.cudnn.enabled = False
    args.cuda = torch.cuda.is_available()

    if args.multinode_init:
        args.rank = int(os.getenv('RANK', 0))
        args.world_size = int(os.getenv("WORLD_SIZE", 1))

    # initialize distributed process group and set device
    if args.rank > 0:
        torch.cuda.set_device(args.rank % torch.cuda.device_count())

    if args.world_size > 1:
        init_method='tcp://'
        if not args.multinode_init:
            init_method+='localhost:6000'
        else:
            master_ip = os.getenv('MASTER_ADDR', 'localhost')
            master_port = os.getenv('MASTER_PORT', '6666')
            init_method+=master_ip+':'+master_port
        torch.distributed.init_process_group(backend=args.distributed_backend, world_size=args.world_size,
                                             rank=args.rank, init_method=init_method)
    # Set the random seed manually for reproducibility.
    if args.seed is not None and args.seed > 0:
        random.seed(args.seed)
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if args.cuda:
            torch.cuda.manual_seed(args.seed)

    if args.loss_scale != 1 and args.dynamic_loss_scale:
        raise RuntimeError("Static loss scale and dynamic loss scale cannot be used together.")

    (train_data, val_data, test_data), tokenizer = data_config.apply(args)

    args.data_size = tokenizer.num_tokens
    model, optim, LR, LR_Warmer, criterion = setup_model_and_optim(args, train_data, tokenizer)

    lr = args.lr
    best_val_loss = None

    # If saving process intermittently create directory for saving
    if args.save_iters > 0 and not os.path.exists(os.path.splitext(args.save)[0]) and args.rank < 1:
        os.makedirs(os.path.splitext(args.save)[0])

    # At any point you can hit Ctrl + C to break out of training early.
    try:
        total_iters = 0
        elapsed_time = 0
        skipped_iters = 0
        if args.load_optim:
            total_iters = optim_sd['iter']
            skipped_iters = optim_sd['skipped_iter']
        for epoch in range(1, args.epochs+1):
            if args.rank <= 0:
                with open(args.save+'.train_lock', 'wb') as f:
                    pass
            epoch_start_time = time.time()
            val_loss, skipped_iters = train(epoch, model, optim, train_data, LR, LR_Warmer, criterion,
                                            args, total_iters, skipped_iters, elapsed_time)
            elapsed_time += time.time() - epoch_start_time
            total_iters += args.train_iters
            if val_data is not None:
                print('entering eval')
                val_loss = evaluate(val_data, model, criterion, args)
            print('-' * 89)
            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.4f} | '
                  'valid ppl {:8.4f}'.format(epoch, (time.time() - epoch_start_time),
                                             val_loss, math.exp(min(val_loss, 20))))
            print('-' * 89)
            # Save the model if the validation loss is the best we've seen so far.
            if (not best_val_loss or val_loss < best_val_loss) and args.rank <= 0:
                torch.save(model.state_dict(), args.save)
                best_val_loss = val_loss
            if args.world_size == 1 or torch.distributed.get_rank() == 0:
                try:
                    os.remove(args.save+'.train_lock')
                except:
                    pass
#            if args.world_size > 1:
#                torch.distributed.barrier()
            torch.cuda.synchronize()

    except KeyboardInterrupt:
        print('-' * 89)
        print('Exiting from training early')

    #while os.path.exists(args.save+'.train_lock'):
    #    time.sleep(1)

    # Load the best saved model.
    #if os.path.exists(args.save):
    #    model.load_state_dict(torch.load(args.save, 'cpu'))

    # if not args.no_weight_norm and args.rank <= 0:
    #    remove_weight_norm(model)
    #    torch.save(model.state_dict(), args.save)

    if test_data is not None:
        # Run on test data.
        print('entering test')
        test_loss = evaluate(test_data, model, criterion, args)
        print('=' * 89)
        print('| End of training | test loss {:5.4f} | test ppl {:8.4f}'.format(
            test_loss, math.exp(min(test_loss, 20))))
        print('=' * 89)

if __name__ == "__main__":
    main()




# PROJECT: NVIDIA_sentiment-discovery FILE: generate.py
###############################################################################
# Language Modeling on Penn Tree Bank
#
# This file generates new sentences sampled from the language model
#
###############################################################################

import os
import math

import argparse

import torch
from torch.autograd import Variable

from apex.reparameterization import apply_weight_norm, remove_weight_norm

import model

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style({'font.family': 'monospace'})


parser = argparse.ArgumentParser(description='PyTorch Sentiment Discovery Generation/Visualization')

# Model parameters.
parser.add_argument('--model', type=str, default='mLSTM',
                    help='type of recurrent net (RNNTanh, RNNReLU, LSTM, mLSTM, GRU')
parser.add_argument('--emsize', type=int, default=64,
                    help='size of word embeddings')
parser.add_argument('--nhid', type=int, default=4096,
                    help='number of hidden units per layer')
parser.add_argument('--nlayers', type=int, default=1,
                    help='number of layers')
parser.add_argument('--dropout', type=float, default=0.0,
                    help='dropout applied to layers (0 = no dropout)')
parser.add_argument('--all_layers', action='store_true',
                    help='if more than one layer is used, extract features from all layers, not just the last layer')
parser.add_argument('--tied', action='store_true',
                    help='tie the word embedding and softmax weights')
parser.add_argument('--load_model', type=str, default='model.pt',
                    help='model checkpoint to use')
parser.add_argument('--save', type=str, default='generated.txt',
                    help='output file for generated text')
parser.add_argument('--gen_length', type=int, default='1000',
                    help='number of tokens to generate')
parser.add_argument('--seed', type=int, default=-1,
                    help='random seed')
parser.add_argument('--temperature', type=float, default=1.0,
                    help='temperature - higher will increase diversity')
parser.add_argument('--log-interval', type=int, default=100,
                    help='reporting interval')
parser.add_argument('--fp16', action='store_true',
                    help='run in fp16 mode')
parser.add_argument('--neuron', type=int, default=-1,
                    help='''specifies which neuron to analyze for visualization or overwriting.
                         Defaults to maximally weighted neuron during classification steps''')
parser.add_argument('--visualize', action='store_true',
                    help='generates heatmap of main neuron activation [not working yet]')
parser.add_argument('--overwrite', type=float, default=None,
                    help='Overwrite value of neuron s.t. generated text reads as a +1/-1 classification')
parser.add_argument('--text', default='',
                    help='warm up generation with specified text first')
args = parser.parse_args()

args.data_size = 256

args.cuda = torch.cuda.is_available()

# Set the random seed manually for reproducibility.
if args.seed >= 0:
    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

#if args.temperature < 1e-3:
#    parser.error("--temperature has to be greater or equal 1e-3")

model = model.RNNModel(args.model, args.data_size, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)
if args.cuda:
    model.cuda()

if args.fp16:
    model.half()
with open(args.load_model, 'rb') as f:
    sd = torch.load(f)
try:
    model.load_state_dict(sd)
except:
    apply_weight_norm(model.rnn)
    model.load_state_dict(sd)
    remove_weight_norm(model)

def get_neuron_and_polarity(sd, neuron):
    """return a +/- 1 indicating the polarity of the specified neuron in the module"""
    if neuron == -1:
        neuron = None
    if 'classifier' in sd:
        sd = sd['classifier']
        if 'weight' in sd:
            weight = sd['weight']
        else:
            return neuron, 1
    else:
        return neuron, 1
    if neuron is None:
        val, neuron = torch.max(torch.abs(weight[0].float()), 0)
        neuron = neuron[0]
    val = weight[0][neuron]
    if val >= 0:
        polarity = 1
    else:
        polarity = -1
    return neuron, polarity

def process_hidden(cell, hidden, neuron, mask=False, mask_value=1, polarity=1):
    feat = cell.data[:, neuron]
    rtn_feat = feat.clone()
    if mask:
#        feat.fill_(mask_value*polarity)
        hidden.data[:, neuron].fill_(mask_value*polarity)
    return rtn_feat[0]

def model_step(model, input, neuron=None, mask=False, mask_value=1, polarity=1):
    out, _ = model(input)
    if neuron is not None:
        hidden = model.rnn.rnns[-1].hidden
        if len(hidden) > 1:
            hidden, cell = hidden
        else:
            hidden = cell = hidden
        feat = process_hidden(cell, hidden, neuron, mask, mask_value, polarity)
        return out, feat
    return out

def sample(out, temperature):
    if temperature == 0:
        char_idx = torch.max(out.squeeze().data, 0)[1][0]
    else:
        word_weights = out.float().squeeze().data.div(args.temperature).exp().cpu()
        char_idx = torch.multinomial(word_weights, 1)[0]
    return char_idx

def process_text(text, model, input, temperature, neuron=None, mask=False, overwrite=1, polarity=1):
    chrs = []
    vals = []
    for c in text:
        input.data.fill_(int(ord(c)))
        if neuron:
            ch, val = model_step(model, input, neuron, mask, overwrite, polarity)
            vals.append(val)
        else:
            ch = model_step(model, input, neuron, mask, overwrite, polarity)
#        ch = sample(ch, temperature)
    input.data.fill_(sample(ch, temperature))
    chrs = list(text)
#    chrs.append(chr(ch))
    return chrs, vals

def generate(gen_length, model, input, temperature, neuron=None, mask=False, overwrite=1, polarity=1):
    chrs = []
    vals = []
    for i in range(gen_length):
        chrs.append(chr(input.data[0]))
        if neuron:
            ch, val = model_step(model, input, neuron, mask, overwrite, polarity)
            vals.append(val)
        else:
            ch = model_step(model, input, neuron, mask, overwrite, polarity)
        ch = sample(ch, temperature)
        input.data.fill_(ch)
#        chrs.append(chr(ch))
#    chrs.pop()
    return chrs, vals

def make_heatmap(text, values, save=None, polarity=1):
    cell_height=.325
    cell_width=.15
    n_limit = 74
    text = list(map(lambda x: x.replace('\n', '\\n'), text))
    num_chars = len(text)
    total_chars = math.ceil(num_chars/float(n_limit))*n_limit
    mask = np.array([0]*num_chars + [1]*(total_chars-num_chars))
    text = np.array(text+[' ']*(total_chars-num_chars))
    values = np.array(values+[0]*(total_chars-num_chars))
    values *= polarity

    values = values.reshape(-1, n_limit)
    text = text.reshape(-1, n_limit)
    mask = mask.reshape(-1, n_limit)
    num_rows = len(values)
    plt.figure(figsize=(cell_width*n_limit, cell_height*num_rows))
    hmap=sns.heatmap(values, annot=text, mask=mask, fmt='', vmin=-1, vmax=1, cmap='RdYlGn',
                     xticklabels=False, yticklabels=False, cbar=False)
    plt.tight_layout()
    if save is not None:
        plt.savefig(save)
    # clear plot for next graph since we returned `hmap`
    plt.clf()
    return hmap


neuron, polarity = get_neuron_and_polarity(sd, args.neuron)
neuron = neuron if args.visualize or args.overwrite is not None else None
mask = args.overwrite is not None
    
model.eval()

hidden = model.rnn.init_hidden(1)
input = Variable(torch.LongTensor([int(ord('\n'))]))
if args.cuda:
    input = input.cuda()
input = input.view(1,1).contiguous()
model_step(model, input, neuron, mask, args.overwrite, polarity)
input.data.fill_(int(ord(' ')))
out = model_step(model, input, neuron, mask, args.overwrite, polarity)
if neuron is not None:
    out = out[0]
input.data.fill_(sample(out, args.temperature))

outchrs = []
outvals = []
#with open(args.save, 'w') as outf:
with torch.no_grad():
    if args.text != '':
        chrs, vals = process_text(args.text, model, input, args.temperature, neuron, mask, args.overwrite, polarity)
        outchrs += chrs
        outvals += vals
    chrs, vals = generate(args.gen_length, model, input, args.temperature, neuron, mask, args.overwrite, polarity)
    outchrs += chrs
    outvals += vals
outstr = ''.join(outchrs)
print(outstr)
with open(args.save, 'w') as f:
    f.write(outstr)

if args.visualize:
    make_heatmap(outchrs, outvals, os.path.splitext(args.save)[0]+'.png', polarity)

# PROJECT: NVIDIA_sentiment-discovery FILE: arguments.py
###############################################################################
# BSD 3-Clause License
#
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Author & Contact: Raul Puri (raulp@nvidia.com)
###############################################################################

from configure_data import configure_data

def add_general_args(parser):
    group = parser.add_argument_group('general', 'general purpose arguments')
    group.add_argument('--model', type=str, default='mLSTM',
                        help='type of recurrent net (RNNTanh, RNNReLU, LSTM, mLSTM, GRU)')
    group.add_argument('--lr', type=float, default=5e-4,
                        help='initial learning rate')
    group.add_argument('--constant-decay', type=int, default=None,
                        help='number of iterations to decay LR over,' + \
                             ' None means decay to zero over training')
    group.add_argument('--clip', type=float, default=0,
                        help='gradient clipping')
    group.add_argument('--epochs', type=int, default=1,
                        help='upper epoch limit')
    group.add_argument('--tied', action='store_true',
                        help='tie the word embedding and softmax weights')
    group.add_argument('--seed', type=int, default=1234,
                        help='random seed')
    group.add_argument('--log-interval', type=int, default=100, metavar='N',
                        help='report interval')
    group.add_argument('--save', type=str,  default='lang_model.pt',
                        help='path to save the final model')
    group.add_argument('--load', type=str, default=None,
                        help='path to a previously saved model checkpoint')
    group.add_argument('--load-optim', action='store_true',
                        help='load most recent optimizer to resume training')
    group.add_argument('--save-iters', type=int, default=10000, metavar='N',
                        help='save current model progress interval')
    group.add_argument('--save-optim', action='store_true',
                        help='save most recent optimizer')
    group.add_argument('--fp16', action='store_true',
                        help='Run model in pseudo-fp16 mode (fp16 storage fp32 math).')
    group.add_argument('--dynamic-loss-scale', action='store_true',
                        help='Dynamically look for loss scalar for fp16 convergance help.')
    group.add_argument('--no-weight-norm', action='store_true',
                        help='Add weight normalization to model.')
    group.add_argument('--loss-scale', type=float, default=1,
                        help='Static loss scaling, positive power of 2 values can improve fp16 convergence.')
    group.add_argument('--world-size', type=int, default=1,
                        help='number of distributed workers')
    group.add_argument('--distributed-backend', default='gloo',
                        help='which backend to use for distributed training. One of [gloo, nccl]')
    group.add_argument('--rank', type=int, default=-1,
                        help='distributed worker rank. Typically set automatically from multiproc.py')
    group.add_argument('--optim', default='Adam',
                        help='One of PyTorch\'s optimizers (Adam, SGD, etc). Default: Adam')
    group.add_argument('--chkpt-grad', action='store_true',
                        help='checkpoint gradients to allow for training with larger models and sequences')
    group.add_argument('--multinode-init', action='store_true',
                        help='initialize multinode. Environment variables should be set as according to https://pytorch.org/docs/stable/distributed.html')
    return parser

def add_unsupervised_data_args(parser):
    data_config, data_group = configure_data(parser)
    # Set unsupervised L2R language modeling option defaults
    data_config.set_defaults(data_set_type='L2R', transpose=True)
    data_group.set_defaults(split='100,1,1')
    # Create unsupervised-L2R-specific options
    group = parser.add_argument_group('language modeling data options')
    group.add_argument('--seq-length', type=int, default=256,
                        help="Maximum sequence length to process (for unsupervised rec)")
    group.add_argument('--eval-seq-length', type=int, default=256,
                        help="Maximum sequence length to process for evaluation")
    group.add_argument('--lazy', action='store_true',
                        help='whether to lazy evaluate the data set')
    group.add_argument('--persist-state', type=int, default=1,
                        help='0=reset state after every sample in a shard, 1=reset state after every shard, -1=never reset state')
    group.add_argument('--train-iters', type=int, default=1000,
                        help="""number of iterations per epoch to run training for""")
    group.add_argument('--eval-iters', type=int, default=100,
                        help="""number of iterations per epoch to run validation/test for""")
    group.add_argument('--decay-style', type=str, default=None, choices=['constant', 'linear', 'cosine', 'exponential'],
                        help='one of constant(None), linear, cosine, or exponential')
    group.add_argument('--stlr-cut-frac', type=float, default=None,
                        help='what proportion of iterations to peak the slanted triangular learning rate')
    group.add_argument('--warmup', type=float, default=0,
                        help='percentage of data to warmup on (.03 = 3% of all training iters). Default 0')
    return data_config, parser

def add_model_args(parser):
    args, _ = parser.parse_known_args()
    if args.model.lower() == 'transformer':
        return add_transformer_args(parser) 
    else:
        return add_recurrent_args(parser)

def add_recurrent_args(parser):
    group = parser.add_argument_group('recurrent', 'arguments for building recurrent nets')
    group.add_argument('--num-hidden-warmup', type=int, default=0,
                        help='number of times to conduct hidden state warmup passes through inputs to be used for transfer tasks')
    group.add_argument('--emsize', type=int, default=64,
                        help='size of word embeddings')
    group.add_argument('--nhid', type=int, default=4096,
                        help='number of hidden units per layer')
    group.add_argument('--nlayers', type=int, default=1,
                        help='number of layers')
    group.add_argument('--dropout', type=float, default=0.0,
                        help='dropout applied to layers (0 = no dropout)')
    group.add_argument('--neural-alphabet', action='store_true',
                       help='whether to use the neural alphabet encoder structure')
    group.add_argument('--alphabet-size', type=int, default=128,
                       help='number of letters in neural alphabet')
    group.add_argument('--ncontext', type=int, default=2,
                       help='number of context characters used in neural alphabet encoder structure')
    group.add_argument('--residuals', action='store_true',
                        help='whether to implement residual connections between stackedRNN layers')
    return parser

def add_transformer_args(parser):
    group = parser.add_argument_group('transformer', 'args for specifically building a transformer network')
    group.add_argument('--dropout', type=float, default=0.1,
                        help='dropout probability -- transformer only')
    group.add_argument('--attention-dropout', type=float, default=0.0,
                        help='dropout probability for attention weights -- transformer only')
    group.add_argument('--relu-dropout', type=float, default=0.1,
                        help='dropout probability after ReLU in FFN -- transformer only')
    #ignore the encoder args for transformer. That's meant for seq2seq transformer
    group.add_argument('--encoder-embed-path', type=str, default=None,
                        help='path to pre-trained encoder embedding')
    group.add_argument('--encoder-embed-dim', type=int, default=64, # originally 512 but 64 for char level
                        help='encoder embedding dimension')
    group.add_argument('--encoder-ffn-embed-dim', type=int, default=256, # originally 2048 but scaled for char level
                        help='encoder embedding dimension for FFN')
    group.add_argument('--encoder-layers', type=int, default=6,
                        help='num encoder layers')
    group.add_argument('--encoder-attention-heads', type=int, default=8,
                        help='num encoder attention heads')
    group.add_argument('--encoder-normalize-before', default=False, action='store_true',
                        help='apply layernorm before each encoder block')
    group.add_argument('--encoder-learned-pos', default=False, action='store_true',
                        help='use learned positional embeddings in the encoder')
    group.add_argument('--decoder-embed-path', type=str, default=None,
                        help='path to pre-trained decoder embedding')
    group.add_argument('--decoder-embed-dim', type=int, default=64, # originally 512 but 64 for char level
                        help='decoder embedding dimension')
    group.add_argument('--decoder-ffn-embed-dim', type=int, default=256, # originally 2048 but scaled for char level
                        help='decoder embedding dimension for FFN')
    group.add_argument('--decoder-layers', type=int, default=6,
                        help='num decoder layers')
    group.add_argument('--decoder-attention-heads', type=int, default=8,
                        help='num decoder attention heads')
    group.add_argument('--decoder-learned-pos', default=False, action='store_true',
                        help='use learned positional embeddings in the decoder')
    group.add_argument('--decoder-normalize-before', default=False, action='store_true',
                        help='apply layernorm before each decoder block')
    group.add_argument('--share-decoder-input-output-embed', default=False, action='store_true',
                        help='share decoder input and output embeddings')
    group.add_argument('--share-all-embeddings', default=False, action='store_true',
                        help='share encoder, decoder and output embeddings'
                             ' (requires shared dictionary and embed dim)')
    group.add_argument('--use-final-embed', action='store_true',
                        help='whether to use the final timestep embeddings as output of transformer (in classification)')
    return parser

def add_classifier_model_args(parser):
    group = parser.add_argument_group('classifier', 'arguments used in training a classifier on top of a language model')
    group.add_argument('--max-seq-len', type=int, default=None,
                        help='maximum sequence length to use for classification. Transformer uses a lot of memory and needs shorter sequences.')
    group.add_argument('--classifier-hidden-layers', default=None, nargs='+',
                        help='sizes of hidden layers for binary classifier on top of language model, so excluding the input layer and final "1"')
    group.add_argument('--classifier-hidden-activation', type=str, default='PReLU',
                        help='[defaults to PReLU] activations used in hidden layers of MLP classifier (ReLU, Tanh, torch.nn module names)')
    group.add_argument('--classifier-dropout', type=float, default=0.1,
                        help='Dropout in layers of MLP classifier')
    group.add_argument('--all-layers', action='store_true',
                        help='if more than one layer is used, extract features from all layers, not just the last layer')
    group.add_argument('--concat-max', action='store_true',
                        help='whether to concatenate max pools onto cell/hidden states of RNNFeaturizer')
    group.add_argument('--concat-min', action='store_true',
                        help='whether to concatenate min pools onto cell/hidden states of RNNFeaturizer')
    group.add_argument('--concat-mean', action='store_true',
                        help='whether to concatenate mean pools onto cell/hidden states of RNNFeaturizer')
    group.add_argument('--get-hidden', action='store_true',
                        help='whether to use the hidden state (as opposed to cell state) as features for classifier')
    group.add_argument('--neurons', default=1, type=int,
                        help='number of nenurons to extract as features')
    group.add_argument('--heads-per-class', type=int, default=1,
                       help='set > 1 for multiple heads per class prediction (variance, regularlization)')
    parser.add_argument('--use-softmax', action='store_true', help='use softmax for classification')
    group.add_argument('--double-thresh', action='store_true',
                       help='whether to report all metrics at once')
    group.add_argument('--dual-thresh', action='store_true',
                        help='for 2 columns positive and negative, thresholds classes s.t. positive, negative, neutral labels are available')
    group.add_argument('--joint-binary-train', action='store_true',
                       help='Train with dual thresholded (positive/negative/neutral) classes and other normal binary classes.\
                             Arguments to non-binary-cols must be passed with positive negative classes first.\
                             Ex: `--non-binary-cols positive negative <other classes>`')

    group.set_defaults(epochs=5)
    return parser

def add_sentiment_transfer_args(parser):
    data_config, data_group = configure_data(parser)
    # Set transfer learning data option defaults
    data_group.set_defaults(split='1.', data=['data/binary_sst/train.csv'])
    data_group.set_defaults(valid=['data/binary_sst/val.csv'], test=['data/binary_sst/test.csv'])
    # Create transfer-learning-specific options
    group = parser.add_argument_group('sentiment_transfer', 'arguments used for sentiment_transfer script')
    group.add_argument('--mcc', action='store_true',
                        help='whether to use the matthews correlation coefficient as a measure of accuracy (for CoLA)')
    group.add_argument('--save-results', type=str,  default='sentiment',
                        help='path to save intermediate and final results of transfer')
    group.add_argument('--no-test-eval', action='store_true',
                        help='whether to not evaluate the test model (useful when your test set has no labels)')
    group.add_argument('--write-results', type=str, default='',
                        help='write results of model on test (or train if none is specified) data to specified filepath ')
    group.add_argument('--use-cached', action='store_true',
                        help='reuse cached featurizations from a previous run')
    group.add_argument('--drop-neurons', action='store_true',
                        help='drop top neurons instead of keeping them')

    return data_config, data_group, group, parser

def add_run_classifier_args(parser):
    data_config, data_group = configure_data(parser)
    # Set classification data option defaults
    data_group.set_defaults(split='1.', data=['data/binary_sst/train.csv'])
    data_group.set_defaults(shuffle=False)
    # Create classification-specific options
    group = parser.add_argument_group('run_classifier', 'arguments used for run classifier script')
    group.add_argument('--save_probs', type=str,  default='clf_results.npy',
                        help='path to save numpy of predicted probabilities')
    group.add_argument('--write-results', type=str, default='',
                        help='path to location for CSV -- write results of model on data \
                             input strings + results and variances. Will not write if empty') 
    return data_config, data_group, group, parser

def add_finetune_classifier_args(parser):
    data_config, data_group = configure_data(parser)
    # Set finetuning data option defaults
    data_group.set_defaults(split='1.', data=['data/binary_sst/train.csv'])
    data_group.set_defaults(valid=['data/binary_sst/val.csv'], test=['data/binary_sst/test.csv'])
    data_group.set_defaults(shuffle=True)
    # Create finetuning-specific options
    parser.set_defaults(get_hidden=True)
    data_group.add_argument('--seq-length', type=int, default=256,
                             help="Maximum sequence length to process (for unsupervised rec)")
    data_group.add_argument('--lazy', action='store_true',
                             help='whether to lazy evaluate the data set')
    group = parser.add_argument_group('finetune_classifier', 'arguments used for finetune script')
    group.add_argument('--use-logreg', action='store_true',
                        help='use scikitlearn logistic regression instead of finetuning whole classifier')
    group.add_argument('--stlr-cut-frac', type=float, default=None,
                        help='what proportion of iterations to peak the slanted triangular learning rate')
    group.add_argument('--cos-cut-frac', type=float, default=None,
                        help='what proportion of iterations to peak the cosine learning rate')
    group.add_argument('--lr-decay', type=float, default=1.0,
                        help='amount to multiply lr by to decay every epoch')
    group.add_argument('--momentum', type=float, default=0.0,
                        help='momentum for SGD')
    group.add_argument('--weight-decay', type=float, default=0,
                        help='weight decay for MLP optimization')
    group.add_argument('--freeze-lm', action='store_true',
                        help='keep lanuage model froze -- don\'t backprop to Transformer/RNN')
    group.add_argument('--aux-lm-loss', action='store_true',
                        help='whether to use language modeling objective as aux loss')
    group.add_argument('--aux-lm-loss-weight', type=float, default=1.0,
                        help='LM model weight -- NOTE: default is 1.0 for back compatible. Way too high -- reasonable around 0.02')
    group.add_argument('--aux-head-variance-loss-weight', type=float, default=0,
                        help='Set above 0.0 to force heads to learn different final-layer embeddings. Reasonable value ~10.-100.')
    group.add_argument('--use-class-multihead-average', action='store_true',
                        help='Use average output for multihead per class -- not necessary to use with --class-single-threshold [just average the thresholds]')
    group.add_argument('--thresh-test-preds', type=str, default=None,
                        help='path to thresholds for test outputs')
    group.add_argument('--report-metric', type=str, default='f1', choices=['jacc', 'acc', 'f1', 'mcc', 'precision', 'recall', 'var', 'all'],
                        help='what metric to report performance (save best model)')
    group.add_argument('--all-metrics', action='store_true',
                        help='Overloads report metrics and reports all metrics at once')
    group.add_argument('--threshold-metric', type=str, default='f1', choices=['jacc', 'acc', 'f1', 'mcc', 'precision', 'recall', 'var', 'all'],
                        help='which metric to use when choosing ideal thresholds?')
    group.add_argument('--micro', action='store_true',
                        help='whether to use micro averaging for metrics')
    group.add_argument('--global-tweaks', type=int, default=0,
                        help='HACK: Pass int (1000 for example) to tweak individual thresholds toward best global average [good for SemEval]. Will increase threshold on rare, hard to measure, categories.')
    group.add_argument('--save-finetune', action='store_true',
                        help='save finetuned models at every epoch of finetuning')
    group.add_argument('--model-version-name', type=str, default='classifier',
                        help='space to version model name -- for saving')
    group.add_argument('--automatic-thresholding', action='store_true',
                        help='automatically select classification thresholds based on validation performance. \
                            (test results are also reported using the thresholds)')
    group.add_argument('--no-test-eval', action='store_true',
                        help='Do not report test metrics, write test and val results to disk instead.')
    group.add_argument('--decay-style', type=str, default=None, choices=['constant', 'linear', 'cosine', 'exponential'],
                        help='Learning rate decay, one of constant(None), linear, cosine, or exponential')
    group.add_argument('--warmup-epochs', type=float, default=0.,
                        help='number of epochs to warm up learning rate over.')
    group.add_argument('--decay-epochs', type=float, default=-1, 
                        help='number of epochs to decay for. If -1 decays for all of training')
    group.add_argument('--load-finetuned', action='store_true',
                        help='load not just the language model but a previously finetuned full classifier checkpoint')

    return data_config, data_group, group, parser

# PROJECT: NVIDIA_sentiment-discovery FILE: experiments/run_clf_multihead.py
import argparse
import itertools
import sys
import subprocess
import os

if __name__ == '__main__':
    parser = argparse.ArgumentParser("Let's run some multihead experiments!")
    parser.add_argument('--gpu', type=int, default=0,
                        help='which gpu to run on')
    parser.add_argument('--train', type=str, default='./data/semeval/val.csv',
                        help='using nvidia training dataset')
    parser.add_argument('--val', type=str, default='./data/semeval/val.csv',
                        help='using nvidia val dataset')
    parser.add_argument('--test', type=str, default='./data/semeval/val.csv')
    parser.add_argument('--process-fn', type=str, default='process_str', choices=['process_str', 'process_tweet'],
                        help='what preprocessing function to use to process text. One of [process_str, process_tweet].')
    parser.add_argument('--text-key', default='text', type=str)

    args = parser.parse_args()

    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(args.gpu)

    plutchik_cols = ' '.join(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust'])

    base_command = "python3 finetune_classifier.py --data {train} --valid {val} --test {test} --warmup-epochs 0.5 --epochs 20 " \
        + "--text-key {text_key} --optim Adam --all-metrics --automatic-thresholding --batch-size 16 --save-finetune --process-fn {proc} " \
        + "--aux-lm-loss --aux-lm-loss-weight 0.02 --classifier-hidden-layers 4096 2048 1024 8 --classifier-dropout 0.3 --non-binary-cols " + plutchik_cols + ' '

    transformer_options = "--lr 1e-5 --tokenizer-type SentencePieceTokenizer --tokenizer-path ama_32k_tokenizer.model --vocab-size 32000 --decoder-layers 12 "\
        +" --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-learned-pos --model transformer --load transformer.pt --use-final-embed --max-seq-len 150 " \
        +"  --dropout 0.2 --attention-dropout 0.2 --relu-dropout 0.2 --model-version-name transformer_multihead" 

    mlstm_options = " --lr 1e-5 --load mlstm.pt --model-version-name mlstm_multihead"

    formatted_base_command = base_command.format(train=args.train, val=args.val, test=args.test, text_key=args.text_key, proc=args.process_fn)
    transformer_command = formatted_base_command + transformer_options
    print('*' * 100)
    print("EXPERIMENT: Transformer, {}, {}, {}".format('multihead', args.train, args.val))
    print('*' * 100)
    sys.stdout.flush()
    sys.stderr.flush()
    subprocess.call(transformer_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)

    mlstm_command = formatted_base_command + mlstm_options
    print('*' * 100)
    print("EXPERIMENT: mLSTM, {}, {}, {}".format('multihead', args.train, args.val))
    print('*' * 100)
    sys.stdout.flush()
    sys.stderr.flush()
    subprocess.call(mlstm_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)
# PROJECT: NVIDIA_sentiment-discovery FILE: experiments/run_clf_sst.py
import argparse
import itertools
import sys
import subprocess
import os

if __name__ == '__main__':
    parser = argparse.ArgumentParser("Let's run some sst experiments!")
    parser.add_argument('--gpu', type=int, default=0,
                        help='which gpu to run on')

    args = parser.parse_args()

    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(args.gpu)

    base_command = "python3 finetune_classifier.py --warmup-epochs 0.5 --epochs 20 " \
        + "--optim Adam --all-metrics --threshold-metric f1 --automatic-thresholding --batch-size 16 " \
        + "--aux-lm-loss --aux-lm-loss-weight 0.02 --save-finetune " 

    transformer_options = "--lr 1e-5 --tokenizer-type SentencePieceTokenizer --tokenizer-path ama_32k_tokenizer.model --vocab-size 32000 --decoder-layers 12 "\
        +" --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-learned-pos --model transformer --load transformer.pt --use-final-embed --max-seq-len 150 " \
        +"  --dropout 0.2 --attention-dropout 0.2 --relu-dropout 0.2 --model-version-name transformer_sst_binary" 

    mlstm_options = " --lr 1e-5 --load mlstm.pt --model-version-name mlstm_sst_binary"

    formatted_base_command = base_command
    transformer_command = formatted_base_command + transformer_options
    print('*' * 100)
    print("EXPERIMENT: Transformer, {}, ".format('sst',))
    print('*' * 100)
    sys.stdout.flush()
    sys.stderr.flush()
    subprocess.call(transformer_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)

    mlstm_command = formatted_base_command + mlstm_options
    print('*' * 100)
    print("EXPERIMENT: mLSTM, {}, ".format('sst', ))
    print('*' * 100)
    sys.stdout.flush()
    sys.stderr.flush()
    subprocess.call(mlstm_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)
# PROJECT: NVIDIA_sentiment-discovery FILE: experiments/run_clf_single_head.py
import argparse
import itertools
import sys
import subprocess
import os

if __name__ == '__main__':
    parser = argparse.ArgumentParser("Let's run some singlehead experiments!")
    parser.add_argument('--gpu', type=int, default=0,
                        help='which gpu to run on')
    parser.add_argument('--train', type=str, default='./data/semeval/val.csv',
                        help='using nvidia training dataset')
    parser.add_argument('--val', type=str, default='./data/semeval/val.csv',
                        help='using nvidia val dataset')
    parser.add_argument('--test', type=str, default='./data/semeval/val.csv')
    parser.add_argument('--process-fn', type=str, default='process_str', choices=['process_str', 'process_tweet'],
                        help='what preprocessing function to use to process text. One of [process_str, process_tweet].')
    parser.add_argument('--text-key', default='text', type=str)

    args = parser.parse_args()

    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(args.gpu)

    plutchik_cols = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']

    base_command = "python3 finetune_classifier.py --data {train} --valid {val} --test {test} --warmup-epochs 0.5 --epochs 20 " \
        + "--text-key {text_key} --optim Adam --label-key {label_key} --all-metrics --automatic-thresholding --batch-size 16 --process-fn {proc} " \
        + "--aux-lm-loss --aux-lm-loss-weight 0.02 --classifier-hidden-layers 1 --classifier-dropout 0.3 "

    transformer_options = "--lr 1e-5 --tokenizer-type SentencePieceTokenizer --tokenizer-path ama_32k_tokenizer.model --vocab-size 32000 --decoder-layers 12 "\
        +" --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-learned-pos --model transformer --load transformer.pt --use-final-embed --max-seq-len 150 "\
        +"  --dropout 0.2 --attention-dropout 0.2 --relu-dropout 0.2" 

    mlstm_options = " --lr 1e-5 --load mlstm.pt"

    for label_key in plutchik_cols:
        formatted_base_command = base_command.format(label_key=label_key, train=args.train, val=args.val, test=args.test, text_key=args.text_key, proc=args.process_fn)
        transformer_command = formatted_base_command + transformer_options
        print('*' * 100)
        print("EXPERIMENT: Transformer, {}, {}, {}".format(label_key, args.train, args.val))
        print('*' * 100)
        sys.stdout.flush()
        sys.stderr.flush()
        subprocess.call(transformer_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)

        mlstm_command = formatted_base_command + mlstm_options
        print('*' * 100)
        print("EXPERIMENT: mLSTM, {}, {}, {}".format(label_key, args.train, args.val))
        print('*' * 100)
        sys.stdout.flush()
        sys.stderr.flush()
        subprocess.call(mlstm_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)

# PROJECT: NVIDIA_sentiment-discovery FILE: experiments/run_clf_binary.py
import argparse
import itertools
import sys
import subprocess
import os

if __name__ == '__main__':
    parser = argparse.ArgumentParser("Let's run some binary sentiment experiments!")
    parser.add_argument('--gpu', type=int, default=0,
                        help='which gpu to run on')
    parser.add_argument('--train', type=str, default='./data/semeval/train.csv',
                        help='using nvidia training dataset')
    parser.add_argument('--val', type=str, default='./data/semeval/val.csv',
                        help='using nvidia val dataset')
    parser.add_argument('--test', type=str, default='./data/semeval/val.csv')
    parser.add_argument('--process-fn', type=str, default='process_str', choices=['process_str', 'process_tweet'],
                        help='what preprocessing function to use to process text. One of [process_str, process_tweet].')
    parser.add_argument('--text-key', default='text', type=str)

    args = parser.parse_args()

    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(args.gpu)

    binary_cols = ' '.join(['positive', 'negative'])

    base_command = "python3 finetune_classifier.py --data {train} --valid {val} --test {test} --warmup-epochs 0.5 --epochs 20 " \
        + "--text-key {text_key} --optim Adam --all-metrics --threshold-metric f1 --automatic-thresholding --batch-size 16 --dual-thresh --process-fn {proc} " \
        + "--aux-lm-loss --aux-lm-loss-weight 0.02 --classifier-hidden-layers 4096 2048 1024 2 --classifier-dropout 0.3 --non-binary-cols " + binary_cols + ' '

    transformer_options = "--lr 1e-5 --tokenizer-type SentencePieceTokenizer --tokenizer-path ama_32k_tokenizer.model --vocab-size 32000 --decoder-layers 12 "\
        +" --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-learned-pos --model transformer --load transformer.pt --use-final-embed --max-seq-len 150 " \
        +"  --dropout 0.2 --attention-dropout 0.2 --relu-dropout 0.2" 

    mlstm_options = " --lr 1e-5 --load mlstm.pt"

    formatted_base_command = base_command.format(train=args.train, val=args.val, test=args.test, text_key=args.text_key, proc=args.process_fn)
    transformer_command = formatted_base_command + transformer_options
    print('*' * 100)
    print("EXPERIMENT: Transformer, {}, {}, {}".format('binary', args.train, args.val))
    print('*' * 100)
    sys.stdout.flush()
    sys.stderr.flush()
    subprocess.call(transformer_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)

    mlstm_command = formatted_base_command + mlstm_options
    print('*' * 100)
    print("EXPERIMENT: mLSTM, {}, {}, {}".format('binary', args.train, args.val))
    print('*' * 100)
    sys.stdout.flush()
    sys.stderr.flush()
    subprocess.call(mlstm_command.split(), stdout=sys.stdout, stderr=sys.stderr, env=env)
# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/lazy_loader.py
import os
import mmap
import pickle as pkl
import time
from itertools import accumulate
from threading import Lock

import torch

def get_lazy_path(path):
    """
    Gets path where lazy files are stored.
    """
    return os.path.splitext(path)[0]+'.lazy'

def exists_lazy(path, data_type='data'):
    """
    Check if we've already made a lazy version of this file for the `data_type` field.
    """
    if not os.path.exists(get_lazy_path(path)):
        return False
    contents = os.listdir(get_lazy_path(path))
    if data_type not in contents:
        return False
    if data_type+'.len.pkl' not in contents:
        return False
    return True

def make_lazy(path, strs, data_type='data'):
    """
    Make lazy version of `data_type` field of the file.
    """
    lazypath = get_lazy_path(path)
    if not os.path.exists(lazypath):
        os.makedirs(lazypath)
    datapath = os.path.join(lazypath, data_type)
    lenpath = os.path.join(lazypath, data_type+'.len.pkl')
    if not torch.distributed._initialized or torch.distributed.get_rank() == 0:
        with open(datapath, 'wb') as f:
            str_lens = []
            str_cnt = 0
            for s in strs:
                if isinstance(s, dict):
                    s = s['text']
                encoded = s.encode('utf-8')
                f.write(encoded)
                str_cnt = len(encoded)
                str_lens.append(str_cnt)
        pkl.dump(str_lens, open(lenpath, 'wb'))
    else:
        while not os.path.exists(lenpath):
            time.sleep(1)

def split_strings(strings, start, chr_lens):
    """
    Split strings based on string lengths and given start.
    """
    return [strings[i-start:j-start] for i, j in zip([start]+chr_lens[:-1], chr_lens)]

class ProcessorTokenizer:
    def __init__(self, tokenizer, process_fn=None):
        self.tokenizer = tokenizer
        self.process_fn = process_fn

    def __call__(self, string):
        return self.tokenizer(string, process_fn=self.process_fn)

class lazy_array_loader(object):
    """
    Arguments:
        path: path to directory where array entries are concatenated into one big string file
            and the .len file are located
        data_type (str): Some datsets have multiple fields that are stored in different paths.
            `data_type` specifies which of these fields to load in this class
        mem_map  (boolean): Specifies whether to memory map file `path`
        map_fn (callable): Fetched strings are passed through map_fn before being returned.
    """
    def __init__(self, path, data_type='data', mem_map=False, map_fn=None):
        lazypath = get_lazy_path(path)
        datapath = os.path.join(lazypath, data_type)
        #get file where array entries are concatenated into one big string
        self._file = open(datapath, 'rb')
        self.file = self._file
        #memory map file if necessary
        self.mem_map = mem_map
        if self.mem_map:
            self.file = mmap.mmap(self.file.fileno(), 0, prot=mmap.PROT_READ)
        lenpath = os.path.join(lazypath, data_type+'.len.pkl')
        self.lens = pkl.load(open(lenpath, 'rb'))
        self.ends = list(accumulate(self.lens))
        self.dumb_ends = list(self.ends)
        self.read_lock = Lock()
        self.map_fn = map_fn

    def SetTokenizer(self, tokenizer):
        self.map_fn = ProcessorTokenizer(tokenizer, self.map_fn)

    def __getitem__(self, index):
        """read file and splice strings based on string ending array `ends` """
        if not isinstance(index, slice):
            if index == 0:
                start = 0
            else:
                start = self.ends[index-1]
            end = self.ends[index]
            rtn = self.file_read(start, end)
            if self.map_fn is not None:
                return self.map_fn(rtn)
        else:
            chr_lens = self.ends[index]
            if index.start == 0 or index.start is None:
                start = 0
            else:
                start = self.ends[index.start-1]
            stop = chr_lens[-1]
            strings = self.file_read(start, stop)
            rtn = split_strings(strings, start, chr_lens)
            if self.map_fn is not None:
                return self.map_fn([s for s in rtn])
        return rtn

    def __len__(self):
        return len(self.ends)

    def file_read(self, start=0, end=None):
        """read specified portion of file"""
        #TODO: Solve race condition
        #Seek to start of file read
        self.read_lock.acquire()
        self.file.seek(start)
        ##### Getting context-switched here
        #read to end of file if no end point provided
        if end is None:
            rtn = self.file.read()
        #else read amount needed to reach end point
        else:
            rtn = self.file.read(end-start)
        self.read_lock.release()
        #TODO: @raulp figure out mem map byte string bug
        #if mem map'd need to decode byte string to string
        # rtn = rtn.decode('utf-8')
        rtn = str(rtn)
        if self.mem_map:
            rtn = rtn.decode('unicode_escape')
        return rtn


# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/preprocess.py
import os
import re
import html
import unicodedata

import unidecode
import torch

try:
    import emoji
except:
    print(Warning("emoji import unavailable"))


HTML_CLEANER_REGEX = re.compile('<.*?>')

def clean_html(text):
    """remove html div tags"""
    text = str(text)
    return re.sub(HTML_CLEANER_REGEX, ' ', text)

def binarize_labels(labels, hard=True):
    """If hard, binarizes labels to values of 0 & 1. If soft thresholds labels to [0,1] range."""
    labels = np.array(labels)
    min_label = min(labels)
    label_range = max(labels)-min_label
    if label_range == 0:
        return labels
    labels = (labels-min_label)/label_range
    if hard:
        labels = (labels > .5).astype(int)
    return labels

def remove_accents(input_str):
    nfkd_form = unicodedata.normalize('NFKD', input_str)
    return u"".join([c for c in nfkd_form if not unicodedata.combining(c)])

def process_str(text, front_pad='\n ', end_pad=' ', maxlen=None, clean_markup=True,
                clean_unicode=True, encode='utf-8', limit_repeats=3):
    """
    Processes utf-8 encoded text according to the criterion specified in seciton 4 of https://arxiv.org/pdf/1704.01444.pdf (Radford et al).
    We use unidecode to clean unicode text into ascii readable text
    """
    if clean_markup:
        text = clean_html(text)

    if clean_unicode:
        text = unidecode.unidecode(text)

    text = html.unescape(text)
    text = text.split()
    if maxlen is not None:
        len2use = maxlen-len(front_pad)-len(end_pad)
        text = text[:len2use]

    if limit_repeats > 0:
        remove_repeats(text, limit_repeats, join=False)

    text = front_pad+(" ".join(text))+end_pad

    if encode is not None:
        text = text.encode(encoding=encode)
        text = ''.join(chr(c) for c in text)

    return text

def remove_repeats(string, n, join=True):
    count = 0
    output = []
    last = ''
    for c in string:
        if c == last:
            count = count + 1
        else:
            count = 0
            last = c
        if count < n:
            output.append(c)
    if join:
        return "".join(output)
    return output

def tokenize_str_batch(strings, rtn_maxlen=True, process=True, maxlen=None, ids=False, rtn_processed=True):
    """
    Tokenizes a list of strings into a ByteTensor
    Args:
        strings: List of utf-8 encoded strings to tokenize into ByteTensor form
        rtn_maxlen: Boolean with functionality specified in Returns.lens
    Returns:
        batch_tensor: ByteTensor of shape `[len(strings),maxlen_of_strings]`
        lens: Length of each string in strings after being preprocessed with `preprocess` (useful for
            dynamic length rnns). If `rtn_maxlen` is `True` then max(lens) is returned instead.
    """
    if process:
        processed_strings = [process_str(x, maxlen=maxlen) for x in strings]
    else:
        processed_strings = [x.encode('utf-8', 'replace') for x in strings]

    tensor_type = torch.ByteTensor

    lens, batch_tensor = batch_tokens(processed_strings, tensor_type)
    maxlen = max(lens)
    rounded_maxlen = max(lens)

    rtn = []
    if not rtn_maxlen and rtn_maxlen is not None:
        rtn = [batch_tensor, lens]
    elif rtn_maxlen is None:
        rtn = [batch_tensor]
    else:
        rtn = [batch_tensor, rounded_maxlen]
    if rtn_processed:
        rtn += [processed_strings]
    return tuple(rtn)

def batch_tokens(token_lists, tensor_type=torch.LongTensor, fill_value=0):
    lens = list(map(len, token_lists))
    batch_tensor = fill_value * torch.ones(len(lens), max(lens)).type(tensor_type)
    for i, string in enumerate(token_lists):
        _tokenize_str(string, tensor_type, batch_tensor[i])
    return batch_tensor, lens

def _tokenize_str(data, tensor_type, char_tensor=None):
    """
    Parses a utf-8 encoded string and assigns to ByteTensor char_tensor.
    If no char_tensor is provide one is created.
    Typically used internally by `tokenize_str_batch`.
    """
    if char_tensor is None:
        if isinstance(data, str):
            # data could either be a string or a list of ids.
            data = data.encode()
        char_tensor = tensor_type(len(data))
    for i, char in enumerate(data):
        char_tensor[i] = char

EMOJI_DESCRIPTION_SCRUB = re.compile(r':(\S+?):')
HASHTAG_BEFORE = re.compile(r'#(\S+)')
BAD_HASHTAG_LOGIC = re.compile(r'(\S+)!!')
FIND_MENTIONS = re.compile(r'@(\S+)')
LEADING_NAMES = re.compile(r'^\s*((?:@\S+\s*)+)')
TAIL_NAMES = re.compile(r'\s*((?:@\S+\s*)+)$')

def process_tweet(s, save_text_formatting=True, keep_emoji=False, keep_usernames=False):
    # NOTE: will sometimes need to use Windows encoding here, depending on how CSV is generated.
    # All results saved in UTF-8
    # TODO: Try to get input data in UTF-8 and don't let it touch windows (Excel). That loses emoji, among other things

    # Clean up the text before tokenizing.
    # Why is this necessary?
    # Unsupervised training (and tokenization) is usually on clean, unformatted text.
    # Supervised training/classification may be on tweets -- with non-ASCII, hashtags, emoji, URLs.
    # Not obvious what to do. Two options:
    # A. Rewrite formatting to something in ASCII, then finetune.
    # B. Remove all formatting, keep only the text.
    if save_text_formatting:
        s = re.sub(r'https\S+', r'xxxx', str(s))
    else:
        s = re.sub(r'https\S+', r' ', str(s))
        s = re.sub(r'x{3,5}', r' ', str(s))
    
    # Try to rewrite all non-ASCII if known printable equivalent
    s = re.sub(r'\\n', ' ', s)
    s = re.sub(r'\s', ' ', s)
    s = re.sub(r'<br>', ' ', s)
    s = re.sub(r'&amp;', '&', s)
    s = re.sub(r'&#039;', "'", s)
    s = re.sub(r'&gt;', '>', s)
    s = re.sub(r'&lt;', '<', s)
    s = re.sub(r'\'', "'", s)

    # Rewrite emoji as words? Need to import a function for that.
    # If no formatting, just get the raw words -- else save formatting so model can "learn" emoji
    # TODO: Debug to show differences?
    if save_text_formatting:
        s = emoji.demojize(s)
    elif keep_emoji:
        s = emoji.demojize(s)
        # Transliterating directly is ineffective w/o emoji training. Try to shorten & fix
        s = s.replace('face_with', '')
        s = s.replace('face_', '')
        s = s.replace('_face', '')
        # remove emjoi formatting (: and _)
        # TODO: A/B test -- better to put emoji in parens, or just print to screen?
        #s = re.sub(EMOJI_DESCRIPTION_SCRUB, r' (\1) ', s)
        s = re.sub(EMOJI_DESCRIPTION_SCRUB, r' \1 ', s)
        # TODO -- better to replace '_' within the emoji only...
        s = s.replace('(_', '(')
        s = s.replace('_', ' ')

    # Remove all non-printable and non-ASCII characters, including unparsed emoji
    s = re.sub(r"\\x[0-9a-z]{2,3,4}", "", s)
    # NOTE: We can't use "remove accents" as long as foreign text and emoji gets parsed as characters. Better to delete it.
    # Replace accents with non-accented English letter, if possible.
    # WARNING: Will try to parse corrupted text... (as aAAAa_A)
    s = remove_accents(s)
    # Rewrite or remove hashtag symbols -- important text, but not included in ASCII unsupervised set
    if save_text_formatting:
        s = re.sub(HASHTAG_BEFORE, r'\1!!', s)
    else:
        s = re.sub(HASHTAG_BEFORE, r'\1', s)
        # bad logic in case ^^ done already
        s = re.sub(BAD_HASHTAG_LOGIC, r'\1', s)
    # Keep user names -- or delete them if not saving formatting.
    # NOTE: This is not an obvious choice -- we could also treat mentions vs replies differently. Or we could sub xxx for user name
    # The question is, does name in the @mention matter for category prediction? For emotion, it should not, most likely.
    if save_text_formatting:
        # TODO -- should we keep but anonymize mentions? Same as we rewrite URLs.
        pass
    else:
        # If removing formatting, either remove all mentions, or just the @ sign.
        if keep_usernames:
            # quick cleanup extra spaces
            s = ' '.join(s.split())

            # If keep usernames, *still* remove leading and trailing names in @ mentions (or tail mentions)
            # Why? These are not part of the text -- and should not change sentiment
            s = re.sub(LEADING_NAMES, r' ', s)
            s = re.sub(TAIL_NAMES, r' ', s)

            # Keep remaining mentions, as in "what I like about @nvidia drivers"
            s = re.sub(FIND_MENTIONS, r'\1', s)
        else:
            s = re.sub(FIND_MENTIONS, r' ', s)
    #s = re.sub(re.compile(r'@(\S+)'), r'@', s)
    # Just in case -- remove any non-ASCII and unprintable characters, apart from whitespace
    s = "".join(x for x in s if (x.isspace() or (31 < ord(x) < 127)))
    # Final cleanup -- remove extra spaces created by rewrite.
    s = ' '.join(s.split())
    return s

# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/loaders.py
import collections
import sys
if sys.version_info[0] == 2:
    import Queue as queue
    string_classes = basestring
else:
    import queue
    string_classes = (str, bytes)
import threading
import traceback
import math
import time

import torch
from torch.utils import data
import torch.multiprocessing as multiprocessing

import numpy as np

from .preprocess import tokenize_str_batch, batch_tokens
from .samplers import DistributedBatchSampler, BatchSampler, TransposedSampler, RandomShardSampler, DistributedBatchShardSampler, BatchShardSampler
from .tokenization import Tokenization

_use_shared_memory = False
"""Whether to use shared memory in default_collate"""

numpy_type_map = {
    'float64': torch.DoubleTensor,
    'float32': torch.FloatTensor,
    'float16': torch.HalfTensor,
    'int64': torch.LongTensor,
    'int32': torch.IntTensor,
    'int16': torch.ShortTensor,
    'int8': torch.CharTensor,
    'uint8': torch.ByteTensor,
}

samples = []

def default_collate(batch, maxlen=None, process=False):
    """
    normal default collate except for string classes we use our own tokenize_str_batch
        function to batch strings
    """
    "Puts each data field into a tensor with outer dimension batch size"
    if torch.is_tensor(batch[0]):
        out = None
        if _use_shared_memory:
            numel = sum([x.numel() for x in batch])
            storage = batch[0].storage()._new_shared(numel)
            out = batch[0].new(storage)
        return torch.stack(batch, 0, out=out)
    elif type(batch[0]).__module__ == 'numpy':
        elem = batch[0]
        if type(elem).__name__ == 'ndarray':
            return torch.stack([torch.from_numpy(b) for b in batch], 0)
        if elem.shape == ():
            py_type = float if elem.dtype.name.startswith('float') else int
            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))
    elif isinstance(batch[0], Tokenization):
        pad = batch[0].pad
        tokenization, text, original_text = zip(*([(tokenization.tokenization, tokenization.text, tokenization.original_text) for tokenization in batch]))
        return [batch_tokens(tokenization, fill_value=pad)[0], text, original_text]
    elif isinstance(batch[0], int):
        return torch.LongTensor(batch)
    elif isinstance(batch[0], float):
        return torch.DoubleTensor(batch)
    elif isinstance(batch[0], string_classes):
        return tokenize_str_batch(batch, rtn_maxlen=None, process=process, maxlen=maxlen)
    elif isinstance(batch[0], collections.Mapping):
        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}
    elif isinstance(batch[0], collections.Sequence):
        transposed = zip(*batch)
        return [default_collate(samples) for samples in transposed]

    raise TypeError(("batch must contain tensors, numbers, dicts or lists; found {}"
                        .format(type(batch[0]))))

def pin_memory_batch(batch):
    if isinstance(batch, torch.Tensor):
        return batch.pin_memory()
    elif isinstance(batch, string_classes):
        return batch
    elif isinstance(batch, collections.Mapping):
        return {k: pin_memory_batch(sample) for k, sample in batch.items()}
    elif isinstance(batch, collections.Sequence):
        return [pin_memory_batch(sample) for sample in batch]
    else:
        return batch

class DataLoader(data.DataLoader):
    """normal data loader except with options for distributed data batch sampling + wrap around"""
    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,
                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,
                 transpose=False, world_size=2, rank=-1, distributed=False, wrap_last=False,
                 timeout=0, worker_init_fn=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.timeout = timeout
        self.worker_init_fn = worker_init_fn
        if timeout < 0:
            raise ValueError('timeout option should be non-negative')

        if batch_sampler is not None:
            if batch_size > 1 or shuffle or sampler is not None or drop_last:
                raise ValueError('batch_sampler is mutually exclusive with \
                                    batch_size, shuffle, sampler, and drop_last')

        if sampler is not None and shuffle:
            raise ValueError('sampler is mutually exclusive with shuffle')

        if self.num_workers < 0:
            raise ValueError('num_workers cannot be negative; '
                             'use num_workers=0 to disable multiprocessing.')

        if batch_sampler is None:
            if sampler is None:
                if shuffle:
                    sampler = data.sampler.RandomSampler(dataset)
                else:
                    if transpose:
                        sampler = TransposedSampler(dataset, batch_size)
                    else:
                        sampler = data.sampler.SequentialSampler(dataset)
            if distributed:
                batch_sampler = DistributedBatchSampler(sampler, batch_size, drop_last,
                                                        world_size=world_size, rank=rank, wrap_last=wrap_last)
            else:
                batch_sampler = BatchSampler(sampler, batch_size, drop_last, wrap_last=wrap_last)

        self.sampler = sampler
        self.batch_sampler = batch_sampler
        self.last_iter = None

class ShardLoader(object):
    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,
                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,
                 transpose=False, world_size=2, rank=-1, distributed=False, wrap_last=False,
                 timeout=0, worker_init_fn=None, seq_len=-1, persist_state=0, samples_per_shard=1):
        self.dataset = dataset
        self.batch_size = batch_size
        self.seq_len = seq_len
        self.persist_state = persist_state
        self.samples_per_shard = samples_per_shard
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.timeout = timeout
        self.worker_init_fn = worker_init_fn
        if timeout < 0:
            raise ValueError('timeout option should be non-negative')

        if batch_sampler is not None:
            if batch_size > 1 or shuffle or sampler is not None or drop_last:
                raise ValueError('batch_sampler is mutually exclusive with \
                                    batch_size, shuffle, sampler, and drop_last')

        if sampler is not None and shuffle:
            raise ValueError('sampler is mutually exclusive with shuffle')

        if self.num_workers < 0:
            raise ValueError('num_workers cannot be negative; '
                             'use num_workers=0 to disable multiprocessing.')

        self.distributed=distributed
        self.world_size=world_size
        self.rank=rank
        if self.distributed:
            self.batch_size = math.ceil(self.batch_size/self.world_size)

        if batch_sampler is None:
            if sampler is None:
                sampler = RandomShardSampler(self.dataset, self.samples_per_shard, self.seq_len, self.persist_state)
            if self.distributed:
                batch_sampler = DistributedBatchShardSampler(sampler, self.batch_size, self.drop_last, world_size=self.world_size, rank=self.rank)
            else:
                batch_sampler = BatchShardSampler(sampler, self.batch_size, self.drop_last)
        else:
            sampler = batch_sampler.sampler 

        self.sampler = sampler
        self.batch_sampler = batch_sampler
        self.last_iter = None


    def set_seq_len(self, seq_len):
        self.seq_len = seq_len
        self.batch_sampler.set_seq_len(seq_len)

    def set_samples_per_shard(self, samples_per_shard):
        self.samples_per_shard = samples_per_shard
        self.batch_sampler.set_samples_per_shard(samples_per_shard)

    def set_persist_state(self, persist_state):
        self.persist_state = persist_state
        self.batch_sampler.set_persist_state(persist_state)

    def __len__(self):
        return len(self.batch_sampler)/self.batch_size

    def __iter__(self):
        return _ShardLoaderIter(self)

class _ShardLoaderIter(object):
    def __init__(self, shardloader):
        self.shardloader = shardloader
        self.num_workers = self.shardloader.num_workers
        self.batch_sampler = self.shardloader.batch_sampler
        self.collate_fn = self.shardloader.collate_fn
        self.pin_memory = self.shardloader.pin_memory
        self.batch_size = self.batch_sampler.batch_size
        self.timeout = self.shardloader.timeout
        if self.num_workers == 0:
            self.queue_manager = (q for q in self.batch_sampler.manage_queues())
        else:
            self.queue_manager = _ShardLoaderManager(self.batch_sampler, self.num_workers, self.collate_fn, self.pin_memory, self.timeout)

    def __iter__(self):
        return self

    def __next__(self):
        if self.num_workers == 0:
            return self.collate_fn(next(self.queue_manager))
        else:
            return next(self.queue_manager)

MP_STATUS_CHECK_INTERVAL = 5.0

class _ShardLoaderManager(object):
    def __init__(self, batch_sampler, num_workers, collate_fn, pin_memory=False, timeout=False):
        self.batch_sampler = batch_sampler
        self.batch_size = self.batch_sampler.batch_size
        self.num_workers = num_workers
        self.queue_size = num_workers*2
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.timeout = timeout

        self.data_queues = []
        self.workers = []

        indices_per_worker = self.batch_size // self.num_workers
        all_indices = list(range(self.batch_size))
        for i in range(num_workers):
            data_queue = multiprocessing.Queue(self.queue_size)
            self.data_queues.append(data_queue)
            batch_indices = all_indices[indices_per_worker*i:indices_per_worker*(i+1)]
            w = multiprocessing.Process(target=self.batch_sampler.manage_queues_multiproc,
                                        args=(batch_indices, data_queue))
            w.daemon = True
            w.start()
            self.workers.append(w)

        self.output_queue = queue.Queue(self.queue_size)
        cur_device = -1
        if torch.cuda.is_available():
            cur_device = torch.cuda.current_device()
        self.output_thread = threading.Thread(target=_shardloader_pin_memory_loop,
                                              args=(self.output_queue, self.data_queues,
                                                    self.collate_fn, self.pin_memory,
                                                    cur_device))
        self.output_thread.daemon = True
        self.output_thread.start()

    def __iter__(self):
        return self

    def _get_batch(self):
        # In the non-timeout case, worker exit is covered by SIGCHLD handler.
        # But if `pin_memory=True`, we still need account for the possibility
        # that `pin_memory_thread` dies.
        if self.timeout > 0:
            try:
                return self.output_queue.get(timeout=self.timeout)
            except queue.Empty:
                raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))
        elif self.pin_memory:
            while self.output_thread.is_alive():
                try:
                    return self.output_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
                except queue.Empty:
                    continue
            else:
                # while condition is false, i.e., pin_memory_thread died.
                raise RuntimeError('Pin memory thread exited unexpectedly')
            # In this case, `self.data_queue` is a `queue.Queue`,. But we don't
            # need to call `.task_done()` because we don't use `.join()`.
        else:
            return self.output_queue.get(block=True)

    def __next__(self):
        return self._get_batch()


def _shardloader_pin_memory_loop(output_queue, data_queues, collate_fn, pin_memory=False, device_id=-1, timeout=0):
    queue_results = [list() for _ in data_queues]
    output_queue_len = output_queue.maxsize
    if device_id >= 0:
        torch.cuda.set_device(device_id)
    while True:
        for i, data_queue in enumerate(data_queues):
            try:
                res = data_queue.get_nowait()
                queue_results[i].append(res)
            except queue.Empty:
                continue
        if sum(len(q)>=1 for q in queue_results) >= len(data_queues):
            batch = []
            for q in queue_results:
                batch.extend(q.pop(0))
            batch = collate_fn(batch)
            if pin_memory:
                batch = pin_memory_batch(batch)
            output_queue.put(batch, block=True)

# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/datasets.py
import os
import time
from operator import itemgetter
from bisect import bisect_left, bisect_right
import json
from itertools import accumulate
import csv
import collections

import torch
from torch.utils import data
import pandas as pd
import numpy as np

from .preprocess import process_str, binarize_labels
from .lazy_loader import lazy_array_loader, exists_lazy, make_lazy
from .cache import array_cache
from .tokenization import Tokenization

PERSIST_ALL = -1
PERSIST_SHARD = 1
RESET_STATE = 0

def get_processed_path(path, text_key='text', label_key='label'):
    filepath, ext = os.path.splitext(path)
    return filepath+'.%s.%s'%(text_key, label_key)+ext

def get_load_path_and_should_process(path, text_key='text', label_key='label'):
    processed_path = get_processed_path(path, text_key, label_key)
    exists = os.path.exists(processed_path)
    if not exists:
        return path, True
    return processed_path, False

def save_preprocessed(ds, text_key='text', label_key='label'):
    processed_path = get_processed_path(ds.path, text_key, label_key)
    if not torch.distributed._initialized or torch.distributed.get_rank() == 0:
        ds.write(path=processed_path)
    return processed_path

class ConcatDataset(data.Dataset):
    """
    Dataset to concatenate multiple datasets.
    Purpose: useful to assemble different existing datasets, possibly
    large-scale datasets as the concatenation operation is done in an
    on-the-fly manner.
    Arguments:
        datasets (sequence): List of datasets to be concatenated.
    """

    @staticmethod
    def cumsum(sequence):
        r, s = [], 0
        for e in sequence:
            l = len(e)
            r.append(l + s)
            s += l
        return r

    def __init__(self, datasets, **kwargs):
        super(ConcatDataset, self).__init__()
        assert len(datasets) > 0, 'datasets should not be an empty iterable'
        self.datasets = list(datasets)
        self.cumulative_sizes = self.cumsum(self.datasets)
        self._X = None
        self._Y = None

    def SetTokenizer(self, tokenizer):
        for ds in self.datasets:
            ds.SetTokenizer(tokenizer)

    def __len__(self):
        return self.cumulative_sizes[-1]

    def __getitem__(self, idx):
        dataset_idx = bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        return self.datasets[dataset_idx][sample_idx]

    @property
    def X(self):
        if self._X is None:
            self._X = []
            for data in self.datasets:
                self._X.extend(data.X)
        return self._X

    @property
    def Y(self):
        if self._Y is None:
            self._Y = []
            for data in self.datasets:
                self._Y.extend(list(data.Y))
            self._Y = np.array(self._Y)
        return self._Y

    @property
    def cummulative_sizes(self):
        warnings.warn("cummulative_sizes attribute is renamed to "
                      "cumulative_sizes", DeprecationWarning, stacklevel=2)
        return self.cumulative_sizes

class SplitDataset(data.Dataset):
    """
    Dataset wrapper to access a subset of another dataset.
    Purpose: useful to index into existing datasets, possibly
    large-scale datasets as the subindexing operation is done in an
    on-the-fly manner.
    Arguments:
        ds (Dataset or array-like): List of datasets to be subindexed
        split_inds (1D array-like): List of indices part of subset
    """
    def __init__(self, ds, split_inds, **kwargs):
        self.split_inds = list(split_inds)
        self.wrapped_data = ds
        self.is_lazy = isinstance(ds, lazy_array_loader)
        if self.is_lazy:
            self.lens = itemgetter(*self.split_inds)(list(self.wrapped_data.lens))
        self._X = None
        self._Y = None

    def __len__(self):
        return len(self.split_inds)

    def __getitem__(self, index):
        return self.wrapped_data[self.split_inds[index]]

    @property
    def X(self):
        if self._X is None:
            self._X = itemgetter(*self.split_inds)(self.wrapped_data.X)
        return self._X

    @property
    def Y(self):
        if self._Y is None:
            self._Y = np.array(itemgetter(*self.split_inds)(self.wrapped_data.Y))
        return self._Y

    def __iter__(self):
        for idx in self.split_inds:
            yield self.wrapped_data[idx]

def split_ds(ds, split=[.8,.2,.0], shuffle=True):
    """
    Split a dataset into subsets given proportions of how
    much to allocate per split. If a split is 0% returns None for that split.
    Purpose: Useful for creating train/val/test splits
    Arguments:
        ds (Dataset or array-like): Data to be split.
        split (1D array-like): proportions to split `ds`. `sum(splits) != 0`
        shuffle (boolean): Randomly split dataset. Default: True
    """
    split_sum = sum(split)
    if split_sum == 0:
        raise Exception('Split cannot sum to 0.')
    split = np.array(split)
    split /= split_sum
    ds_len = len(ds)
    inds = np.arange(ds_len)
    if shuffle:
        np.random.shuffle(inds)
    start_idx = 0
    residual_idx = 0
    rtn_ds = [None]*len(split)
    for i, f in enumerate(split):
        if f != 0:
            proportion = ds_len*split[i]
            residual_idx += proportion % 1
            split_ = int(int(proportion) + residual_idx)
            split_inds = inds[start_idx:start_idx+max(split_, 1)]
            rtn_ds[i] = SplitDataset(ds, split_inds)
            start_idx += split_
            residual_idx %= 1
    return rtn_ds

class csv_dataset(data.Dataset):
    """
    Class for loading datasets from csv files.
    Purpose: Useful for loading data for unsupervised modeling or transfer tasks
    Arguments:
        path (str): Path to csv file with dataset.
        tokenizer (data_utils.Tokenizer): Tokenizer to use when processing text. Default: None
        preprocess_fn (callable): Callable that process a string into desired format.
        delim (str): delimiter for csv. Default: ','
        binarize_sent (bool): binarize label values to 0 or 1 if they\'re on a different scale. Default: False
        drop_unlabeled (bool): drop rows with unlabelled sentiment values. Always fills remaining empty
            columns with -1 (regardless if rows are dropped based on sentiment value) Default: False
        text_key (str): key to get text from csv. Default: 'sentence'
        label_key (str): key to get label from json dictionary. Default: 'label'
    Attributes:
        X (list): all strings from the csv file
        Y (np.ndarray): labels to train against
    """
    def __init__(self, path, tokenizer=None, preprocess_fn=None, delim=',',
                binarize_sent=False, drop_unlabeled=False, text_key='sentence', label_key='label',
                **kwargs):
        self.preprocess_fn = preprocess_fn
        self.tokenizer = self.SetTokenizer(tokenizer)
        self.path = path
        self.delim = delim
        self.text_key = text_key
        self.label_key = label_key
        self.drop_unlabeled = drop_unlabeled

        if '.tsv' in self.path:
            self.delim = '\t'


        self.X = []
        self.Y = []
        try:
            cols = [text_key]
            if isinstance(label_key, list):
                cols += label_key
            else:
                cols += [label_key]
            data = pd.read_csv(self.path, sep=self.delim, usecols=cols, encoding='latin-1')
        except:
            data = pd.read_csv(self.path, sep=self.delim, usecols=[text_key], encoding='latin-1')

        data = data.dropna(axis=0)

        self.X = data[text_key].values.tolist()
        try:
            self.Y = data[label_key].values
        except Exception as e:
            self.Y = np.ones(len(self.X))*-1

        if binarize_sent:
            self.Y = binarize_labels(self.Y, hard=binarize_sent)

    def SetTokenizer(self, tokenizer):
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.X)

    def __getitem__(self, index):
        """process string and return string,label,and stringlen"""
        x = self.X[index]
        if self.tokenizer is not None:
            x = self.tokenizer.EncodeAsIds(x, self.preprocess_fn)
        elif self.preprocess_fn is not None:
            x = self.preprocess_fn(x)
        y = self.Y[index]
        return {'text': x, 'length': len(x), 'label': y}

    def write(self, writer_gen=None, path=None, skip_header=False):
        """
        given a generator of metrics for each of the data points X_i,
            write the metrics, text, and labels to a csv file
        """
        if path is None:
            path = self.path+'.results'
        print('generating csv at ' + path)
        with open(path, 'w') as csvfile:
            c = csv.writer(csvfile, delimiter=self.delim)
            if writer_gen is not None:
                #if first item of generator is a header of what the metrics mean then write header to csv file
                if not skip_header:
                    header = (self.label_key,)+tuple(next(writer_gen))+(self.text_key,)
                    c.writerow(header)
                for i, row in enumerate(writer_gen):
                    row = (self.Y[i],)+tuple(row)+(self.X[i],)
                    c.writerow(row)
            else:
                c.writerow([self.label_key, self.text_key])
                for row in zip(self.Y, self.X):
                    c.writerow(row)

class json_dataset(data.Dataset):
    """
    Class for loading datasets from a json dump.
    Purpose: Useful for loading data for unsupervised modeling or transfer tasks
    Arguments:
        path (str): path to json file with dataset.
        tokenizer (data_utils.Tokenizer): Tokenizer to use when processing text. Default: None
        preprocess_fn (callable): callable function that process a string into desired format.
            Takes string, maxlen=None, encode=None as arguments. Default: process_str
        text_key (str): key to get text from json dictionary. Default: 'sentence'
        label_key (str): key to get label from json dictionary. Default: 'label'
    Attributes:
        all_strs (list): list of all strings from the dataset
        all_labels (list): list of all labels from the dataset (if they have it)
    """
    def __init__(self, path, tokenizer=None, preprocess_fn=process_str, binarize_sent=False,
                text_key='sentence', label_key='label', loose_json=False, **kwargs):
        self.preprocess_fn = preprocess_fn
        self.path = path
        self.tokenizer = self.SetTokenizer(tokenizer)
        self.X = []
        self.Y = []
        self.text_key = text_key
        self.label_key = label_key
        self.loose_json = loose_json

        for j in self.load_json_stream(self.path):
            s = j[text_key]
            self.X.append(s)
            self.Y.append(j[label_key])

        if binarize_sent:
            self.Y = binarize_labels(self.Y, hard=binarize_sent)

    def SetTokenizer(self, tokenizer):
        self.tokenizer = tokenizer

    def __getitem__(self, index):
        """gets the index'th string from the dataset"""
        x = self.X[index]
        if self.tokenizer is not None:
            x = self.tokenizer.EncodeAsIds(x, self.preprocess_fn)
        elif self.preprocess_fn is not None:
            x = self.preprocess_fn(x)
        y = self.Y[index]
        return {'text': x, 'length': len(x), 'label': y}

    def __len__(self):
        return len(self.X)

    def write(self, writer_gen=None, path=None, skip_header=False):
        """
        given a generator of metrics for each of the data points X_i,
            write the metrics, text, and labels to a json file
        """
        if path is None:
            path = self.path+'.results'

        jsons = []

        if writer_gen is not None:
            #if first item of generator is a header of what the metrics mean then write header to csv file
            def gen_helper():
                keys = {}
                keys[0] = self.label_key
                if not skip_header:
                    for idx, k in enumerate(tuple(next(writer_gen))):
                        keys[idx+1] = k
                for i, row in enumerate(writer_gen):
                    if i == 0 and skip_header:
                        for idx, _ in enumerate(row):
                            keys[idx+1] = 'metric_%d'%(idx,)
                    j = {}
                    for idx, v in enumerate((self.Y[i],)+tuple(row)):
                        k = keys[idx]
                        j[k] = v
                    yield j
        else:
            def gen_helper():
                for y in self.Y:
                    j = {}
                    j[self.label_key] = y
                    yield j

        def out_stream():
            for i, j in enumerate(gen_helper()):
                j[self.text_key] = self.X[i]
                yield j

        self.save_json_stream(path, out_stream())

    def save_json_stream(self, save_path, json_stream):
        if self.loose_json:
            with open(save_path, 'w') as f:
                for i, j in enumerate(json_stream):
                    write_string = ''
                    if i != 0:
                        write_string = '\n'
                    write_string += json.dumps(j)
                    f.write(write_string)
        else:
            jsons = [j for j in json_stream]
            json.dump(jsons, open(save_path, 'w'), separators=(',', ':'))

    def load_json_stream(self, load_path):
        if not self.loose_json:
            jsons = json.load(open(load_path, 'r'))
            generator = iter(jsons)
        else:
            def gen_helper():
                with open(load_path, 'r') as f:
                    for row in f:
                        yield json.loads(row)
            generator = gen_helper()

        for j in generator:
            if self.label_key not in j:
                j[self.label_key] = -1
            yield j

class data_shard(object):
    """
    Data Shard of multiple tokenizations.
    Purpose: Useful in L2R unsupervised learning. It's stateful and on consecutive
    calls to `get` it returns the next sequence of tokens following the last 
    sequence of tokens returned.
    Arguments:
        data (Tokenization or list): data comprising the data shard. Either a Tokenization or list of Tokenizations.
        seq_len (int): sequence length to sample from shard
        persist_state (int): one of -1,0,1 specifying whether to never reset state,
            reset after every sentence, or at end of every shard. Default: 0
    Attributes:
        all_seq (list): list of all tokenizations
        seq_ends (list): cummulative lengths of `all_strs` if they were all concat'd to gether.
            `itertools.accumulate([len(s) for s in all_strs])
        total_toks (int): `seq_ends[-1]`
        num_seq (int): `len(all_seq)`
    """
    def __init__(self, data, seq_len=-1, persist_state=0, **kwargs):
        self.seq_len = seq_len
        self.persist_state = persist_state

        if isinstance(data, Tokenization):
            self.num_seq = 1
            self.all_seq = [data]
            self.seq_ends = [len(data)]
        else:
            self.num_seq = len(data)
            self.all_seq = data
            self.seq_ends = [len(self.all_seq[0])]
            for i in range(1, self.num_seq):
                s = self.all_seq[i]
                self.seq_ends.append(len(s)+self.seq_ends[-1])

        self.pad = self.all_seq[-1].pad
        self.total_toks = self.seq_ends[-1]
        self.counter = 0
        self.seq_counter = 0
        self.intra_seq_counter = 0

    def set_seq_len(self, val):
        self.seq_len = val

    def _get(self, seq_len):
        """
        Get next sequence and reset mask of `seq_len` length.
        """
        rtn_mask = []
        rtn = []
        if seq_len <= 0:
            self.counter = self.total_toks
            rtn = []
            for seq in self.all_seq:
                s = seq[:]
                rtn.extend(s)
                rtn_mask.extend(self.get_string_mask(s))
                self.seq_counter += 1
        else:
            rtn = []
            #add one to the sequence length because we need [0:seq_len] as inputs and [1:seq_len+1] as targets
            seq_len += 1
            while self.seq_counter < self.num_seq and not len(rtn) >= seq_len:
                tokenization = self.all_seq[self.seq_counter]
                num_chars = seq_len - len(rtn)
                start = self.intra_seq_counter
                end = start + num_chars
                seq = list(tokenization[start:end])
                rtn.extend(seq)
                rtn_mask.extend(self.get_string_mask(seq))
                seq_complete = len(rtn) == seq_len
                self.intra_seq_counter += len(seq)
                if self.intra_seq_counter >= len(tokenization):
                    if seq_complete:
                        # if sampled seq_len+1 tokens ends on the last token of an example do not advance intra_seq_counter as the last token will be needed for input during next sample
                        self.intra_seq_counter -= 1
                    else:
                        self.seq_counter += 1
                        self.intra_seq_counter = 0
                else:
                    self.intra_seq_counter -= 1
        return rtn, rtn_mask

    def get_string_mask(self, s):
        """
        Get hidden state reset mask for string being currently sampled.
        """
        start_mask = 0
        if self.persist_state == PERSIST_SHARD:
            start_mask = (self.seq_counter == 0 and self.intra_seq_counter == 0)
        elif self.persist_state == RESET_STATE:
            start_mask = self.intra_seq_counter == 0
        return [start_mask] + [0] * (len(s)-1)


    def get(self, seq_len=None):
        """
        Get the next sequence from the data shard as well as state reset and padding/loss masks.
        Returns a sequence of seq_len+1 so that i`nputs, targets = sequence[:-1], sequence[1:]`
        """
        if seq_len is None:
            seq_len = self.seq_len
        rtn, rtn_mask = self._get(seq_len)
        rtn_len = len(rtn)
        # returned sequence should be seq_len+1 length since it needs to contain inputs and targets
        num_padding = seq_len - (rtn_len-1)
        if num_padding > 0:
            rtn.extend([self.pad] * num_padding)
            rtn_mask.extend([0] * num_padding)
        if seq_len > 0:
            self.counter += seq_len
            # mask all padding + the last valid target token to 0 since they won't be used in loss computation
            loss_mask = [1]*(rtn_len-1) + [0]*(num_padding+1)
        else:
            self.counter = self.total_toks
            loss_mask = [1]*rtn_len
        return np.array(rtn), np.array(rtn_mask), np.array(loss_mask)

    def is_last(self):
        return self.counter >= self.total_toks-self.seq_len -1
        
    def is_done(self):
        return self.counter >= self.total_toks-1

    def __iter__(self):
        self.counter = 0
        while self.counter < self.total_toks:
            yield self.get(self.seq_len)
# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/cache.py
class array_cache(object):
    """
    Arguments:
        cache_strs (list-like): List like object with __len__ and __getitem__
        cache_block_size (int): number of strings to cache in one cache block. Default: 64
        cache_size (int): number of caches blocks to store before removing (LRU). Default: 32
    Attributes:
        num_strs (int): len(cache_strs)
        cache (dict): holds cache blocks
        cache_blocks (list): list of keys for blocks stored in caches
    """
    def __init__(self, cache_strs, cache_block_size=64, cache_size=32):
        super(array_cache, self).__init__()
        self.cache_size = cache_size
        self.cache_block_size = cache_block_size
        self.cache_strs = cache_strs
        self.num_strs = len(self.cache_strs)
        self.cache = {}
        self.cache_blocks = []

    def __getitem__(self, index):
        #get index of cache block of size cache_block_size
        block_ind = index//self.cache_block_size
        if block_ind not in self.cache:
            self.clean_out_cache()
            cache_block = self.cache_strs[index:min(index+self.cache_block_size, self.num_strs)]
            #store cache block in cache
            self.cache[block_ind] = (cache_block)
            #append key to cache block list
            self.cache_blocks.append(block_ind)
        else:
            cache_block = self.cache[block_ind]
        #get a strings index inside of a cache block
        block_ind_ind = index%self.cache_size

        return cache_block[block_ind_ind]

    def __len__(self):
        return len(self.cache_strs)

    def clean_out_cache(self):
        """gets index of oldest cache block. and removes the block from cache and removes the index"""
        if len(self.cache_blocks) >= self.cache_size:
            block_ind = self.cache_blocks.pop(0)
            del self.cache[block_ind]


# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/__init__.py
import os
import math

from .samplers import BatchSampler, DistributedBatchSampler, TransposedSampler, RandomShardSampler, BatchShardSampler, DistributedBatchShardSampler
from .loaders import DataLoader, ShardLoader
from .preprocess import tokenize_str_batch, binarize_labels, process_str, process_tweet, batch_tokens
from .datasets import json_dataset, csv_dataset, split_ds, get_processed_path, ConcatDataset, SplitDataset, data_shard
from .lazy_loader import exists_lazy, make_lazy, lazy_array_loader
from .tokenization import Tokenization, CommandToken, Tokenizer, CharacterLevelTokenizer, make_tokenizer

TRAIN_DATA = 0
VAL_DATA = 1
TEST_DATA = 2

def should_split(split):
    return max(split)/sum(split) != 1.

def get_ext(path):
    return os.path.splitext(path)[1]

def get_dataset(path, **kwargs):
    """gets dataset object based on keyword args and file at `path`"""
    ext = get_ext(path)
    if ext =='.json':
        text = json_dataset(path, **kwargs)
    elif ext in ['.csv', '.tsv']:
        text = csv_dataset(path, **kwargs)
    else:
        raise NotImplementedError('data file type %s is not supported'%(ext))
    return text

def make_dataset(path, seq_length, text_key, label_key, lazy=False, process_fn=process_str, split=[1.],
                delim=',', loose=False, binarize_sent=False, drop_unlabeled=False, tokenizer=None,
                tokenizer_type='CharacterLevelTokenizer', tokenizer_model_path=None, vocab_size=None,
                model_type='bpe', pad_token=0, character_converage=1.0, non_binary_cols=None, **kwargs):
    if isinstance(process_fn, str):
        process_fn = eval(process_fn)
    if non_binary_cols is not None:
        label_key = non_binary_cols
    def get_dataset_from_path(path_):
        if lazy:
            if not exists_lazy(path_, data_type='data'):
                text = get_dataset(path_, text_key=text_key, label_key=label_key, binarize_sent=binarize_sent,
                    delim=delim, drop_unlabeled=drop_unlabeled, loose_json=loose)
                make_lazy(path_, text.X, data_type='data')
            text = lazy_array_loader(path_, data_type='data', map_fn=process_fn)
        else:
            text = get_dataset(path_, text_key=text_key, label_key=label_key, binarize_sent=binarize_sent,
                    delim=delim, drop_unlabeled=drop_unlabeled, loose_json=loose, preprocess_fn=process_fn)
        return text
    if isinstance(path, str):
        path = [path]
    datasets = [get_dataset_from_path(p) for p in path]
    if len(datasets) == 1:
        ds = datasets[0]
    else:
        ds = ConcatDataset(datasets)
    if tokenizer is None:
        tokenizer = make_tokenizer(tokenizer_type, ds, tokenizer_model_path, vocab_size, model_type, 
                                    pad_token, character_converage)
    ds.SetTokenizer(tokenizer)
    if should_split(split):
        ds = split_ds(ds, split)
    return ds, tokenizer

# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/tokenization.py
from collections import namedtuple
import random
import os

import sentencepiece as spm

def make_tokenizer(tokenizer_type, corpus, model_path=None, vocab_size=None, model_type='bpe', pad_token=0, character_coverage=1.0):
    tokenizer_class = tokenizer_type
    if isinstance(tokenizer_class, str):
        tokenizer_class = eval(tokenizer_class)
    return tokenizer_class(corpus=corpus, vocab_size=vocab_size, model_path=model_path, model_type=model_type,
                            pad_token=pad_token, character_coverage=character_coverage)

class Tokenization(object):
    def __init__(self, tokenization, text=None, original_text=None, command_tokens=None, asIds=True):
        self.tokenization = tokenization
        self.text = text
        if self.text is None:
            self.text = self.tokenization
        self.original_text = original_text
        if self.original_text is None:
            self.original_text = self.text
        self.command_tokens = command_tokens
        self.asIds = asIds
        self.parse_command_tokens()

    def parse_command_tokens(self):
        if self.command_tokens is None:
            return
        for command_token in self.command_tokens:
            if self.asIds:
                setattr(self, command_token.name, command_token.Id)
            else:
                setattr(self, command_token.name, command_token.token)

    def __getitem__(self, index):
        return self.tokenization[index]

    def __len__(self):
        return len(self.tokenization)

    def append(self, other):
        if isinstance(other, Tokenization):
            self.tokenization.extend(other.tokenization)
            self.text += other.text
            self.original_text += other.original_text
        else:
            self.tokenization.append(other)
        return self

    def extend(self, other):
        if isinstance(other, Tokenization):
            self.tokenization.extend(other.tokenization)
            self.text += other.text
            self.original_text += other.original_text
        else:
            self.tokenization.extend(other)
        return self


COMMAND_TUPLE = namedtuple('CommandToken', ('name', 'token', 'Id'))

token_format = "<{0}>"

def prep_command_tokens(tokenlist):
    return [CommandToken(tok[0], token_format.format(tok[0]), tok[1]) for tok in tokenlist]

class CommandToken(object):
    def __init__(self, name, token, Id):
        self.name = name
        self.token = token
        self.Id = Id

    def __str__(self):
        return str(COMMAND_TUPLE(self.name, self.token, self.Id))


class Tokenizer(object):
    def __init__(self, command_tokens=None):
        self.command_tokens = command_tokens
        self.command_token_map = {}
        self.command_id_map = {}
        if command_tokens is not None:
            self.command_name_map = {tok.name: tok for tok in command_tokens}
            self.command_token_map = {tok.token: tok for tok in command_tokens}
            self.command_id_map = {tok.Id: tok for tok in command_tokens}
        self.num_command_tokens = len(self.command_tokens)
        if not hasattr(self, 'num_text_tokens'):
            self.num_text_tokens = 0
        if not hasattr(self, 'num_tokens'):
            self.num_tokens = self.num_command_tokens + self.num_text_tokens

    def __call__(self, text, process_fn=None):
        return self.EncodeAsIds(text, process_fn)

    @staticmethod
    def exists(model_path):
        raise NotImplementedError('Tokenizer exists method not implemented')

    def Train(self, corpus):
        raise NotImplementedError('Tokenizer Train not implemented')

    def EncodeAsIds(self, text, process_fn=None):
        raise NotImplementedError('Tokenizer EncodeAsIds not implemented')

    def EncodeAsTokens(self, text, process_fn=None):
        raise NotImplementedError('Tokenizer EncodeAsTokens not implemented')

    def IdToToken(self, Id):
        raise NotImplementedError('Tokenizer IdToToken not implemented')

    def TokenToId(self, token):
        raise NotImplementedError('Tokenizer TokenToId not implemented')

    def DecodeIds(self, Ids):
        raise NotImplementedError('Tokenizer DecodeIds not implemented')

    def DecodeTokens(self, Tokens):
        raise NotImplementedError('Tokenizer DecodeTokens not implemented')
        

class CharacterLevelTokenizer(Tokenizer):
    def __init__(self, pad_token=0, **kwargs):
        self.num_text_tokens = 256
        super(CharacterLevelTokenizer, self).__init__(prep_command_tokens([('pad', pad_token)]))

    @staticmethod
    def exists(model_path):
        return True

    def Train(self, corpus):
        pass

    def EncodeAsIds(self, text, process_fn=None):
        processed_text = text
        if process_fn is not None:
            processed_text = process_fn(processed_text)
            processed_text = str(processed_text)
        tokens = [self.TokenToId(c) for c in processed_text]
        return Tokenization(tokens, processed_text, text, self.command_tokens)

    def EncodeAsTokens(self, text, process_fn=None):
        processed_text = text
        if process_fn is not None:
            processed_text = process_fn(processed_text)
        processed_text = str(processed_text)
        tokens = [c for c in processed_text]
        return Tokenization(tokens, processed_text, text, self.command_tokens, asIds=False)

    def IdToToken(self, Id):
        return chr(Id - self.num_command_tokens)

    def TokenToId(self, token):
        return ord(token) + self.num_command_tokens

    def DecodeIds(self, Ids):
        if isinstance(Ids, Tokenization):
            Ids = Ids.tokenization
        return ''.join([self.IdToToken(tok) for tok in Ids])

    def DecodeTokens(self, Tokens):
        if isinstance(Tokens, Tokenization):
            Tokens = Tokens.tokenization
        return ''.join(Tokens)


def write_corpus_as_lines(dataset, filepath):
    """
    Take Dataset or corpus, split it into lines, and write it to `filepath`.
    Return the total number of lines, and max length line.
    """
    total_sentence_count = 0
    maxlen = 0
    with open(filepath, 'w') as f:
        for entry in dataset:
            if isinstance(entry, dict):
                entry = entry['text']
            lines = entry.strip().split('\n')
            for line in lines:
                sentences = line.split('.')
                total_sentence_count += len(sentences)
                for sentence in sentences:
                    maxlen = max(len(line), maxlen)
                    f.write(sentence+'.\n')
    return total_sentence_count, maxlen

MAX_SENTENCEPIECE_SENTENCES = 100000000

class SentencePieceTokenizer(Tokenizer):
    def __init__(self, model_type='bpe', vocab_size=None, corpus=None, model_path=None, character_coverage=1.0, pad_token=0, **kwargs):
        self.character_coverage = character_coverage
        self.model_type = model_type.lower()
        self.spm_model = model_path
        self.num_text_tokens = vocab_size
        make_train = not SentencePieceTokenizer.exists(self.spm_model)
        if make_train:
            assert corpus is not None and self.num_text_tokens is not None
            self.Train(corpus, self.num_text_tokens)
        self.load_spm_model()
        super(SentencePieceTokenizer, self).__init__(prep_command_tokens([('pad', pad_token)]))

    @staticmethod
    def exists(model_path):
        if model_path is None:
            return False
        # check if path exists
        dne = not os.path.exists(model_path)
        # check if path.model exists
        if dne and not model_path.endswith('.model'):
            dne = not os.path.exists(model_path+'.model')
        return not dne

    def load_spm_model(self):
        if not os.path.exists(self.spm_model) and not self.spm_model.endswith('.model'):
            self.spm_model = self.spm_model+'.model'
        self.sp = spm.SentencePieceProcessor()
        self.sp.Load(self.spm_model)
        self.vocab_size = self.num_text_tokens = len(self.sp)

    def Train(self, corpus, num_text_tokens):
        self.num_text_tokens = num_text_tokens
        use_model_path = self.spm_model
        random_hash = str(random.randint(0, 2147483647))
        if use_model_path is None:
            use_model_path = random_hash
        if use_model_path.endswith('.model'):
            use_model_path = use_model_path[:use_model_path.rfind('.model')]
        input_path = use_model_path+'.txt.'+random_hash
        print('Writing temporary dataset for tokenization to '+input_path)
        line_count, maxlenline = write_corpus_as_lines(corpus, input_path)
        line_count = min(line_count, MAX_SENTENCEPIECE_SENTENCES)
        print('Training sentencepiece model')
        train_string = '--input={file_path} --model_prefix={model_prefix} --vocab_size={vocab_size}' \
            + ' --model_type={model_type} --input_sentence_size={input_sentence_size} --character_coverage={character_coverage} ' \
            + '--max_sentence_length={max_len}'
        train_string = train_string.format(file_path=input_path, model_prefix=use_model_path, vocab_size=num_text_tokens,
                            model_type=self.model_type, input_sentence_size=int(line_count), character_coverage=self.character_coverage,#)#,
                            max_len=str(maxlenline))
        spm.SentencePieceTrainer.Train(train_string)
        os.remove(input_path)
        self.spm_model = use_model_path+'.model'
        print('Sentencepiece model written to '+self.spm_model)

    def EncodeAsIds(self, text, process_fn=None):
        processed_text = text
        if process_fn is not None:
            processed_text = process_fn(processed_text)
        tokens = [tok + self.num_command_tokens for tok in self.sp.EncodeAsIds(processed_text)]
        # tokens = self.sp.EncodeAsIds(processed_text)
        return Tokenization(tokens, processed_text, text, self.command_tokens)

    def EncodeAsTokens(self, text, process_fn=None):
        processed_text = text
        if process_fn is not None:
            processed_text = process_fn(processed_text)
        tokens = self.sp.EncodeAsTokens(processed_text)
        return Tokenization(tokens, processed_text, text, self.command_tokens, asIds=False)

    def IdToToken(self, Id):
        return self.sp.IdToToken(Id - self.num_command_tokens)
        # return self.sp.IdToToken(Id)

    def TokenToId(self, token):
        return self.sp.TokenToId(token) + self.num_command_tokens

    def DecodeIds(self, Ids):
        if isinstance(Ids, Tokenization):
            Ids = Ids.tokenization
        return self.sp.DecodeIds([Ids - self.num_command_tokens])
        # return self.sp.DecodeIds(Ids)

    def DecodeTokens(self, Tokens):
        if isinstance(Tokens, Tokenization):
            Tokens = Tokens.tokenization
        return self.sp.DecodeTokens(Tokens)
# PROJECT: NVIDIA_sentiment-discovery FILE: data_utils/samplers.py
import math
import os
import sys

import torch
from torch.utils import data
import numpy as np

from .datasets import data_shard

class DistributedBatchSampler(data.sampler.BatchSampler):
    """
    similar to normal implementation of distributed batch sampler, except if sampler is transposed sampler
    has option to wrap around instead of not dropping last half batch. This is useful for persisting state
    """
    def __init__(self, sampler, batch_size, drop_last, rank=-1, world_size=2, wrap_last=False):
        super(DistributedBatchSampler, self).__init__(sampler, batch_size, drop_last)
        if rank == -1:
            rank = torch.distributed.get_rank()
        self.rank = rank
        self.world_size = world_size
        self.sampler.wrap_around = 0
        self.wrap_around = 0
        self.wrap_last = wrap_last
        self.start_iter = 0

    def __iter__(self):
        batch = []
        last_batch = None
        i = 0
        for idx in self.data_iterator(self.sampler, wrap_around=False):
            batch.append(idx)
            if len(batch) == self.batch_size:
                tbatch = self._batch(batch)
                if i >= self.start_iter:
                    yield tbatch
                    self.start_iter = 0
                i += 1
                last_batch = np.array(list(tbatch))
                batch = []
        batch_len = len(batch)
        if batch_len > 0 and not self.drop_last:
            if self.wrap_last:
                self.sampler.wrap_around -= (self.batch_size)
                self.wrap_around += (len(batch))
                self.wrap_around %= self.batch_size
                if isinstance(self.sampler, TransposedSampler):
                    for i, idx in enumerate(self.data_iterator(self.sampler, wrap_around=True)):
                        if i == 0:
                            continue
                        batch.append(idx)
                        new_batch_len = len(batch)
                        if len(batch) == self.batch_size:
                            break
            yield self._batch(batch)
        if self.wrap_last:
            self.sampler.wrap_around += self.batch_size

    def data_iterator(self, _iter, wrap_around=False):
        """iterates through data and handles wrap around"""
        for i, idx in enumerate(_iter):
            if i < self.wrap_around%self.batch_size:
                continue
            if wrap_around:
                self.wrap_around += 1
                self.wrap_around %= self.batch_size
            yield idx

    def _batch(self, batch):
        """extracts samples only pertaining to this worker's batch"""
        start = self.rank*self.batch_size//self.world_size
        end = (self.rank+1)*self.batch_size//self.world_size
        return batch[start:end]

class BatchSampler(data.sampler.BatchSampler):
    """
    Normal implementation of batch sampler, except if sampler is transposed sampler it
    has option to wrap around instead of not dropping last half batch.
    Useful for persisting state.
    """
    def __init__(self, sampler, batch_size, drop_last, wrap_last=False):
        super(BatchSampler, self).__init__(sampler, batch_size, drop_last)
        self.wrap_around = 0
        self.sampler.wrap_around = 0
        self.wrap_last = wrap_last
        self.start_iter = 0

    def __iter__(self):
        batch = []
        last_batch = None
        i = 0
        for idx in self.data_iterator(self.sampler, wrap_around=False):
            batch.append(idx)
            new_batch_len = len(batch)
            if new_batch_len == self.batch_size:
                if i >= self.start_iter:
                    yield batch
                    self.start_iter = 0
                i += 1
                last_batch = np.array(list(batch))
                batch = []

        if len(batch) > 0 and (self.wrap_last or not self.drop_last):
            if self.wrap_last:
                self.sampler.wrap_around -= (self.batch_size)
                self.wrap_around += (len(batch))
                self.wrap_around %= self.batch_size
                if isinstance(self.sampler, TransposedSampler):
                    for i, idx in enumerate(self.data_iterator(self.sampler, wrap_around=True)):
                        if i == 0:
                            continue
                        batch.append(idx)
                        if len(batch) == self.batch_size:
                            break
            yield batch
        if self.wrap_last:
            self.sampler.wrap_around += self.batch_size

    def data_iterator(self, _iter, wrap_around=False):
        """iterates through data and handles wrap around"""
        for i, idx in enumerate(_iter):
            if i < self.wrap_around%self.batch_size:
                continue

            if wrap_around:
                self.wrap_around += 1
                self.wrap_around %= self.batch_size
            yield idx

class TransposedSampler(data.sampler.Sampler):
    """
    Instead of performing sequential sampling, samples array in a transposed fashion given the
    batch size to sampled. Instead of generating the following indices for a batch size of 2
        1 3 5
        2 4 6
    It will generate
        1 2 3
        4 5 6
    """
    def __init__(self, data_source, batch_size, data_sampler=None):
        self.data_source = data_source
        self.batch_size = batch_size
        self.len_ds = len(data_source)
        self.strat_width = self.len_ds//batch_size
        #self.strat_width = math.ceil(self.len_ds/batch_size)
        self.data_sampler = data_sampler
        self.wrap_around = 0

    def transpose_helper(self, x):
        """computes index corrseponding to transpose of index x"""
        return ((x%self.batch_size)*self.strat_width+(x//self.batch_size))%self.len_ds
        x += self.wrap_around
        return ((x%self.batch_size)*self.strat_width+(x//self.batch_size))%self.len_ds

    def __iter__(self):
        if self.data_sampler is None:
            return iter(map(self.transpose_helper, range(len(self))))
        return iter(map(self.transpose_helper, iter(self.data_sampler)))

    def __len__(self):
        #return self.len_ds
        return self.strat_width*self.batch_size


class RandomShardSampler(object):
    """
    Sampler for data shards.
    Purpose: Samples data shards used for L2R unsupervised modeling from the `data_source`.
    Arguments:
        data_source (Dataset or array-like): Dataset of tokenizations to sample data from.
        samples_per_shard (int): Number of samples per shard to gather from `data_source`.
        seq_len (int): seq_len value to use when creating a data shard. Can be reset later with
            `set_seq_len`.
        persist_state (int): persist_state value to use when creating a data shard. See 
            data_utils.data_shard documentation for valid values. Can be reset later with 
            `set_persist_state`.
        random_state (np.RandomState): Random number generator state to use for sampling data. If
            no value is supplied it uses numpy's default random state (not thread safe).
    """

    def __init__(self, data_source, samples_per_shard, seq_len=-1, persist_state=0):
        self.data_source = data_source
        self.source_size = len(data_source)
        self.samples_per_shard = samples_per_shard
        self.seq_len = seq_len
        self.persist_state = persist_state

    def set_seq_len(self, seq_len):
        self.seq_len = seq_len

    def set_samples_per_shard(self, samples_per_shard):
        self.samples_per_shard = samples_per_shard

    def set_persist_state(self, persist_state):
        self.persist_state = persist_state

    def get(self, random_state, samples_per_shard=None):
        """
        Uses either supplied random state or default random state to sample data from 
        the data source, create a datashard, and return it.
        """
        if samples_per_shard is None:
            samples_per_shard = self.samples_per_shard
        sample_ids = random_state.randint(self.source_size, size=samples_per_shard)
        samples = [self.data_source[i] for i in sample_ids]
        samples = [sample['text'] if isinstance(sample, dict) else sample for sample in samples]
        return data_shard(samples, self.seq_len, self.persist_state)


    def __len__(self):
        return self.source_size


class BatchShardSampler(object):
    """
    Class to manage the random state of and sample a batch of active shards.
    Uses one random state per batch index to control sampling of data shards for that batch index.
    Purpose: Intended for use with data_utils.ShardLoader to perform L2R unsupervised Learning.
    Arguments: 
        shard_sampler (RandomShardSampler): shard sampler used to sample data shards.
        batch_size (int): Batch size to sample.
        drop_last (boolean): Pretty much useless. Used to give a fake length.
        random_batch (list): List of random states to use.
    Attributes:
        batch (list): Batch of shard queues (a list that contains shards). Call `.get` and 
            `.isdone()` on `shard_queue[0]` to get next batch and check if shard is done.
    """
    def __init__(self, shard_sampler, batch_size, drop_last, random_batch=None):
        self.shard_sampler = shard_sampler
        self.batch_size = batch_size
        self.drop_last = drop_last
        # self.batch = None
        self.random_batch = random_batch
        if self.random_batch is None:
            self.random_batch = [np.random.RandomState(seed) for seed in np.random.randint(batch_size*999, size=batch_size)]

    def set_seq_len(self, seq_len):
        self.seq_len = seq_len
        self.shard_sampler.set_seq_len(seq_len)

    def set_samples_per_shard(self, samples_per_shard):
        self.samples_per_shard = samples_per_shard
        self.shard_sampler.set_samples_per_shard(samples_per_shard)

    def set_persist_state(self, persist_state):
        self.persist_state = persist_state
        self.shard_sampler.set_persist_state(persist_state)

    def get_shard(self, b):
        return self.shard_sampler.get(random_state=self.random_batch[b])

    def iter_queue(self, b):
        live_shard = self.get_shard(b)
        while True:
            if live_shard.is_done():
                live_shard = self.get_shard(b)
            yield live_shard.get()

    def manage_queues(self):
        queues = [self.iter_queue(b) for b in range(self.batch_size)]
        while True:
            yield [next(q) for q in queues]

    def manage_queues_multiproc(self, queue_indices=None, output_queue=None):
        assert output_queue is not None
        if queue_indices is None:
            queue_indices = list(range(self.batch_size))
        queues = [self.iter_queue(b) for b in queue_indices]

        while True:
            output_queue.put([next(q) for q in queues], block=True)

    def __iter__(self):
        return self.manage_queues()

    def __len__(self):
        if self.drop_last:
            return len(self.shard_sampler) // self.batch_size
        else:
            return (len(self.shard_sampler) + self.batch_size - 1) // self.batch_size

class DistributedBatchShardSampler(BatchShardSampler):
    """
    Coordinates random states so that shard sampling for distributed training can be coordinated
    without any communication between distributed processes. This is possible since random numbers
    are pseudo-deterministic, so if the random states of the global batch are known data loading
    can be coordinated without communication with other processes.
    Purpose: For use with distributed training of L2R modeling.
    Arguments: 
        shard_sampler (RandomShardSampler): Shard sampler used to sample data shards.
        local_batch_size (int): Local batch size to sample.
        drop_last (boolean): Pretty much useless. Used to give a fake length.
        local_random_batch (list): List of random states to use locally for this worker.
        world_size (int): Number of workers in distributed training.
        rank (int): Rank of this distributed worker.
        batch (list): Batch of shard queues (a list that contains shards). Call `.get` and 
            `.isdone()` on `shard_queue[0]` to get next batch and check if shard is done.
    """
    def __init__(self, shard_sampler, local_batch_size, drop_last, local_random_batch=None, world_size=1, rank=0):
        self.global_batch_size = int(local_batch_size*world_size)
        if local_random_batch is None:
            local_random_batch = [np.random.RandomState(seed) for seed in np.random.randint(self.global_batch_size*999, size=self.global_batch_size)]
            local_random_batch = local_random_batch[local_batch_size*rank:local_batch_size*(rank+1)]
        super(DistributedBatchShardSampler, self).__init__(shard_sampler, local_batch_size, drop_last, local_random_batch)

    def __len__(self):
        if self.drop_last:
            return len(self.sampler) // self.global_batch_size
        else:
            return (len(self.sampler) + self.global_batch_size - 1) // self.global_batch_size
# PROJECT: NVIDIA_sentiment-discovery FILE: logreg_utils.py
###############################################################################
# BSD 3-Clause License
#
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#   
# Copyright (c) 2017, openai. All rights reserved.
###############################################################################
"""
Modified version of openai implementation https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/utils.py
Modified to handle multiple classes, different metrics, thresholding, and dropping neurons.
"""
import collections

import numpy as np
from sklearn.linear_model import LogisticRegression

from metric_utils import update_info_dict, get_metric
from threshold import _binary_threshold, _neutral_threshold_two_output

def train_logreg(trX, trY, vaX=None, vaY=None, teX=None, teY=None, penalty='l1', max_iter=100,
        C=2**np.arange(-8, 1).astype(np.float), seed=42, model=None, eval_test=True, neurons=None,
        drop_neurons=False, report_metric='acc', automatic_thresholding=False, threshold_metric='acc', micro=False):
    
    # if only integer is provided for C make it iterable so we can loop over
    if not isinstance(C, collections.Iterable):
        C = list([C])
    # extract features for given neuron indices
    if neurons is not None:
        if drop_neurons:
            all_neurons = set(list(range(trX.shape[-1])))
            neurons = set(list(neurons))
            neurons = list(all_neurons - neurons)
        trX = trX[:, neurons]
        if vaX is not None:
            vaX = vaX[:, neurons]
        if teX is not None:
            teX = teX[:, neurons]
    # Cross validation over C
    n_classes = 1
    if len(trY.shape)>1:
        n_classes = trY.shape[-1]
    scores = []
    if model is None:
        for i, c in enumerate(C):
            if n_classes <= 1:
                model = LogisticRegression(C=c, penalty=penalty, max_iter=max_iter, random_state=seed)
                model.fit(trX, trY)
                blank_info_dict = {'fp' : 0, 'tp' : 0, 'fn' : 0, 'tn' : 0, 'std' : 0.,
                                   'metric' : threshold_metric, 'micro' : micro}
                if vaX is not None:
                    info_dict = update_info_dict(blank_info_dict.copy(), vaY, model.predict_proba(vaX)[:, -1])
                else:
                    info_dict = update_info_dict(blank_info_dict.copy(), trY, model.predict_proba(trX)[:, -1])
                scores.append(get_metric(info_dict))
                print(scores[-1])
                del model
            else:
                info_dicts = []
                model = []
                for cls in range(n_classes):
                    _model = LogisticRegression(C=c, penalty=penalty, max_iter=max_iter, random_state=seed)
                    _model.fit(trX, trY[:, cls])
                    blank_info_dict = {'fp' : 0, 'tp' : 0, 'fn' : 0, 'tn' : 0, 'std' : 0.,
                                       'metric' : threshold_metric, 'micro' : micro}
                    if vaX is not None:
                        info_dict = update_info_dict(blank_info_dict.copy(), vaY[:, cls], _model.predict_proba(vaX)[:, -1])
                    else:
                        info_dict = update_info_dict(blank_info_dict.copy(), trY[:, cls], _model.predict_proba(trX)[:, -1])
                    info_dicts.append(info_dict)
                    model.append(_model)
                scores.append(get_metric(info_dicts))
                print(scores[-1])
                del model
        c = C[np.argmax(scores)]
        if n_classes <= 1:
            model = LogisticRegression(C=c, penalty=penalty, max_iter=max_iter, random_state=seed)
            model.fit(trX, trY)
        else:
            model = []
            for cls in range(n_classes):
                _model = LogisticRegression(C=c, penalty=penalty, max_iter=max_iter, random_state=seed)
                _model.fit(trX, trY[:, cls])
                model.append(_model)
    else:
        c = model.C
    # predict probabilities and get accuracy of regression model on train, val, test as appropriate
    # also get number of regression weights that are not zero. (number of features used for modeling)
    scores = []
    if n_classes == 1:
        nnotzero = np.sum(model.coef_ != 0)
        preds = model.predict_proba(trX)[:, -1]
        train_score = get_metric(update_info_dict(blank_info_dict.copy(), trY, preds), report_metric)
    else:
        nnotzero = 0
        preds = []
        info_dicts = []
        for cls in range(n_classes):
            nnotzero += np.sum(model[cls].coef_ != 0)
            _preds = model[cls].predict_proba(trX)[:, -1]
            info_dicts.append(update_info_dict(blank_info_dict.copy(), trY[:, cls], _preds))
            preds.append(_preds)
        nnotzero/=n_classes
        train_score = get_metric(info_dicts, report_metric)
        preds = np.concatenate([p.reshape((-1, 1)) for p in preds], axis=1)
    scores.append(train_score * 100)
    if vaX is None:
        eval_data = trX
        eval_labels = trY
        val_score = train_score
    else:
        eval_data = vaX
        eval_labels = vaY
        if n_classes == 1:
            preds = model.predict_proba(vaX)[:, -1]
            val_score = get_metric(update_info_dict(blank_info_dict.copy(), vaY, preds), report_metric)
        else:
            preds = []
            info_dicts = []
            for cls in range(n_classes):
                _preds = model[cls].predict_proba(vaX)[:, -1]
                info_dicts.append(update_info_dict(blank_info_dict.copy(), vaY[:, cls], _preds))
                preds.append(_preds)
            val_score = get_metric(info_dicts, report_metric)
            preds = np.concatenate([p.reshape((-1, 1)) for p in preds], axis=1)
    val_preds = preds
    val_labels = eval_labels
    scores.append(val_score * 100)
    eval_score = val_score
    threshold = np.array([.5]*n_classes)
    if automatic_thresholding:
        _, threshold, _, _ = _binary_threshold(preds.reshape(-1, n_classes), eval_labels.reshape(-1, n_classes), threshold_metric, micro)
        threshold = float(threshold.squeeze())
    if teX is not None and teY is not None and eval_test:
        eval_data = teX
        eval_labels = teY
        if n_classes == 1:
            preds = model.predict_proba(eval_data)[:, -1]
        else:
            preds = []
            for cls in range(n_classes):
                _preds = model[cls].predict_proba(eval_data)[:, -1]
                preds.append(_preds)
            preds = np.concatenate([p.reshape((-1, 1)) for p in preds], axis=1)
    if n_classes == 1:
        threshold = float(threshold.squeeze())
        eval_score = get_metric(update_info_dict(blank_info_dict.copy(), eval_labels, preds, threshold=threshold), report_metric)
    else:
        info_dicts = []
        for cls in range(n_classes):
            info_dicts.append(update_info_dict(blank_info_dict.copy(), eval_labels[:, cls], preds[:, cls], threshold=threshold[cls]))
        eval_score = get_metric(info_dicts, report_metric)

    scores.append(eval_score * 100)
    return model, scores, preds, c, nnotzero
# PROJECT: NVIDIA_sentiment-discovery FILE: metric_utils.py
import torch
import itertools

# At pain of messing up a good thing, also collect standard deviation (total) -- divided by total items for average
def update_info_dict(info_dict, labels, preds, threshold=0.5, std=None):
    preds = (torch.tensor(preds) > threshold).long()
    labels = (torch.tensor(labels) > threshold).long()
    # For backward compatibility -- if no std, assume it's zero -- and put it on CUDA if needed
    if std is not None:
        info_dict['std'] += torch.sum(torch.tensor(std)).float()
    else:
        info_dict['std'] += torch.sum((preds == 1) & (preds == 0)).float()
    info_dict['tp'] += torch.sum((preds == 1) & (labels == 1)).float()
    info_dict['tn'] += torch.sum((preds == 0) & (labels == 0)).float()
    info_dict['fp'] += torch.sum((preds == 1) & (labels == 0)).float()
    info_dict['fn'] += torch.sum((preds == 0) & (labels == 1)).float()
    return info_dict

# Mis-nomer -- returns standard deviation per class.
def get_variance(tp, tn, fp, fn, std):
    total = tp + tn + fp + fn
    return std / total

# TODO: Also return variance per class (in multihead sense) as a metric
def get_metric(infos, metric=None, micro=False):
    """Essentially a case-switch for getting a metric"""
    metrics = {
        'acc'  : get_accuracy,
        'jacc' : get_jaccard_index,
        'f1'   : get_f1,
        'mcc'  : get_mcc,
        'recall': get_recall,
        'precision': get_precision,
        'var'  : get_variance
    }
    tp = tn = fp = fn = std = 0
    if isinstance(infos, dict):
        infos = [infos]
    metric = metrics[infos[0].get('metric') or metric]
    micro = infos[0].get('micro') or micro
    stats = ['tp', 'tn', 'fp', 'fn', 'std']

    if micro:
        # micro averaging computes the metric after aggregating
        # all of the parameters from sets being averaged
        for info in infos:
            tp += info['tp']
            tn += info['tn']
            fp += info['fp']
            fn += info['fn']
            std += info['std']
        return metric(tp, tn, fp, fn, std)
    else:
        # macro averaging computes the metric on each set
        # and averages the metrics afterward
        individual_metrics = []
        for info in infos:
            individual_metrics.append(metric(*[info[s].item() for s in stats]))
        return sum(individual_metrics) / len(individual_metrics)

# Metrics as functions of true positive, true negative,
# false positive, false negative, standard deviation
def get_precision(tp, tn, fp, fn, std):
    if tp == 0:
        return 0
    return tp / (tp + fp)

def get_recall(tp, tn, fp, fn, std):
    if tp == 0:
        return 0
    return tp / (tp + fn)

def get_jaccard_index(tp, tn, fp, fn, std):
    if tp == 0:
        return 0
    return (tp) / (tp + fp + fn)

def get_accuracy(tp, tn, fp, fn, std):
    return (tp + tn) / (tp + tn + fp + fn)

def get_f1(tp, tn, fp, fn, std):
    if tp == 0:
        return 0
    return 2.0 * tp / (2 * tp + fp + fn)

def get_mcc(tp, tn, fp, fn, std):
    total = (tp + tn + fp + fn)
    for v in tp, tn, fp, fn:
        v /= total
    denom = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5
    denom = denom if denom > 1e-8 else 1
    return (tp * tn - fp * fn) / denom


# PROJECT: NVIDIA_sentiment-discovery FILE: learning_rates.py
from torch.optim.lr_scheduler import _LRScheduler
import math

class LinearLR(_LRScheduler):
    """
    A scheduler for linear learning rate decay to 0 over a specified number of steps.
    Args:
        optimizer (Optimizer): Wrapped optimizer.
        max_iters (int): Period of learning rate decay. When last_iter==max_iters lr=max(min_lr,0)
        last_iter (int): The index of last iteration step. Default: -1
        min_lr (float): smallest allowed learning rate (acts as a clamp to prevent too small learning rates). Default: 1e-8
    Example:
        >>> # Assuming optimizer also uses lr = 0.0005 for all groups
        >>> scheduler = LinearLR(optimizer, max_iters=10, last_iter=-1, min_lr=1e-8)
        >>> for iter in range(10):
        >>>     train(...)
        >>>        scheduler.step()
        >>> validate(...)
    """
    def __init__(self, optimizer, max_iters, last_iter=-1, min_lr=1e-8):
        self.optimizer = optimizer
        self.max_iters = max_iters
        self.num_iters = last_iter
        self.min_lr = min_lr
        self.done = False
        if last_iter == -1:
            for group in optimizer.param_groups:
                group.setdefault('initial_lr', group['lr'])
        else:
            for i, group in enumerate(optimizer.param_groups):
                if 'initial_lr' not in group:
                    raise KeyError("param 'initial_lr' is not specified "
                                   "in param_groups[{}] when resuming an optimizer".format(i))
        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))
        self.step(last_iter + 1)

    def get_lr(self):
        return [self.decay_func(base_lr) for base_lr in self.base_lrs]

    def decay_func(self, init_lr):
        new_lr = init_lr*((self.max_iters-self.num_iters)/self.max_iters)
        return max(new_lr, self.min_lr)

    def step(self, epoch=None):
        if epoch is None:
            epoch = self.num_iters + 1
        self.num_iters = epoch
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr
        return self.done

class ConstantLR(_LRScheduler):
    def __init__(self, optimizer, lr):
        self.optimizer = optimizer
        for group in optimizer.param_groups:
            group['lr'] = lr

    def step(self, step_num=None):
        pass

class SlantedTriangularLR(_LRScheduler):
    """
    Implements the "slanted triangular learning rate schedule used for ULMFiT as a function of
    the number of training iterations" (arxiv.org/pdf/1801.06146.pdf)
    Args:
        optimizer (Optimizer): Wrapped optimizer.
        lr_ratio (float): ratio of minimum to maximum learning rate (32 in paper)
        max_val (float): highest learning rate (attained at peak of slanted triangle - 0.01 in paper)
        cut_frac (float): proportion of iterations during which learning rate is increasing (0.1 in paper)
        num_iters (int): total number of iterations expected (should be one epoch)
    """
    def __init__(self, optimizer, lr_ratio=100, max_val=6.25e-5, cut_frac=0.002, num_iters=1000):
        self.optimizer = optimizer
        self.min_val = max_val / lr_ratio
        self.max_val = max_val
        self.peak_iter = num_iters * cut_frac
        self.end_triangle_iter = num_iters
        self.num_iters = 0
        self.lr_func = self.create_lr_func()


        for group in optimizer.param_groups:
            group['weight_decay'] = 0.01
            if 'name' in group.keys() and group['name'] == 'low':
                group['lr'] = self.min_val / 2.6
            else:
                group['lr'] = self.min_val

    def create_lr_func(self):
        lr_range = self.max_val - self.min_val

        up_slope = lr_range / self.peak_iter
        up_intercept = self.min_val
        down_slope = -lr_range / (self.end_triangle_iter - self.peak_iter)
        down_intercept = -down_slope * self.peak_iter + self.max_val

        def lr_func():
            if self.num_iters <= self.peak_iter:
                return up_slope * self.num_iters + up_intercept
            else:
                return down_slope * self.num_iters + down_intercept

        return lr_func

    def step(self, step_num=None):
        if step_num is None:
            step_num = self.num_iters + 1
        self.num_iters = step_num
        new_lr = self.lr_func()
        for group in self.optimizer.param_groups:
            if 'name' in group.keys() and group['name'] == 'low':
                group['lr'] = new_lr / 2.6
            else:
                group['lr'] = new_lr


class CosineAnnealingLR(_LRScheduler):
    """Anneals the learning rate from start to zero along a cosine curve."""

    def __init__(self, optimizer, start_lr, warmup_iter, num_iters):
        self.optimizer = optimizer
        self.start_lr = start_lr
        self.warmup_iter = warmup_iter
        self.num_iters = 0
        self.end_iter = num_iters

    def get_lr(self):
        # https://openreview.net/pdf?id=BJYwwY9ll pg. 4
        if self.num_iters <= self.warmup_iter:
            return float(self.start_lr) * self.num_iters / self.warmup_iter
        else:
            return self.start_lr / 2.0 * (math.cos(math.pi * (self.num_iters - self.warmup_iter) / self.end_iter) + 1)

    def step(self, step_num=None):
        if step_num is None:
            step_num = self.num_iters + 1
        self.num_iters = step_num
        new_lr = self.get_lr()
        for group in self.optimizer.param_groups:
            group['lr'] = new_lr

class AnnealingLR(_LRScheduler):
    """Anneals the learning rate from start to zero along a cosine curve."""

    DECAY_STYLES = ['linear', 'cosine', 'exponential', 'constant', 'None']

    def __init__(self, optimizer, start_lr, warmup_iter, num_iters, decay_style=None):
        self.optimizer = optimizer
        self.start_lr = start_lr
        self.warmup_iter = warmup_iter
        self.num_iters = 0
        self.end_iter = num_iters
        self.decay_style = decay_style.lower() if isinstance(decay_style, str) else None
        print('decaying', decay_style)

    def get_lr(self):
        # https://openreview.net/pdf?id=BJYwwY9ll pg. 4
        if self.num_iters <= self.warmup_iter:
            return float(self.start_lr) * self.num_iters / self.warmup_iter
        else:
            if self.decay_style == self.DECAY_STYLES[0]:
                return self.start_lr*((self.end_iter-(self.num_iters-self.warmup_iter))/self.end_iter)
            elif self.decay_style == self.DECAY_STYLES[1]:
                return self.start_lr / 2.0 * (math.cos(math.pi * (self.num_iters - self.warmup_iter) / self.end_iter) + 1)
            elif self.decay_style == self.DECAY_STYLES[2]:
                #TODO: implement exponential decay
                return self.start_lr
            else:
                return self.start_lr

    def step(self, step_num=None):
        if step_num is None:
            step_num = self.num_iters + 1
        self.num_iters = step_num
        new_lr = self.get_lr()
        for group in self.optimizer.param_groups:
            group['lr'] = new_lr


class DiscriminativeFinetuneWrapper(object):
    def __init__(self, optimizer, layer_lambda, lr_ratio=0.3):
        pass


class WarmupLR:
    def __init__(self, optimizer, max_iters, last_iter=-1):
        self.optimizer = optimizer
        self.max_iters = max_iters
        self.num_iters = last_iter
        self.step(last_iter + 1)

    def scale_lr(self, lr):
        return (lr * (self.num_iters+1) / self.max_iters)

    def step(self, epoch=None):
        if epoch is None:
            epoch = self.num_iters + 1
        self.num_iters = epoch
        if self.num_iters >= self.max_iters:
            return
        for param_group in self.optimizer.param_groups:
            lr = param_group['lr']
            param_group['lr'] = self.scale_lr(lr)

# PROJECT: NVIDIA_sentiment-discovery FILE: run_classifier.py
import argparse
import os
import time
import math
import collections
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F

import numpy as np
import pandas as pd

from reparameterization import apply_weight_norm, remove_weight_norm

from model import SentimentClassifier
from configure_data import configure_data
from arguments import add_general_args, add_model_args, add_classifier_model_args, add_run_classifier_args

def get_data_and_args():
    parser = argparse.ArgumentParser(description='PyTorch Sentiment Discovery Classification')
    parser = add_general_args(parser)
    parser = add_model_args(parser)
    parser = add_classifier_model_args(parser)
    data_config, data_parser, run_classifier_parser, parser = add_run_classifier_args(parser)
    args = parser.parse_args()

    args.cuda = torch.cuda.is_available()
    args.shuffle=False

    if args.seed is not -1:
        torch.manual_seed(args.seed)
        if args.cuda:
            torch.cuda.manual_seed(args.seed)

    (train_data, val_data, test_data), tokenizer = data_config.apply(args)
    args.data_size = tokenizer.num_tokens
    args.padding_idx = tokenizer.command_name_map['pad'].Id
    return (train_data, val_data, test_data), tokenizer, args

def get_model(args):

    sd = None
    model_args = args
    if args.load is not None and args.load != '':
        sd = torch.load(args.load)
        if 'args' in sd:
            model_args = sd['args']
        if 'sd' in sd:
            sd = sd['sd']

    ntokens = model_args.data_size
    concat_pools = model_args.concat_max, model_args.concat_min, model_args.concat_mean
    if args.model == 'transformer':
        model = SentimentClassifier(model_args.model, ntokens, None, None, None, model_args.classifier_hidden_layers, model_args.classifier_dropout,
                                      None, concat_pools, False, model_args)
    else:
        model = SentimentClassifier(model_args.model, ntokens, model_args.emsize, model_args.nhid, model_args.nlayers,
                                      model_args.classifier_hidden_layers, model_args.classifier_dropout, model_args.all_layers, concat_pools, False, model_args)
    args.heads_per_class = model_args.heads_per_class
    args.use_softmax = model_args.use_softmax
    try:
        args.classes = list(model_args.classes)
    except:
        args.classes = [args.label_key]

    try:
        args.dual_thresh = model_args.dual_thresh and not model_args.joint_binary_train
    except:
        args.dual_thresh = False

    if args.cuda:
        model.cuda()

    if args.fp16:
        model.half()

    if sd is not None:
        try:
            model.load_state_dict(sd)
        except:
            # if state dict has weight normalized parameters apply and remove weight norm to model while loading sd
            if hasattr(model.lm_encoder, 'rnn'):
                apply_weight_norm(model.lm_encoder.rnn)
            else:
                apply_weight_norm(model.lm_encoder)
            model.lm_encoder.load_state_dict(sd)
            remove_weight_norm(model)

    if args.neurons > 0:
        print('WARNING. Setting neurons %s' % str(args.neurons))
        model.set_neurons(args.neurons)
    return model

# uses similar function as transform from transfer.py
def classify(model, text, args):
    # Make sure to set *both* parts of the model to .eval() mode. 
    model.lm_encoder.eval()
    model.classifier.eval()
    # Initialize data, append results
    stds = np.array([])
    labels = np.array([])
    label_probs = np.array([])
    first_label = True
    heads_per_class = args.heads_per_class

    def get_batch(batch):
        text = batch['text'][0]
        timesteps = batch['length']
        labels = batch['label']
        text = Variable(text).long()
        timesteps = Variable(timesteps).long()
        labels = Variable(labels).long()
        if args.max_seq_len is not None:
            text = text[:, :args.max_seq_len]
            timesteps = torch.clamp(timesteps, max=args.max_seq_len)
        if args.cuda:
            text, timesteps, labels = text.cuda(), timesteps.cuda(), labels.cuda()
        return text.t(), labels, timesteps-1

    def get_outs(text_batch, length_batch):
        if args.model.lower() == 'transformer':
            class_out, (lm_or_encoder_out, state) = model(text_batch, length_batch, args.get_hidden)
        else:
            model.lm_encoder.rnn.reset_hidden(args.batch_size)
            for _ in range(1 + args.num_hidden_warmup):
                class_out, (lm_or_encoder_out, state) = model(text_batch, length_batch, args.get_hidden)
        if args.use_softmax and args.heads_per_class == 1:
            class_out = F.softmax(class_out, -1)
        return class_out, (lm_or_encoder_out, state)


    tstart = start = time.time()
    n = 0
    len_ds = len(text)
    with torch.no_grad():
        for i, data in tqdm(enumerate(text), total=len(text)):
            text_batch, labels_batch, length_batch = get_batch(data)
            size = text_batch.size(1)
            n += size
            # get predicted probabilities given transposed text and lengths of text
            probs, _ = get_outs(text_batch, length_batch)
#            probs = model(text_batch, length_batch)
            if first_label:
                first_label = False
                labels = []
                label_probs = []
                if heads_per_class > 1:
                    stds = []
            # Save variances, and predictions
            # TODO: Handle multi-head [multiple classes out]
            if heads_per_class > 1:
                _, probs, std, preds = probs
                stds.append(std.data.cpu().numpy())
            else:
                probs, preds = probs
                if args.use_softmax:
                    probs = F.softmax(probs, -1)
            labels.append(preds.data.cpu().numpy())
            label_probs.append(probs.data.cpu().numpy())

            num_char = length_batch.sum().item()

            end = time.time()
            elapsed_time = end - start
            total_time = end - tstart
            start = end

            s_per_batch = total_time / (i+1)
            timeleft = (len_ds - (i+1)) * s_per_batch
            ch_per_s = float(num_char) / elapsed_time

    if not first_label:
        labels = (np.concatenate(labels)) #.flatten())
        label_probs = (np.concatenate(label_probs)) #.flatten())
        if heads_per_class > 1:
            stds = (np.concatenate(stds))
        else:
            stds = np.zeros_like(labels)
    print('%0.3f seconds to transform %d examples' %
                  (time.time() - tstart, n))
    return labels, label_probs, stds

def make_header(classes, heads_per_class=1, softmax=False, dual_thresh=False):
    header = []
    if softmax:
        header.append('prediction')
    for cls in classes:
        if not softmax:
            header.append(cls + ' pred')
        header.append(cls + ' prob')
        if heads_per_class > 1:
            header.append(cls + ' std')
    if dual_thresh:
        header.append('neutral pred')
        header.append('neutral prob')
    return header

def get_row(pred, prob, std, classes, heads_per_class=1, softmax=False, dual_thresh=False):
    row = []
    if softmax:
        row.append(pred[0])
    for i in range(len(classes)):
        if not softmax:
            row.append(pred[i])
        row.append(prob[i])
        if heads_per_class > 1:
            row.append(std[i])
    if dual_thresh:
        row.append(pred[2])
        row.append(prob[2])
    return row 

def get_writer(preds, probs, stds, classes, heads_per_class=1, softmax=False, dual_thresh=False):
    header = make_header(classes, heads_per_class, softmax, dual_thresh)
    yield header
    for pred, prob, std in zip(preds, probs, stds):
        yield get_row(pred, prob, std, classes, heads_per_class, softmax, dual_thresh)

def main():
    (train_data, val_data, test_data), tokenizer, args = get_data_and_args()
    model = get_model(args)

    ypred, yprob, ystd = classify(model, train_data, args)

    save_root = ''
    save_root = os.path.join(save_root, args.save_probs)

    print('saving predicted probabilities to '+save_root)
    np.save(save_root, ypred)
    np.save(save_root+'.prob', yprob)
    np.save(save_root+'.std', ystd)

    if args.write_results is None or args.write_results == '':
        exit()

    print('writing results to '+args.write_results)
    writer = get_writer(ypred, yprob, ystd, args.classes, args.heads_per_class, args.use_softmax, args.dual_thresh)
    train_data.dataset.write(writer, path=args.write_results)

if __name__ == '__main__':
    main()

# PROJECT: NVIDIA_sentiment-discovery FILE: threshold.py
from sklearn import metrics
import itertools
import argparse
import torch
import numpy as np
import pandas as pd
from metric_utils import update_info_dict, get_metric
from collections import defaultdict
from tqdm import tqdm

def binary_threshold(args, labels=None):
    preds = pd.read_csv(args.preds_file, header=None).values
    labels = pd.read_csv(args.labels_file, header=None).values
    avg_metric, best_thresholds, category_metrics, category_best_info_dicts = _binary_threshold(preds, labels, args.metric, args.micro)
    print(avg_metric / preds.shape[1])
    np.savetxt('best_binary_thresholds_{}_{}.txt'.format('micro' if args.micro else 'macro', args.metric), (best_thresholds))


def _binary_threshold(preds, labels, metric, micro, global_tweaks=1000, debug=False, heads_per_class=1, class_single_threshold=False):
    avg_metric = 0
    best_thresholds = []
    info_dicts = []
    category_metrics = []
    # Compute threshold per class... *unless* multiple heads per class and one threshold required.
    num_categories = labels.shape[1]
    for category in range(num_categories):
        category_best_threshold = category_best_metric = 0
        for threshold in np.linspace(0.005, 1, 200):
            if heads_per_class > 1 and class_single_threshold:
                info_dict = update_info_dict(defaultdict(int), labels[:, (category * heads_per_class):(category+1)*heads_per_class], preds[:, (category * heads_per_class):(category+1)*heads_per_class], threshold=threshold)
            else:
                info_dict = update_info_dict(defaultdict(int), labels[:, category], preds[:, category], threshold=threshold)
            metric_score = get_metric(info_dict, metric, micro)
            if metric_score > category_best_metric or category_best_metric==0:
                category_best_metric, category_best_threshold, category_best_info_dict = metric_score, threshold, info_dict
        info_dicts.append(category_best_info_dict)
        category_metrics.append(category_best_metric)
        best_thresholds.append(category_best_threshold)

    # HACK -- use micro average here, even if not elsewhere
    micro = True
    best_metric = get_metric(info_dicts, metric, micro)
    # HACK: Attempt to tune thresholds simultaneously... for overall micro average
    if num_categories < 2:
        global_tweaks = 0
    if debug and global_tweaks > 0:
        print('best after invididual thresholds (micro %s)' % micro)
        print(best_thresholds)
        print(get_metric(info_dicts, metric, micro))
    for i in range(global_tweaks):
        # Choose random category
        category = np.random.randint(num_categories)
        curr_threshold = best_thresholds[category]
        # tweak randomly
        new_threshold = curr_threshold + (0.08 * (np.random.random() - 0.5))
        if heads_per_class > 1 and class_single_threshold:
            info_dict = update_info_dict(defaultdict(int), labels[:, (category * heads_per_class):(category+1)*heads_per_class], preds[:, (category * heads_per_class):(category+1)*heads_per_class], threshold=new_threshold)
        else:
            info_dict = update_info_dict(defaultdict(int), labels[:, category], preds[:, category], threshold=new_threshold)
        old_dict = info_dicts[category]
        info_dicts[category] = info_dict
        # compute *global* metrics
        metric_score = get_metric(info_dicts, metric, micro)
        # save new threshold if global metrics improve
        if metric_score > best_metric:
            # print('Better threshold %.3f for category %d' % (new_threshold, category))
            best_thresholds[category] = round(new_threshold, 3)
            best_metric = metric_score
        else:
            info_dicts[category] = old_dict
    if debug and global_tweaks > 0:
        print('final thresholds')
        print(best_thresholds)
        print(get_metric(info_dicts, metric, micro))

    # OK, now *if* we used multiple heads per class (same threshold) copy these back out to final answers
    if heads_per_class > 1 and class_single_threshold:
        best_thresholds = np.concatenate([[best_thresholds[i]]*heads_per_class for i in range(num_categories)])
    else:
        best_thresholds = np.array(best_thresholds) 
    # print(best_thresholds)
    return get_metric(info_dicts, metric, micro), best_thresholds, category_metrics, info_dicts

def get_auc(args):
    preds = pd.read_csv(args.preds_file, header=None).values
    labels = pd.read_csv(args.labels_file, header=None).values.astype(int)

    aucs = []
    for category in range(preds.shape[1]):
        fpr, tpr, thresholds = metrics.roc_curve(labels[:, category], preds[:, category], pos_label=1)
        aucs.append(metrics.auc(fpr, tpr))

    for idx, auc in enumerate(aucs):
        print('{}: {}\n'.format(idx, auc))


def neutral_threshold_scalar_output(args):
    preds = pd.read_csv(args.preds_file, header=None, names=['preds'])
    labels = pd.read_csv(args.labels_file, header=None, names=['labels'])
    assert preds.shape[1] == labels.shape[1] == 1, "Neutral thresholding only available for single category labels"

    labels['positive'] = labels['labels'].apply(lambda s: int(s == 1))
    labels['negative'] = labels['labels'].apply(lambda s: int(s == 0))
    labels['neutral'] = ((labels['positive'] == labels['negative']).sum() == 2).astype(int)
    labels_vals = labels[['positive', 'negative', 'neutral']].values

    best_pos = best_neg = best_acc = 0
    for pos, neg in tqdm(itertools.product(np.linspace(0.005, 1, 200), repeat=2), total=200 ** 2, unit='setting'):
        if neg > pos:
            continue
        new_df = pd.DataFrame()
        new_df['pos'] = preds['preds'].apply(lambda s: int(s > pos))
        new_df['neg'] = preds['preds'].apply(lambda s: int(s < neg))
        new_df['neutral'] = ((new_df['pos'] == new_df['neg']).sum() == 2).astype(int)

        new_df_vals = new_df.values
        acc = 0
        for new_row, label_row in zip(new_df_vals, labels_vals):
            acc += int((new_row == label_row).sum() == 3)
        acc /= float(labels.shape[0])
        if acc > best_acc:
            best_pos, best_neg, best_acc = pos, neg, acc

    print("Best acc:", best_acc, "Best pos:", best_pos, "Best neg:", best_neg)
    np.savetxt('best_neutral_thresholds.txt', np.array([best_pos, best_neg]))


def neutral_threshold_two_output(args):
    preds = pd.read_csv(args.preds_file, header=None, names=['positive', 'negative']) # ordered positive, negative
    labels = pd.read_csv(args.labels_file, header=None, names=['positive', 'negative'])
    labels['neutral'] = labels['positive'] == labels['negative']
    labels_vals = labels.values

    best_pos = best_neg = best_acc = 0
    for pos, neg in tqdm(itertools.product(np.linspace(0.005, 1, 200), repeat=2), total=200 ** 2, unit='setting'):
        new_df = pd.DataFrame()
        new_df['pos'] = preds['positive'].apply(lambda s: int(s > pos))
        new_df['neg'] = preds['negative'].apply(lambda s: int(s > neg))
        new_df['neutral'] = (new_df['pos'] == new_df['neg']).astype(int)

        new_df_vals = new_df.values
        acc = 0
        for new_row, label_row in zip(new_df_vals, labels_vals):
            if new_row[0] == new_row[1] == 1:
                new_row[0] = new_row[1] = 0
            acc += int((new_row == label_row).sum() == 3)
        acc /= float(labels.shape[0])
        if acc > best_acc:
            best_pos, best_neg, best_acc = pos, neg, acc

    print("Best acc:", best_acc, "Best pos:", best_pos, "Best neg:", best_neg)
    np.savetxt('best_neutral_thresholds.txt', np.array([best_pos, best_neg]))

def neutral_threshold_two_output(args):
    preds = pd.read_csv(args.preds_file, header=None, names=['positive', 'negative']) # ordered positive, negative
    labels = pd.read_csv(args.labels_file, header=None, names=['positive', 'negative'])

    best_acc, (best_pos, best_neg) = _neutral_threshold_two_output(preds.values, labels.values)

    print("Best acc:", best_acc, "Best pos:", best_pos, "Best neg:", best_neg)
    np.savetxt('best_neutral_thresholds.txt', np.array([best_pos, best_neg]))


def _neutral_threshold_two_output(preds, labels, threshold_granularity=30):
    neutral_labels = (labels[:,0] == labels[:,1]).astype(int).reshape(-1, 1)
    labels_vals = np.concatenate([labels[:,:2], neutral_labels], axis=1)

    best_0 = best_1 = best_acc = 0

    for t0, t1 in tqdm(itertools.product(np.linspace(0.005, 1, threshold_granularity), repeat=2), total=threshold_granularity ** 2, unit='setting'):
        t0, t1 = round(t0, 3), round(t1, 3)
        new_df = pd.DataFrame()
        new_df['0'] = (preds[:,0]>t0).astype(int)
        new_df['1'] = (preds[:,1]>t1).astype(int)
        new_df['neutral'] = (new_df['0'] == new_df['1']).astype(int)

        new_df_vals = new_df[['0','1','neutral']].values
        acc = 0
        for new_row, label_row in zip(new_df_vals, labels_vals):
            if new_row[0] == new_row[1] == 1:
                new_row[0] = new_row[1] = 0
            acc += int((new_row == label_row).sum() == 3)
        acc /= labels_vals.shape[0]
        if acc > best_acc:
            best_0, best_1, best_acc = t0, t1, acc

    return best_acc, (best_0, best_1)

def main():
    task_dict = {
        'auc' : get_auc,
        'binary' : binary_threshold,
        'neutral' : neutral_threshold_two_output,
        'scalar' : neutral_threshold_scalar_output,
    }
    parser = argparse.ArgumentParser("Tools for optimizing outputs through ROC/AUC analysis")
    parser.add_argument('--task', type=str, required=True, help='what do you want to do?')
    parser.add_argument('--preds-file', type=str, help='path to predictions file')
    parser.add_argument('--labels-file', type=str, help='path to labels file')
    parser.add_argument('--metric', type=str, default='f1', help='which metric to analyze/optimize')
    parser.add_argument('--micro', action='store_true', help='whether to micro-average metric')

    args = parser.parse_args()
    task_dict[args.task](args)


if __name__ == '__main__':
    main()

# PROJECT: NVIDIA_sentiment-discovery FILE: finetune_classifier.py
import argparse
import os
import sys
import time
import math
import random
import collections
import pandas as pd
import pickle as pkl
import json
import torch
import torch.nn as nn
from torch.autograd import Variable

import numpy as np
from logreg_utils import train_logreg

from fp16 import FP16_Module, FP16_Optimizer
from reparameterization import apply_weight_norm, remove_weight_norm

import model as M
from tqdm import tqdm
from model import DistributedDataParallel as DDP
from configure_data import configure_data
from learning_rates import AnnealingLR, SlantedTriangularLR, ConstantLR
from arguments import add_general_args, add_model_args, add_classifier_model_args, add_finetune_classifier_args
from metric_utils import update_info_dict, get_metric
from threshold import _binary_threshold, _neutral_threshold_two_output

def get_data_and_args():
    parser = argparse.ArgumentParser(description='PyTorch Sentiment Discovery Transfer Learning')
    parser = add_general_args(parser)
    parser = add_model_args(parser)
    parser = add_classifier_model_args(parser)
    data_config, data_parser, finetune_classifier_parser, parser = add_finetune_classifier_args(parser)

    args = parser.parse_args()
    args.cuda = torch.cuda.is_available()

    if args.seed is not -1:
        torch.manual_seed(args.seed)
        if args.cuda:
            torch.cuda.manual_seed(args.seed)

    (train_data, val_data, test_data), tokenizer = data_config.apply(args)
    args.data_size = tokenizer.num_tokens
    args.padding_idx = tokenizer.command_name_map['pad'].Id
    return (train_data, val_data, test_data), tokenizer, args

def get_model_and_optim(args, train_data):
    if args.use_softmax:
        args.report_no_thresholding = True
    ntokens = args.data_size
    concat_pools = args.concat_max, args.concat_min, args.concat_mean
    if args.model == 'transformer':
        model = M.SentimentClassifier(args.model, ntokens, None, None, None, args.classifier_hidden_layers, args.classifier_dropout,
                                      None, concat_pools, args.aux_lm_loss, args)
    else:
        model = M.SentimentClassifier(args.model, ntokens, args.emsize, args.nhid, args.nlayers,
                                      args.classifier_hidden_layers, args.classifier_dropout, args.all_layers, concat_pools, args.aux_lm_loss, args)
    if args.cuda:
        model.cuda()

    if args.fp16:
        model.half()
    # load char embedding and recurrent encoder for featurization
    if args.load is not None and args.load != '':
        with open(args.load, 'rb') as f:
            sd = x = torch.load(f, 'cpu')
            if 'sd' in sd:
                sd = sd['sd']

        if not args.load_finetuned:
            if 'lm_encoder' in sd:
                sd = sd['lm_encoder']
            try:
                model.lm_encoder.load_state_dict(sd)
            except:
                # if state dict has weight normalized parameters apply and remove weight norm to model while loading sd
                if hasattr(model.lm_encoder, 'rnn'):
                    apply_weight_norm(model.lm_encoder.rnn)
                else:
                    apply_weight_norm(model.lm_encoder)
                model.lm_encoder.load_state_dict(sd)
                remove_weight_norm(model)
        else:
            model.load_state_dict(sd)

    if args.thresh_test_preds:
        model.set_thresholds(pd.read_csv(args.thresh_test_preds, header=None).values.squeeze(), args.double_thresh, args.dual_thresh and not args.joint_binary_train)

    optims = {
        'adam' : 'Adam',
        'sgd'  : 'SGD'
    }

    optim = eval('torch.optim.'+ optims[args.optim.lower()])(model.parameters(), lr=args.lr)
    iters_per_epoch = len(train_data)
    num_iters = iters_per_epoch * args.epochs

    assert not (args.stlr_cut_frac and args.cos_cut_frac)
    if args.stlr_cut_frac is not None:
        LR = SlantedTriangularLR(optim, max_val=args.lr, cut_frac=args.stlr_cut_frac, num_iters=num_iters)
    elif args.cos_cut_frac is not None:
        LR = AnnealingLR(optim, start_lr=args.lr, warmup_iter=int(args.cos_cut_frac * num_iters), num_iters=num_iters, decay_style='cosine')
    elif args.decay_style is not None:
        warmup_iters = int(args.warmup_epochs * iters_per_epoch)
        if args.decay_epochs == -1:
            decay_iters = int(args.epochs * iters_per_epoch)
        else:
            decay_iters = int(args.decay_epochs * iters_per_epoch)
        if args.decay_style == 'constant':
            #TODO: implement
            LR = AnnealingLR(optim, start_lr=args.lr, warmup_iter=warmup_iters, num_iters=decay_iters+warmup_iters, decay_style=args.decay_style)
        elif args.decay_style == 'linear':
            #TODO: implement
            LR = AnnealingLR(optim, start_lr=args.lr, warmup_iter=warmup_iters, num_iters=decay_iters+warmup_iters, decay_style=args.decay_style)
        elif args.decay_style == 'cosine':
            LR = AnnealingLR(optim, start_lr=args.lr, warmup_iter=warmup_iters, num_iters=decay_iters+warmup_iters, decay_style=args.decay_style)
        elif args.decay_style == 'exponential':
            #TODO: implement
            LR = ConstantLR(optim, lr=args.lr)
        else:
            LR = ConstantLR(optim, lr=args.lr)
    else:
        LR = ConstantLR(optim, lr=args.lr)
    return model, optim, LR

def get_supervised_batch(batch, use_cuda, model, max_seq_len=None, args=None, save_outputs=False,  heads_per_class=1):
    '''
    Process batch and return tuple of (text, text label, text length) long tensors.
    Text is returned in column format with (time, batch) dimensions.
    '''
    text = batch['text'][0]
    timesteps = batch['length']
    labels = batch['label']
    text = Variable(text).long()
    timesteps = Variable(timesteps).long()
    labels = Variable(labels)
    if max_seq_len is not None:
        text = text[:, :max_seq_len]
        timesteps = torch.clamp(timesteps, max=args.max_seq_len)
    if args.use_softmax:
        labels = Variable(labels).view(-1).long()
    else:
        labels = labels.view(-1, int(model.out_dim/model.heads_per_class)).float()

    if use_cuda:
        text, timesteps, labels = text.cuda(), timesteps.cuda(), labels.cuda()
    return text.t(), labels, timesteps-1

def transform(model, text_batch, labels_batch, length_batch, args, LR=None):
    batch_size = text_batch.size(1)

    def get_outs():
        if args.model.lower() == 'transformer':
            class_out, (lm_or_encoder_out, state) = model(text_batch, length_batch, args.get_hidden)
        else:
            model.lm_encoder.rnn.reset_hidden(args.batch_size)
            for _ in range(1 + args.num_hidden_warmup):
                class_out, (lm_or_encoder_out, state) = model(text_batch, length_batch, args.get_hidden)
        # if args.heads_per_class > 1:
        #     class_out, mean_out, std_out = class_out
        # if args.use_softmax:
        #     class_out = torch.max(class_out,-1)[1].view(-1,1)
        # class_out = class_out.float()
        # if args.heads_per_class > 1:
        #     class_out = class_out, mean_out, std_out
        return class_out, (lm_or_encoder_out, state)

    if LR is not None and not args.use_logreg:
        # doing true finetuning
        class_out, lm_or_encoder_out = get_outs()
    else:
        with torch.no_grad():
            class_out, lm_or_encoder_out = get_outs()

    # class_out = class_out.float().view(-1, model.out_dim)
    return class_out, lm_or_encoder_out

def finetune(model, text, args, val_data=None, LR=None, reg_loss=None, tqdm_desc='nvidia', save_outputs=False,
    heads_per_class=1, default_threshold=0.5, last_thresholds=[], threshold_validation=True, debug=False):
    '''
    Apply featurization `model` to extract features from text in data loader.
    Featurization model should return cell state not hidden state.
    `text` data loader should return tuples of ((text, text length), text label)
    Returns labels and features for samples in text.
    '''
    # NOTE: If in training mode, do not run in .eval() mode. Bug fixed.
    if LR is None:
        model.lm_encoder.eval()
        model.classifier.eval()
    else:
        # Very important to reset back to train mode for future epochs!
        model.lm_encoder.train()
        model.classifier.train()

    # Optionally, freeze language model (train MLP only)
    # NOTE: un-freeze gradients if they every need to be tweaked in future iterations
    if args.freeze_lm:
        for param in model.lm_encoder.parameters():
            param.requires_grad = False

    # Choose which losses to implement
    if args.use_softmax:
        if heads_per_class > 1:
            clf_loss_fn = M.MultiHeadCrossEntropyLoss(heads_per_class=heads_per_class)
        else:
            clf_loss_fn = torch.nn.CrossEntropyLoss()
    else:
        if heads_per_class > 1:
            clf_loss_fn = M.MultiHeadBCELoss(heads_per_class=heads_per_class)
        else:
            clf_loss_fn = torch.nn.BCELoss()
    if args.aux_lm_loss:
        aux_loss_fn = torch.nn.CrossEntropyLoss(reduce=False)
    else:
        aux_loss_fn = None

    if args.thresh_test_preds:
        thresholds = model.get_thresholds()
    elif len(last_thresholds) > 0:
        # Re-use previous thresholds, if provided.
        # Why? More accurate reporting, and not that slow. Don't compute thresholds on training, for example -- but can recycle val threshold
        thresholds = last_thresholds
    else:
        # Default thresholds -- faster, but less accurate
        thresholds = np.array([default_threshold for _ in range(int(model.out_dim/heads_per_class))])

    total_loss = 0
    total_classifier_loss = 0
    total_lm_loss = 0
    total_multihead_variance_loss = 0
    class_accuracies = torch.zeros(model.out_dim).cuda()
    if model.out_dim/heads_per_class > 1 and not args.use_softmax:
        keys = list(args.non_binary_cols)
    elif args.use_softmax:
        keys = [str(m) for m in range(model.out_dim)]
    else:
        keys = ['']
    info_dicts = [{'fp' : 0, 'tp' : 0, 'fn' : 0, 'tn' : 0, 'std' : 0,
                   'metric' : args.report_metric, 'micro' : args.micro} for k in keys]

    # Sanity check -- should do this sooner. Does #classes match expected output?
    assert model.out_dim == len(keys) * heads_per_class, "model.out_dim does not match keys (%s) x heads_per_class (%d)" % (keys, heads_per_class)

    batch_adjustment = 1. / len(text)
    # Save all outputs *IF* small enough, and requested for thresholding -- basically, on validation
    #if threshold_validation and LR is not None:
    all_batches = []
    all_stds = []
    all_labels = []
    for i, data in tqdm(enumerate(text), total=len(text), unit="batch", desc=tqdm_desc, position=1, ncols=100):
        text_batch, labels_batch, length_batch = get_supervised_batch(data, args.cuda, model, args.max_seq_len, args, heads_per_class=args.heads_per_class)
        class_out, (lm_out, _) = transform(model, text_batch, labels_batch, length_batch, args, LR)
        class_std = None
        if heads_per_class > 1:
            all_heads, class_out, class_std, clf_out = class_out
            classifier_loss = clf_loss_fn(all_heads, labels_batch)
        else:
            class_out, clf_out = class_out
            if args.dual_thresh:
                class_out = class_out[:, :-1]
            classifier_loss = clf_loss_fn(class_out, labels_batch)
            if args.use_softmax:
                class_out = F.softmax(class_out, -1)

        loss = classifier_loss
        classifier_loss = classifier_loss.clone() # save for reporting
        # Also compute multihead variance loss -- from classifier [divide by output size since it scales linearly]
        if args.aux_head_variance_loss_weight > 0.:
            multihead_variance_loss = model.classifier.get_last_layer_variance() / model.out_dim
            loss = loss + multihead_variance_loss * args.aux_head_variance_loss_weight
        # Divide by # batches? Since we're looking at the parameters here, and should be batch independent.
        # multihead_variance_loss *= batch_adjustment

        if args.aux_lm_loss:
            lm_labels = text_batch[1:]
            lm_losses = aux_loss_fn(lm_out[:-1].view(-1, lm_out.size(2)).contiguous().float(),
                                      lm_labels.contiguous().view(-1))

            padding_mask = (torch.arange(lm_labels.size(0)).unsqueeze(1).cuda() > length_batch).float()
            portion_unpadded = padding_mask.sum() / padding_mask.size(0)
            lm_loss = portion_unpadded * torch.mean(lm_losses * (padding_mask.view(-1).float()))

            # Scale LM loss -- since it's so big
            if args.aux_lm_loss_weight > 0.:
                loss = loss + lm_loss * args.aux_lm_loss_weight

        # Training
        if LR is not None:
            LR.optimizer.zero_grad()
            loss.backward()
            LR.optimizer.step()
            LR.step()

        # Remove loss from CUDA -- kill gradients and save memory.
        total_loss += loss.detach().cpu().numpy()
        if args.use_softmax:
            labels_batch = onehot(labels_batch.squeeze(), model.out_dim)
            class_out = onehot(clf_out.view(-1), int(model.out_dim/heads_per_class))
        total_classifier_loss += classifier_loss.detach().cpu().numpy()
        if args.aux_lm_loss:
            total_lm_loss += lm_loss.detach().cpu().numpy()
        if args.aux_head_variance_loss_weight > 0:
            total_multihead_variance_loss += multihead_variance_loss.detach().cpu().numpy()
        for j in range(int(model.out_dim/heads_per_class)):
            std = None
            if class_std is not None:
                std = class_std[:,j]
            info_dicts[j] = update_info_dict(info_dicts[j], labels_batch[:, j], class_out[:, j], thresholds[j], std=std)
        # Save, for overall thresholding (not on training)
        if threshold_validation and LR is None:
            all_labels.append(labels_batch.detach().cpu().numpy())
            all_batches.append(class_out.detach().cpu().numpy())
            if class_std is not None:
                all_stds.append(class_std.detach().cpu().numpy())

    if threshold_validation and LR is None:
        all_batches = np.concatenate(all_batches)
        all_labels = np.concatenate(all_labels)
        if heads_per_class > 1:
            all_stds = np.concatenate(all_stds)
        # Compute new thresholds -- per class
        _, thresholds, _, _ =  _binary_threshold(all_batches, all_labels, args.threshold_metric, args.micro, global_tweaks=args.global_tweaks)
        info_dicts = [{'fp' : 0, 'tp' : 0, 'fn' : 0, 'tn' : 0, 'std' : 0.,
               'metric' : args.report_metric, 'micro' : args.micro} for k in keys]
        # In multihead case, look at class averages? Why? More predictive. Works especially well when we force single per-class threshold.
        for j in range(int(model.out_dim/heads_per_class)):
            std = None
            if heads_per_class > 1:
                std = all_stds[:, j]
            info_dicts[j] = update_info_dict(info_dicts[j], all_labels[:, j], all_batches[:, j], thresholds[j], std=std)

    # Metrics for all items -- with current best thresholds
    total_metrics, class_metric_strs = get_metric_report(info_dicts, args, keys, LR)

    # Show losses
    if debug:
        tqdm.write('losses -- total / classifier / LM / multihead_variance')
        tqdm.write(total_loss * batch_adjustment)
        tqdm.write(total_classifier_loss * batch_adjustment)
        tqdm.write(total_lm_loss * batch_adjustment)
        tqdm.write(total_multihead_variance_loss * batch_adjustment)

    return total_loss.item() / (i + 1), total_metrics, class_metric_strs, thresholds

def onehot(sparse, nclasses):
    rows = len(sparse)
    rtn = torch.zeros(rows, math.floor(nclasses))
    rtn[torch.arange(rows), sparse.squeeze().cpu()] = 1
    return rtn

def get_metric_report(info_dicts, args, keys=['-'], LR=None):
    class_metric_strs, total_metrics = [], []
    report_metrics = ['jacc', 'acc', 'mcc', 'f1', 'recall', 'precision', 'var'] if args.all_metrics else [args.report_metric]
    for m in report_metrics:
        for d in info_dicts:
            d.update({'metric' : m})
        class_metrics = [get_metric(d) for d in info_dicts]
        total_metrics.append(get_metric(info_dicts))

        if LR is not None:
            delim = '-'
        else:
            delim = {'mcc' : '#', 'f1' : '+', 'jacc' : '=', 'acc' : '>', 'var' : '%', 'recall': '<', 'precision':'~'}[m]
        class_metric_strs.append(", ".join('{} {} {:5.2f}'.format(k, delim, f * 100) for k, f in zip(keys, class_metrics)))

    return total_metrics, class_metric_strs

def generate_outputs(model, text, args, thresholds=None, debug=False):
    model.eval()
    collected_outputs = []
    collected_labels = []
    # Collect category standard deviations, across multiple heads
    collected_outputs_std = []

    for i, data in tqdm(enumerate(text), total=len(text), unit='batch', desc='predictions', position=1, ncols=100):
        text_batch, labels_batch, length_batch = get_supervised_batch(data, args.cuda, model, args.max_seq_len, args, save_outputs=True, heads_per_class=args.heads_per_class)
        class_out, (lm_out, _) = transform(model, text_batch, labels_batch, length_batch, args)
        # Take the average per-category if requested
        if args.heads_per_class > 1:
            _, class_out, class_std, clf_out = class_out
        else:
            class_out, clf_out = class_out
            if args.use_softmax:
                class_out = F.softmax(class_out, -1)
            class_std = torch.zeros(class_out.shape)

        if args.thresh_test_preds or thresholds is not None:
            class_out = clf_out

        if args.use_softmax:
            labels_batch = onehot(labels_batch.squeeze(), int(model.out_dim/args.heads_per_class)).cuda()
            class_out = onehot(torch.max(clf_out, -1)[1].squeeze(), int(model.out_dim/args.heads_per_class))

        collected_outputs.append(torch.tensor(class_out).cuda().float())
        collected_labels.append(labels_batch)
        collected_outputs_std.append(torch.tensor(class_std).cuda().float())

    collected_outputs = torch.cat(collected_outputs, 0)
    collected_outputs_std = torch.cat(collected_outputs_std, 0)
    collected_labels = torch.cat(collected_labels, 0)

    return collected_outputs, collected_labels, collected_outputs_std

def write_results(preds, labels, save):
    labels_file = os.path.splitext(save)[0] + '_labels.txt'
    # HACK -- handle both tensors and numpy arrays here:
    if isinstance(preds, np.ndarray):
        np.savetxt(save, preds.astype(int), delimiter=',')
        np.savetxt(labels_file, labels.astype(int), delimiter=',')
    else:
        np.savetxt(save, preds.cpu().numpy().astype(int), delimiter=',')
        np.savetxt(labels_file, labels.cpu().numpy().astype(int), delimiter=',')

def main():
    (train_data, val_data, test_data), tokenizer, args = get_data_and_args()
    # Print args for logging & reproduction. Need to know, including default args
    if test_data is None:
        test_data = val_data
    model, optim, LR = get_model_and_optim(args, train_data)

    # save_root = '' if args.load is None else args.load
    # save_root = save_root.replace('.current', '')
    # save_root = os.path.splitext(save_root)[0]
    # save_root += '_transfer'
    save_root = os.path.join('', args.model_version_name)
    if not os.path.exists(save_root):
        os.makedirs(save_root)
    print('writing results to '+save_root)

    def clf_reg_loss(reg_penalty=.125, order=1):
        loss = 0
        for p in model.classifier.parameters():
            loss += torch.abs(p).sum()*reg_penalty
        return loss
    reg_loss = clf_reg_loss
    init_params = list(model.lm_encoder.parameters())

    if args.use_logreg:
        def transform_for_logreg(model, data, args, desc='train'):
            if data is None:
                return None, None

            X_out = []
            Y_out = []
            for i, batch in tqdm(enumerate(data), total=len(data), unit="batch", desc=desc, position=0, ncols=100):
                text_batch, labels_batch, length_batch = get_supervised_batch(batch, args.cuda, model, args.max_seq_len, args, heads_per_class=args.heads_per_class)
                # if args.non_binary_cols:
                #     labels_batch = labels_batch[:,0]-labels_batch[:,1]+1
                _, (_, state) = transform(model, text_batch, labels_batch, length_batch, args)
                X_out.append(state.cpu().numpy())
                Y_out.append(labels_batch.cpu().numpy())
            X_out = np.concatenate(X_out)
            Y_out = np.concatenate(Y_out)
            return X_out, Y_out

        model.eval()
        trX, trY = transform_for_logreg(model, train_data, args, desc='train')
        vaX, vaY = transform_for_logreg(model, val_data, args, desc='val')
        teX, teY = transform_for_logreg(model, test_data, args, desc='test')

        logreg_model, logreg_scores, logreg_preds, c, nnotzero = train_logreg(trX, trY, vaX, vaY, teX, teY, eval_test=not args.no_test_eval, 
                                                                              report_metric=args.report_metric, threshold_metric=args.threshold_metric,
                                                                              automatic_thresholding=args.automatic_thresholding, micro=args.micro)
        print(', '.join([str(score) for score in logreg_scores]), 'train, val, test accuracy for all neuron regression')
        print(str(c)+' regularization coefficient used')
        print(str(nnotzero) + ' features used in all neuron regression\n')
    else:
        best_vaY = 0
        vaT = [] # Current "best thresholds" so we can get reasonable estimates on training set
        for e in tqdm(range(args.epochs), unit="epoch", desc="epochs", position=0, ncols=100):
            if args.use_softmax:
                vaT = []
            save_outputs = False
            report_metrics = ['jacc', 'acc','mcc', 'f1', 'recall', 'precision', 'var'] if args.all_metrics else [args.report_metric]
            print_str = ""
            trXt, trY, trC, _ = finetune(model, train_data, args, val_data=val_data, LR=LR, reg_loss=reg_loss, tqdm_desc='train', heads_per_class=args.heads_per_class, last_thresholds=vaT, threshold_validation=False)
            data_str_base = "Train Loss: {:4.2f} Train {:5s} (All): {:5.2f}, Train Class {:5s}: {}"
            for idx, m in enumerate(report_metrics):
                data_str = data_str_base.format(trXt, m, trY[idx] * 100, m, trC[idx])
                print_str += data_str + " " * max(0, 110 - len(data_str)) + "\n"

            vaXt, vaY = None, None
            if val_data is not None:
                vaXt, vaY, vaC, vaT = finetune(model, val_data, args, tqdm_desc='val', heads_per_class=args.heads_per_class, last_thresholds=vaT)
                # Take command line, for metric for which to measure best performance against.
                # NOTE: F1, MCC, Jaccard are good measures. Accuracy is not -- since so skewed.
                selection_metric = ['jacc', 'acc','mcc', 'f1', 'recall', 'precision', 'var'].index(args.threshold_metric)
                avg_Y = vaY[selection_metric]
                tqdm.write('avg '+args.threshold_metric+' metric '+str(avg_Y))
                if avg_Y > best_vaY:
                    save_outputs = True
                    best_vaY = avg_Y
                elif avg_Y == best_vaY and random.random() > 0.5:
                    save_outputs = True
                    best_vaY = avg_Y
                data_str_base = "Val   Loss: {:4.2f} Val   {:5s} (All): {:5.2f}, Val   Class {:5s}: {}"
                for idx, m in enumerate(report_metrics):
                    data_str = data_str_base.format(vaXt, m, vaY[idx] * 100, m, vaC[idx])
                    print_str += data_str + " " * max(0, 110 - len(data_str)) + "\n"
            tqdm.write(print_str[:-1])
            teXt, teY = None, None
            if test_data is not None:
                # Hardcode -- enable to always save outputs [regardless of metrics]
                # save_outputs = True
                if save_outputs:
                    tqdm.write('performing test eval')
                    try:
                        with torch.no_grad():
                            if not args.no_test_eval:
                                auto_thresholds = None
                                dual_thresholds = None
                                # NOTE -- we manually threshold to F1 [not necessarily good]
                                V_pred, V_label, V_std = generate_outputs(model, val_data, args)
                                if args.automatic_thresholding:
                                    if args.dual_thresh:
                                        # get dual threshold (do not call auto thresholds)
                                        # TODO: Handle multiple heads per class
                                        _, dual_thresholds = _neutral_threshold_two_output(V_pred.cpu().numpy(), V_label.cpu().numpy())
                                        model.set_thresholds(dual_thresholds, dual_threshold=args.dual_thresh and not args.joint_binary_train)
                                    else:
                                        # Use args.threshold_metric to choose which category to threshold on. F1 and Jaccard are good options
                                        # NOTE: For multiple heads per class, can threshold each head (default) or single threshold. Little difference once model converges.
                                        auto_thresholds = vaT
                                        # _, auto_thresholds, _, _ = _binary_threshold(V_pred.view(-1, int(model.out_dim/args.heads_per_class)).contiguous(), V_label.view(-1, int(model.out_dim/args.heads_per_class)).contiguous(),
                                        #     args.threshold_metric, args.micro, global_tweaks=args.global_tweaks)
                                        model.set_thresholds(auto_thresholds, args.double_thresh)
                                T_pred, T_label, T_std = generate_outputs(model, test_data, args, auto_thresholds)
                                if not args.use_softmax and int(model.out_dim/args.heads_per_class) > 1:
                                    keys = list(args.non_binary_cols)
                                    if args.dual_thresh:
                                        if len(keys) == len(dual_thresholds):
                                            tqdm.write('Dual thresholds: %s' % str(list(zip(keys, dual_thresholds))))
                                        keys += ['neutral']
                                    else:
                                        tqdm.write('Class thresholds: %s' % str(list(zip(keys, auto_thresholds))))
                                elif args.use_softmax:
                                    keys = [str(m) for m in range(model.out_dim)]
                                else:
                                    tqdm.write('Class threshold: %s' % str([args.label_key, auto_thresholds[0]]))
                                    keys = ['']
                                info_dicts = [{'fp' : 0, 'tp' : 0, 'fn' : 0, 'tn' : 0, 'std' : 0.,
                                               'metric' : args.report_metric, 'micro' : True} for k in keys]
                                #perform dual threshold here, adding the neutral labels to T_label, thresholding existing predictions and adding neutral preds to T_Pred
                                if args.dual_thresh:
                                    if dual_thresholds is None:
                                        dual_thresholds = [.5, .5]
                                    def make_onehot_w_neutral(label):
                                        rtn = [0]*3
                                        rtn[label] = 1
                                        return rtn
                                    def get_label(pos_neg):
                                        thresholded = [pos_neg[0]>=dual_thresholds[0], pos_neg[1]>=dual_thresholds[1]]
                                        if thresholded[0] == thresholded[1]:
                                            return 2
                                        return thresholded.index(1)
                                    def get_new_std(std):
                                        return std[0], std[1], (std[0]+std[1])/2
                                    new_labels = []
                                    new_preds = []
                                    T_std = torch.cat([T_std[:,:2], T_std[:,:2].mean(-1).view(-1, 1)], -1).cpu().numpy()
                                    for j, lab in  enumerate(T_label):
                                        pred = T_pred[j]
                                        new_preds.append(make_onehot_w_neutral(get_label(pred)))
                                        new_labels.append(make_onehot_w_neutral(get_label(lab)))
                                    T_pred = np.array(new_preds)
                                    T_label = np.array(new_labels)

                                # HACK: If dual threshold, hardcoded -- assume positive, negative and neutral -- in that order
                                # It's ok to train with other categories (after positive, neutral) as auxilary loss -- but won't calculate in test
                                if args.dual_thresh and args.joint_binary_train:
                                    keys = ['positive', 'negative', 'neutral']
                                    info_dicts = [{'fp' : 0, 'tp' : 0, 'fn' : 0, 'tn' : 0, 'std' : 0.,
                                                   'metric' : args.report_metric, 'micro' : True} for k in keys]
                                for j, k in enumerate(keys):
                                    update_info_dict(info_dicts[j], T_pred[:,j], T_label[:,j], std=T_std[:,j])
                                total_metrics, metric_strings = get_metric_report(info_dicts, args, keys)
                                test_str = ''
                                test_str_base = "Test  {:5s} (micro): {:5.2f}, Test  Class {:5s}: {}"
                                for idx, m in enumerate(report_metrics):
                                    data_str = test_str_base.format(m, total_metrics[idx] * 100, m, metric_strings[idx])
                                    test_str += data_str + " " * max(0, 110 - len(data_str)) + "\n"
                                tqdm.write(test_str[:-1])
                                # tqdm.write(str(total_metrics))
                                # tqdm.write('; '.join(metric_strings))
                            else:
                                V_pred, V_label, V_std = generate_outputs(model, val_data, args)

                                T_pred, T_label, T_std = generate_outputs(model, test_data, args)
                            val_path = os.path.join(save_root, 'val_results.txt')
                            tqdm.write('Saving validation prediction results of size %s to %s' % (str(T_pred.shape[:]), val_path))
                            write_results(V_pred, V_label, val_path)

                            test_path = os.path.join(save_root, 'test_results.txt')
                            tqdm.write('Saving test prediction results of size %s to %s' % (str(T_pred.shape[:]), test_path))
                            write_results(T_pred, T_label, test_path)
                    except KeyboardInterrupt:
                        pass
                else:
                    pass
            # Save the model, upon request
            if args.save_finetune and save_outputs:
                # Save model if best so far. Note epoch number, and also keys [what is it predicting], as well as optional version number
                # TODO: Add key string to handle multiple runs?
                if args.non_binary_cols:
                    keys = list(args.non_binary_cols)
                else:
                    keys = [args.label_key]
                # Also save args
                args_save_path = os.path.join(save_root, 'args.txt')
                tqdm.write('Saving commandline to %s' % args_save_path)
                with open(args_save_path, 'w') as f:
                    f.write(' '.join(sys.argv[1:]))
                # Save and add thresholds to arguments for easy reloading of model config
                if not args.no_test_eval and args.automatic_thresholding:
                    thresh_save_path = os.path.join(save_root, 'thresh'+'_ep'+str(e)+'.npy')
                    tqdm.write('Saving thresh to %s' % thresh_save_path)
                    if args.dual_thresh:
                        np.save(thresh_save_path, list(zip(keys, dual_thresholds)))
                        args.thresholds = list(zip(keys, dual_thresholds))
                    else:
                        np.save(thresh_save_path, list(zip(keys, auto_thresholds)))
                        args.thresholds = list(zip(keys, auto_thresholds))
                else:
                    args.thresholds = None
                args.classes = keys
                #save full model with args to restore
                clf_save_path = os.path.join(save_root, 'model'+'_ep'+str(e)+'.clf')
                tqdm.write('Saving full classifier to %s' % clf_save_path)
                torch.save({'sd': model.state_dict(), 'args': args}, clf_save_path)


if __name__ == "__main__":
    main()


# python3 finetune.py --data csvs/SemEval-7k-processed-IDs.train.csv --valid csvs/SemEval-7k-processed-IDs.val.csv --test csvs/SemEval-7k-processed-IDs.test.csv --epochs 5 --text_key 32k-ids --ids --optim adam --data_size 32000 --aux-lm-loss --label_key label --all-metrics --automatic-thresholding --batch_size 24 --lr 1.73e-5 --model transformer --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-layers 12  --load /home/adlr-sent.cosmos433/chkpts/tf-768emb-3072ffn-12x8head-learnedpos-32000parts-2cos-300/e170000.pt --decoder-learned-pos --use-final-embed --classifier-hidden-layers 8 --non-binary-cols csvs/cols/plutchik-cols.json  --save-finetune

# PROJECT: NVIDIA_sentiment-discovery FILE: transfer.py
import argparse
import os
import time
import math
import collections
import pickle as pkl
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.autograd import Variable

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import numpy as np
from logreg_utils import train_logreg

from fp16 import FP16_Module, FP16_Optimizer
from reparameterization import apply_weight_norm, remove_weight_norm

from model import RNNFeaturizer, TransformerFeaturizer
from configure_data import configure_data
from arguments import add_general_args, add_model_args, add_classifier_model_args, add_sentiment_transfer_args

def get_data_and_args():
    parser = argparse.ArgumentParser(description='PyTorch Sentiment Discovery Transfer Learning')

    parser = add_general_args(parser)
    parser = add_model_args(parser)
    parser = add_classifier_model_args(parser)
    data_config, data_parser, sentiment_transfer_parser, parser = add_sentiment_transfer_args(parser)
    args = parser.parse_args()

    args.cuda = torch.cuda.is_available()

    if args.seed is not -1:
        torch.manual_seed(args.seed)
        if args.cuda:
            torch.cuda.manual_seed(args.seed)

    (train_data, val_data, test_data), tokenizer = data_config.apply(args)
    args.data_size = tokenizer.num_tokens
    args.padding_idx = tokenizer.command_name_map['pad'].Id
    return (train_data, val_data, test_data), tokenizer, args

def get_model(args):
    ntokens = args.data_size
    concat_pools = [args.concat_max, args.concat_min, args.concat_mean]
    if args.model.lower() == 'transformer':
        model = TransformerFeaturizer(False, args)
    else:
        model = RNNFeaturizer(args.model, ntokens, args.emsize, args.nhid, args.nlayers,
                                          0.0, args.all_layers, concat_pools, residuals=args.residuals)
    if args.cuda:
        model.cuda()

    if args.fp16:
        model.half()

    if args.load is not None and args.load != '':
        # load char embedding and recurrent encoder for featurization
        with open(args.load, 'rb') as f:
            sd = x = torch.load(f)
            if 'sd' in sd:
                sd = sd['sd']
            if 'lm_encoder' in sd:
                sd = sd['lm_encoder']
        try:
            model.load_state_dict(sd)
        except:
            # if state dict has weight normalized parameters apply and remove weight norm to model while loading sd
            if hasattr(model, 'rnn'):
                apply_weight_norm(model.rnn)
            else:
                apply_weight_norm(model)
            model.load_state_dict(sd)
            remove_weight_norm(model)

    return model

def transform(model, text, args):
    '''
    Apply featurization `model` to extract features from text in data loader.
    Featurization model should return cell state not hidden state.
    `text` data loader should return tuples of ((text, text length), text label)
    Returns labels and features for samples in text.
    '''
    model.eval()
    features = np.array([])
    labels = np.array([])
    first_feature = True

    def get_batch(batch):
        '''
        Process batch and return tuple of (text, text label, text length) long tensors.
        Text is returned in column format with (time, batch) dimensions.
        '''
        text = batch['text'][0]
        timesteps = batch['length']
        labels = batch['label']
        text = Variable(text).long()
        timesteps = Variable(timesteps).long()
        labels = Variable(labels).long()
        if args.cuda:
            text, timesteps, labels = text.cuda(), timesteps.cuda(), labels.cuda()
        return text.t(), labels, timesteps-1

    def get_outs(text_batch, length_batch):
        if args.model.lower() == 'transformer':
            cell_out, lm_or_encoder_out = model(text_batch, length_batch, args.get_hidden)
        else:
            model.rnn.reset_hidden(args.batch_size)
            for _ in range(1 + args.num_hidden_warmup):
                cell_out, lm_or_encoder_out = model(text_batch, length_batch, args.get_hidden)
        return cell_out, lm_or_encoder_out

    tstart = start = time.time()
    n = 0
    len_ds = len(text)
    # Use no grad context for improving memory footprint/speed of inference
    with torch.no_grad():
        for i, data in tqdm(enumerate(text), total=len_ds, unit="batch", desc="transform", position=1,  ncols=100):
            text_batch, labels_batch, length_batch = get_batch(data)
            # get batch size and reset hidden state with appropriate batch size
            batch_size = text_batch.size(1)
            n += batch_size
            # extract batch of features from text batch
            cell, _ = get_outs(text_batch, length_batch)
            cell = cell.float()
            if first_feature:
                features = []
                first_feature = False
                labels = []
            labels.append(labels_batch.data.cpu().numpy())
            features.append(cell.data.cpu().numpy())

    if not first_feature:
        features = (np.concatenate(features))
        labels = (np.concatenate(labels))

    print('%0.3f seconds to transform %d examples' %
                  (time.time() - tstart, n))
    return features, labels

def score_and_predict(model, X, Y):
    '''
    Given a binary classification model, predict output classification for numpy features `X`
    and evaluate accuracy against labels `Y`. Labels should be numpy array of 0s and 1s.
    Returns (accuracy, numpy array of classification probabilities)
    '''
    probs = model.predict_proba(X)[:, 1]
    clf = probs > .5
    accuracy = (np.squeeze(Y) == np.squeeze(clf)).mean()
    return accuracy, probs

def get_top_k_neuron_weights(model, k=1):
    """
    Get's the indices of the top weights based on the l1 norm contributions of the weights
    based off of https://rakeshchada.github.io/Sentiment-Neuron.html interpretation of
    https://arxiv.org/pdf/1704.01444.pdf (Radford et. al)
    Args:
        weights: numpy arraylike of shape `[d,num_classes]`
        k: integer specifying how many rows of weights to select
    Returns:
        k_indices: numpy arraylike of shape `[k]` specifying indices of the top k rows
    """
    weights = model.coef_.T
    weight_penalties = np.squeeze(np.linalg.norm(weights, ord=1, axis=1))
    if k == 1:
        k_indices = np.array([np.argmax(weight_penalties)])
    elif k >= np.log(len(weight_penalties)):
        # runs O(nlogn)
        k_indices = np.argsort(weight_penalties)[-k:][::-1]
    else:
        # runs O(n+klogk)
        k_indices = np.argpartition(weight_penalties, -k)[-k:]
        k_indices = (k_indices[np.argsort(weight_penalties[k_indices])])[::-1]
    return k_indices

def plot_logits(save_root, X, Y_pred, top_neurons):
    """plot logits and save to appropriate experiment directory"""
    save_root = os.path.join(save_root,'logit_vis')
    if not os.path.exists(save_root):
        os.makedirs(save_root)

    print('plotting_logits at', save_root)

    for i, n in enumerate(top_neurons):
        plot_logit_and_save(trXt, trY, n, os.path.join(save_root, str(i)+'_'+str(n)))


def plot_logit_and_save(logits, labels, logit_index, name):
    """
    Plots histogram (wrt to what label it is) of logit corresponding to logit_index.
    Saves plotted histogram to name.

    Args:
        logits:
        labels:
        logit_index:
        name:
"""
    logit = logits[:,logit_index]
    plt.title('Distribution of Logit Values')
    plt.ylabel('# of logits per bin')
    plt.xlabel('Logit Value')
    plt.hist(logit[labels < .5], bins=25, alpha=0.5, label='neg')
    plt.hist(logit[labels >= .5], bins=25, alpha=0.5, label='pos')
    plt.legend()
    plt.savefig(name+'.png')
    plt.clf()

def plot_weight_contribs_and_save(coef, name):
    plt.title('Values of Resulting L1 Penalized Weights')
    plt.tick_params(axis='both', which='major')
    coef = normalize(coef)
    plt.plot(range(len(coef[0])), coef.T)
    plt.xlabel('Neuron (Feature) Index')
    plt.ylabel('Neuron (Feature) weight')
    print('saving weight visualization to', name)
    plt.savefig(name)
    plt.clf()

def normalize(coef):
    norm = np.linalg.norm(coef)
    coef = coef/norm
    return coef

def main():
    (train_data, val_data, test_data), tokenizer, args = get_data_and_args()
    model = get_model(args)

    save_root = '' if args.load is None else args.load
    save_root = save_root.replace('.current', '')
    save_root = os.path.splitext(save_root)[0]
    save_root += '_transfer'
    save_root = os.path.join(save_root, args.save_results)
    if not os.path.exists(save_root):
        os.makedirs(save_root)
    print('writing results to '+save_root)

    # featurize train, val, test or use previously cached features if possible
    print('transforming train')
    if not (os.path.exists(os.path.join(save_root, 'trXt.npy')) and args.use_cached):
        trXt, trY = transform(model, train_data, args)
        np.save(os.path.join(save_root, 'trXt'), trXt)
        np.save(os.path.join(save_root, 'trY'), trY)
    else:
        trXt = np.load(os.path.join(save_root, 'trXt.npy'))
        trY = np.load(os.path.join(save_root, 'trY.npy'))
    vaXt, vaY = None, None
    if val_data is not None:
        print('transforming validation')
        if not (os.path.exists(os.path.join(save_root, 'vaXt.npy')) and args.use_cached):
            vaXt, vaY = transform(model, val_data, args)
            np.save(os.path.join(save_root, 'vaXt'), vaXt)
            np.save(os.path.join(save_root, 'vaY'), vaY)
        else:
            vaXt = np.load(os.path.join(save_root, 'vaXt.npy'))
            vaY = np.load(os.path.join(save_root, 'vaY.npy'))
    teXt, teY = None, None
    if test_data is not None:
        print('transforming test')
        if not (os.path.exists(os.path.join(save_root, 'teXt.npy')) and args.use_cached):
            teXt, teY = transform(model, test_data, args)
            np.save(os.path.join(save_root, 'teXt'), teXt)
            np.save(os.path.join(save_root, 'teY'), teY)
        else:
            teXt = np.load(os.path.join(save_root, 'teXt.npy'))
            teY = np.load(os.path.join(save_root, 'teY.npy'))

    # train logistic regression model of featurized text against labels
    start = time.time()
    metric = 'mcc' if args.mcc else 'acc'
    logreg_model, logreg_scores, logreg_probs, c, nnotzero = train_logreg(trXt, trY, vaXt, vaY, teXt, teY, max_iter=args.epochs, eval_test=not args.no_test_eval,
                                                                          seed=args.seed, report_metric=metric, threshold_metric=metric)
    end = time.time()
    elapsed_time = end - start

    with open(os.path.join(save_root, 'all_neurons_score.txt'), 'w') as f:
        f.write(str(logreg_scores))
    with open(os.path.join(save_root, 'all_neurons_probs.pkl'), 'wb') as f:
        pkl.dump(logreg_probs, f)
    with open(os.path.join(save_root, 'neurons.pkl'), 'wb') as f:
        pkl.dump(logreg_model.coef_, f)

    print('all neuron regression took %s seconds'%(str(elapsed_time)))
    print(', '.join([str(score) for score in logreg_scores]), 'train, val, test accuracy for all neuron regression')
    print(str(c)+' regularization coefficient used')
    print(str(nnotzero) + ' features used in all neuron regression\n')

    # save a sentiment classification pytorch model
    sd = {}
    if not args.fp16:
        clf_sd = {'weight': torch.from_numpy(logreg_model.coef_).float(), 'bias': torch.from_numpy(logreg_model.intercept_).float()}
    else:
        clf_sd = {'weight': torch.from_numpy(logreg_model.coef_).half(), 'bias': torch.from_numpy(logreg_model.intercept_).half()}
    sd['classifier'] = clf_sd
    model.float().cpu()
    sd['lm_encoder'] = model.state_dict()
    with open(os.path.join(save_root, 'classifier.pt'), 'wb') as f:
        torch.save(sd, f)
    model.half()
    sd['lm_encoder'] = model.state_dict()
    with open(os.path.join(save_root, 'classifier.pt.16'), 'wb') as f:
        torch.save(sd, f)

    # extract sentiment neuron indices
    sentiment_neurons = get_top_k_neuron_weights(logreg_model, args.neurons)
    print('using neuron(s) %s as features for regression'%(', '.join([str(neuron) for neuron in list(sentiment_neurons.reshape(-1))])))

    # train logistic regression model of features corresponding to sentiment neuron indices against labels
    start = time.time()
    logreg_neuron_model, logreg_neuron_scores, logreg_neuron_probs, neuron_c, neuron_nnotzero = train_logreg(trXt, trY, vaXt, vaY, teXt, teY, max_iter=args.epochs, eval_test=not args.no_test_eval,
                                                                                                             seed=args.seed, neurons=sentiment_neurons, drop_neurons=args.drop_neurons,
                                                                                                             report_metric=metric, threshold_metric=metric)
    end = time.time()

    if args.drop_neurons:
        with open(os.path.join(save_root, 'dropped_neurons_score.txt'), 'w') as f:
            f.write(str(logreg_neuron_scores))

        with open(os.path.join(save_root, 'dropped_neurons_probs.pkl'), 'wb') as f:
            pkl.dump(logreg_neuron_probs, f)

        print('%d dropped neuron regression took %s seconds'%(args.neurons, str(end-start)))
        print(', '.join([str(score) for score in logreg_neuron_scores]), 'train, val, test accuracy for %d dropped neuron regression'%(args.neurons))
        print(str(neuron_c)+' regularization coefficient used')

        start = time.time()
        logreg_neuron_model, logreg_neuron_scores, logreg_neuron_probs, neuron_c, neuron_nnotzero = train_logreg(trXt, trY, vaXt, vaY, teXt, teY, max_iter=args.epochs, eval_test=not args.no_test_eval,
                                                                                                                 seed=args.seed, neurons=sentiment_neurons, report_metric=metric, threshold_metric=metric)
        end = time.time()

    print('%d neuron regression took %s seconds'%(args.neurons, str(end-start)))
    print(', '.join([str(score) for score in logreg_neuron_scores]), 'train, val, test accuracy for %d neuron regression'%(args.neurons))
    print(str(neuron_c)+' regularization coefficient used')

    # log model accuracies, predicted probabilities, and weight/bias of regression model

    with open(os.path.join(save_root, 'all_neurons_score.txt'), 'w') as f:
        f.write(str(logreg_scores))

    with open(os.path.join(save_root, 'neurons_score.txt'), 'w') as f:
        f.write(str(logreg_neuron_scores))

    with open(os.path.join(save_root, 'all_neurons_probs.pkl'), 'wb') as f:
        pkl.dump(logreg_probs, f)

    with open(os.path.join(save_root, 'neurons_probs.pkl'), 'wb') as f:
        pkl.dump(logreg_neuron_probs, f)

    with open(os.path.join(save_root, 'neurons.pkl'), 'wb') as f:
        pkl.dump(logreg_model.coef_, f)

    with open(os.path.join(save_root, 'neuron_bias.pkl'), 'wb') as f:
        pkl.dump(logreg_model.intercept_, f)

    #Plot feats
    use_feats, use_labels = teXt, teY
    if use_feats is None:
        use_feats, use_labels = vaXt, vaY
    if use_feats is None:
        use_feats, use_labels = trXt, trY
    try:
        plot_logits(save_root, use_feats, use_labels, sentiment_neurons)
    except:
        print('no labels to plot logits for')

    plot_weight_contribs_and_save(logreg_model.coef_, os.path.join(save_root, 'weight_vis.png'))


    print('results successfully written to ' + save_root)
    if args.write_results == '':
        exit()

    def get_csv_writer(feats, top_neurons, all_proba, neuron_proba):
        """makes a generator to be used in data_utils.datasets.csv_dataset.write()"""
        header = ['prob w/ all', 'prob w/ %d neuron(s)'%(len(top_neurons),)]
        top_feats = feats[:, top_neurons]
        header += ['neuron %s'%(str(x),) for x in top_neurons]

        yield header

        for i, _ in enumerate(top_feats):
            row = []
            row.append(all_proba[i])
            row.append(neuron_proba[i])
            row.extend(list(top_feats[i].reshape(-1)))
            yield row

    data, use_feats = test_data, teXt
    if use_feats is None:
        data, use_feats = val_data, vaXt
    if use_feats is None:
        data, use_feats = train_data, trXt
    csv_writer = get_csv_writer(use_feats, sentiment_neurons, logreg_probs[-1], logreg_neuron_probs[-1])
    data.dataset.write(csv_writer, path=args.write_results)

if __name__ == '__main__':
    main()

# PROJECT: NVIDIA_sentiment-discovery FILE: setup.py
import os
from setuptools import setup, find_packages
import torch

print("torch.__version__  = ", torch.__version__)
TORCH_MAJOR = int(torch.__version__.split('.')[0])
TORCH_MINOR = int(torch.__version__.split('.')[1])

if TORCH_MAJOR == 0 and TORCH_MINOR < 4:
      raise RuntimeError("Sentiment Discovery requires Pytorch 0.4 or newer.\n" +
                         "The latest stable release can be obtained from https://pytorch.org/")

print("Building module.")
setup(
    name='sentiment_discovery', version='0.4',
#    ext_modules=[cuda_ext,],
    description='PyTorch Extensions written by NVIDIA',
    packages=find_packages(where='.'),
    install_requires=[
        "numpy",
        "pandas",
        "scikit-learn",
        "matplotlib",
        "unidecode",
        "seaborn",
        "sentencepiece",
        "emoji"
    ]
)

# PROJECT: NVIDIA_sentiment-discovery FILE: reparameterization/__init__.py
import torch
from .weight_norm import WeightNorm
from .reparameterization import Reparameterization

def apply_weight_norm(module, name='', dim=0, hook_child=True):
    """
    Applies weight normalization to a parameter in the given module.
    If no parameter is provided, applies weight normalization to all
    parameters in model (except 1-d vectors and scalars).

    .. math::
         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}

    Weight normalization is a reparameterization that decouples the magnitude
    of a weight tensor from its direction. This replaces the parameter specified
    by `name` (e.g. "weight") with two parameters: one specifying the magnitude
    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").
    Weight normalization is implemented via a hook that recomputes the weight
    tensor from the magnitude and direction before every :meth:`~Module.forward`
    call.

    By default, with `dim=0`, the norm is computed independently per output
    channel/plane. To compute a norm over the entire weight tensor, use
    `dim=None`.

    See https://arxiv.org/abs/1602.07868

    Args:
        module (nn.Module): containing module
        name (str, optional): name of weight parameter
        dim (int, optional): dimension over which to compute the norm
        hook_child (boolean, optional): adds reparameterization hook to direct parent of the 
            parameters. If False, it's added to `module` instead. Default: True

    Returns:
        The original module with the weight norm hook

    Example::

        >>> m = apply_weight_norm(nn.Linear(20, 40), name='weight')
        Linear (20 -> 40)
        >>> m.weight_g.size()
        torch.Size([40, 1])
        >>> m.weight_v.size()
        torch.Size([40, 20])

    """
    return apply_reparameterization(module, reparameterization=WeightNorm, hook_child=hook_child,
                                    name=name, dim=dim)

def remove_weight_norm(module, name='', remove_all=False):
    """
    Removes the weight normalization reparameterization of a parameter from a module.
    If no parameter is supplied then all weight norm parameterizations are removed.
    Args:
        module (nn.Module): containing module
        name (str, optional): name of weight parameter
    Example:
        >>> m = apply_weight_norm(nn.Linear(20, 40))
        >>> remove_weight_norm(m)
    """
    return remove_reparameterization(module, reparameterization=WeightNorm,
                                    name=name, remove_all=remove_all)

def get_module_and_name(module, name):
        """
        recursively fetches (possible) child module and name of weight to be reparameterized
        """
        module2use = module
        if name == '':
            return module, name
        names = name.split('.')
        for i, n in enumerate(names):
            param_or_module = getattr(module2use, n)
            if i == len(names)-1:
                if isinstance(param_or_module, torch.nn.Parameter):
                    return module2use, n
                else:
                    return param_or_module, ''
            else:
                module2use = param_or_module

def apply_reparameterization(module, reparameterization=None, name='', dim=0, hook_child=True):
    """
    Applies a given weight reparameterization (such as weight normalization) to
    a parameter in the given module. If no parameter is given, applies the reparameterization
    to all parameters in model (except 1-d vectors and scalars).

    Args:
        module (nn.Module): containing module
        reparameterization (Reparameterization): reparamaterization class to apply
        name (str, optional): name of weight parameter
        dim (int, optional): dimension over which to perform reparameterization op
        hook_child (boolean, optional): adds reparameterization hook to direct parent of the 
            parameters. If False, it's added to `module` instead. Default: True

    Returns:
        The original module with the reparameterization hook

    Example::

        >>> m = apply_reparameterization(nn.Linear(20, 40), WeightNorm)
        Linear (20 -> 40)

    """
    assert reparameterization is not None
    module2use, name2use = get_module_and_name(module, name)
    if name2use != '':
        Reparameterization.apply(module, name, dim, reparameterization, hook_child)
    else:
        names = [n for n,_ in module2use.named_parameters()]
        if name2use != '':
            names = [name2use+'.'+n for n in names]
        if name != '':
            names = [name+'.'+n for n in names]
        for name in names:
            apply_reparameterization(module, reparameterization, name, dim, hook_child)
    return module

def remove_reparameterization(module, reparameterization=Reparameterization,
                                name='', remove_all=False):
    """
    Removes the given reparameterization of a parameter from a module.
    If no parameter is supplied then all reparameterizations are removed.
    Args:
        module (nn.Module): containing module
        reparameterization (Reparameterization): reparamaterization class to apply
        name (str, optional): name of weight parameter
        remove_all (bool, optional): if True, remove all reparamaterizations of given type. Default: False
    Example:
        >>> m = apply_reparameterization(nn.Linear(20, 40),WeightNorm)
        >>> remove_reparameterization(m)
    """
    if name != '' or remove_all:
        to_remove = []
        for k, hook in module._forward_pre_hooks.items():
            if isinstance(hook, reparameterization) and (hook.name == name or remove_all):
                hook.remove(module)
                to_remove.append(k)
        if len(to_remove) > 0:
            for k in to_remove:
                del module._forward_pre_hooks[k]
            return module
        if not remove_all:
            raise ValueError("reparameterization of '{}' not found in {}"
                             .format(name, module))
    else:
        modules = [module]+[x for x in module.modules()]
        for m in modules:
            remove_reparameterization(m, reparameterization=reparameterization, remove_all=True)
        return module

# PROJECT: NVIDIA_sentiment-discovery FILE: reparameterization/weight_norm.py
import torch
from torch.nn.parameter import Parameter
#from ..utils import FusedNorm
import time

from .reparameterization import Reparameterization

def _norm(p, dim):
    """Computes the norm over all dimensions except dim"""
    if dim is None:
        return p.norm()
    elif dim == 0:
        output_size = (p.size(0),) + (1,) * (p.dim() - 1)
        return p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)
    elif dim == p.dim() - 1:
        output_size = (1,) * (p.dim() - 1) + (p.size(-1),)
        return p.contiguous().view(-1, p.size(-1)).norm(dim=0).view(*output_size)
    return _norm(p.transpose(0, dim), 0).transpose(0, dim)

HALF_TYPES = (torch.cuda.HalfTensor, torch.HalfTensor)

class WeightNorm(Reparameterization):
    """
    Weight normalization is a reparameterization that decouples the magnitude
    of a weight tensor from its direction. This replaces the parameter specified
    by `name` (e.g. "weight") with two parameters: one specifying the magnitude
    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").
    Weight normalization is implemented via a hook that recomputes the weight
    tensor from the magnitude and direction before every :meth:`~Module.forward`
    call.

    .. math::
         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}

    By default, with `dim=0`, the norm is computed independently per output
    channel/plane. To compute a norm over the entire weight tensor, use
    `dim=None`.
    """
    def compute_weight(self, module=None, name=None):
        """
        Computes weight normalized weight value to assign value to module attribute
        with name `name`.
        Arguments:
            module (nn.Module): module with weight we'd like to reparameterize
        Returns:
            w (Tensor): Tensor object containing value of reparameterized weight
        """
        if module is None:
            module = self.module
        if name is None:
            name = self.name
        module, name = Reparameterization.get_module_and_name(module, name)
        g = getattr(module, name + '_g')
        v = getattr(module, name + '_v')

        w = (v * (g / _norm(v, self.dim)))

        #fused_norm = FusedNorm.apply
        #v = v.contiguous()
        #w = g*fused_norm(v)


        return w

    def reparameterize(self, name, weight, dim):
        """
        Creates Parameters v and gto be used for weight normalization
        and creates names that for attributes for the module these Parameters
        will correspond to. The parameters will be registered according to the names
        provided.
        Arguments:
            module (nn.Module): module with weight we'd like to reparameterize
            name (str, optional): name of weight parameter
            dim (int, optional): dimension over which to compute parameterization
        Returns:
            names (list, str): names of Parameters to be used for reparameterization
            params (list, Parameter): Parameters to be used for reparameterization
        """
        names = [name + '_g', name + '_v']
        params = [Parameter(_norm(weight, dim).data), Parameter(weight.data)]
        return names, params

# PROJECT: NVIDIA_sentiment-discovery FILE: reparameterization/reparameterization.py
import torch
from torch.nn.parameter import Parameter
import sys
class Reparameterization(object):
    """
    Class interface for performing weight reparameterizations
    Arguments:
        name (str): name of weight parameter
        dim (int): dimension over which to compute the norm
        module (nn.Module): parent module to which param `name` is registered to
        retain_forward (bool, optional): if False deletes weight on call to 
            module.backward. Used to avoid memory leaks with DataParallel Default: True
    Attributes:
        reparameterization_names (list, str): contains names of all parameters 
            needed to compute reparameterization.
        backward_hook_key (int): torch.utils.hooks.RemovableHandle.id for hook used in module backward pass.
    """

    def __init__(self, name, dim, module, retain_forward=True):
        self.name = name
        self.dim = dim
        self.evaluated = False
        self.retain_forward = retain_forward
        self.reparameterization_names = []
        self.backward_hook_key = None
        self.module = module

    def compute_weight(self, module=None, name=None):
        """
        Computes reparameterized weight value to assign value to module attribute
        with name `name`.
        See WeightNorm class for example.
        Arguments:
            module (nn.Module): module with weight we'd like to reparameterize
        Returns:
            w (Tensor): Tensor object containing value of reparameterized weight
        """
        raise NotImplementedError

    def reparameterize(self, name, weight, dim):
        """
        Creates Parameters to be used for reparameterization and creates names that
        for attributes for the module these Parameters will correspond to.
        The parameters will be registered according to the names provided.
        See WeightNorm class for example.
        Arguments:
            module (nn.Module): module with weight we'd like to reparameterize
            name (str, optional): name of weight parameter
            dim (int, optional): dimension over which to compute parameterization
        Returns:
            names (list, str): names of Parameters to be used for reparameterization
            params (list, Parameter): Parameters to be used for reparameterization
        """
        raise NotImplementedError

    @staticmethod
    def apply(module, name, dim, reparameterization=None, hook_child=True):
        """
        Applies reparametrization to module's `name` parameter and modifies instance attributes as appropriate.
        `hook_child` adds reparameterization hook to direct parent of the parameters. If False, it's added to `module` instead.
        """
        if reparameterization is None:
            reparameterization = Reparameterization
        module2use, name2use = Reparameterization.get_module_and_name(module, name)
        # does not work on sparse
        if name2use is None or isinstance(module2use, (torch.nn.Embedding, torch.nn.EmbeddingBag)):
            return

        if hook_child:
            fn = reparameterization(name2use, dim, module2use)
        else:
            fn = reparameterization(name, dim, module)

        weight = getattr(module2use, name2use)
        if weight.dim() <= 1:
            return

        # remove weight from parameter list
        del module2use._parameters[name2use]

        # add parameters of reparameterization of parameter to module
        names, params = fn.reparameterize(name2use, weight, dim)
        for n, p in zip(names, params):
            module2use.register_parameter(n, p)

        # add parameters to reparameterization so they can be removed later
        fn.reparameterization_names = names

        setattr(module2use, name2use, None)

        hook_module = module2use
        if not hook_child:
            hook_module = module
        # recompute weight before every forward()
        hook_module.register_forward_pre_hook(fn)

        # remove weight during backward
        handle = hook_module.register_backward_hook(fn.backward_hook)
        # get hook key so we can delete it later
        fn.backward_hook_key = handle.id

        return fn

    @staticmethod
    def get_module_and_name(module, name):
        """
        recursively fetches (possible) child module and name of weight to be reparameterized
        """
        name2use = None
        module2use = None
        names = name.split('.')
        if len(names) == 1 and names[0] != '':
            name2use = names[0]
            module2use = module
        elif len(names) > 1:
            module2use = module
            name2use = names[0]
            for i in range(len(names)-1):
                module2use = getattr(module2use, name2use)
                name2use = names[i+1]
        return module2use, name2use

    def get_params(self, module):
        """gets params of reparameterization based on known attribute names"""
        return [getattr(module, n) for n in self.reparameterization_names]

    def remove(self, module):
        """removes reparameterization and backward hook (does not remove forward hook)"""
        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
        for p in self.get_params(module2use):
            p.requires_grad = False
        weight = self.compute_weight(module2use, name2use)
        delattr(module2use, name2use)
        for n in self.reparameterization_names:
            del module2use._parameters[n]
        module2use.register_parameter(name2use, Parameter(weight.data))
        del module._backward_hooks[self.backward_hook_key]

    def __call__(self, module, inputs):
        """callable hook for forward pass"""
        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
        _w = getattr(module2use, name2use)
        if not self.evaluated or _w is None:
            if _w is not None:
                delattr(module2use, name2use)
            w = self.compute_weight(module2use, name2use)
            setattr(module2use, name2use, w)
            self.evaluated = True

    def backward_hook(self, module, grad_input, grad_output):
        """callable hook for backward pass"""
        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
        wn = getattr(module2use, name2use)
        self.evaluated = False

# PROJECT: NVIDIA_sentiment-discovery FILE: configure_data.py
import os
import copy

import data_utils

class DataConfig(object):
    def __init__(self, parser, defaults={}):
        super(DataConfig,self).__init__()
        self.parser = parser
        self.defaults = defaults

    def apply(self, opt):
        print('configuring data')
        self.apply_defaults(opt)
        return make_loaders(opt)

    def set_defaults(self, **kwargs):
        for k, v in kwargs.items():
            self.defaults[k] = v

    def apply_defaults(self, opt):
        for k, v in self.defaults.items():
            k = k.replace('-', '_')
            if not hasattr(opt, k):
                setattr(opt, k, v)

def make_loaders(opt):
    """makes training/val/test"""
    batch_size = opt.batch_size * opt.world_size
    eval_batch_size = opt.eval_batch_size * opt.world_size
    seq_length = opt.seq_length
    if seq_length < 0:
        seq_length = seq_length * opt.world_size
    eval_seq_length = opt.eval_seq_length
    if opt.eval_seq_length < 0:
        eval_seq_length = eval_seq_length * opt.world_size
    # data_loader_args = {'num_workers': 0, 'shuffle': opt.shuffle, 'batch_size': batch_size,
    data_loader_args = {'num_workers': 4, 'shuffle': opt.shuffle, 'batch_size': batch_size,
    # data_loader_args = {'num_workers': 1, 'shuffle': opt.shuffle, 'batch_size': batch_size,
                    'pin_memory': True, 'transpose': opt.transpose, 'distributed': opt.world_size > 1,
                    'rank': opt.rank, 'world_size': opt.world_size, 'drop_last': opt.world_size > 1}
    if opt.data_set_type == 'L2R':
        loader_type = data_utils.ShardLoader
        data_loader_args.update({'seq_len': seq_length, 'persist_state': opt.persist_state, 'samples_per_shard': opt.samples_per_shard})
    else:
        loader_type = data_utils.DataLoader
    split = get_split(opt)
    data_set_args = {
        'path': opt.data, 'seq_length': seq_length, 'lazy': opt.lazy, 'delim': opt.delim,
        'text_key': opt.text_key, 'label_key': opt.label_key, 'preprocess': opt.preprocess,
        'ds_type': opt.data_set_type, 'split': split, 'loose': opt.loose_json,
        'tokenizer_type': opt.tokenizer_type, 'tokenizer_model_path': opt.tokenizer_path,
        'vocab_size': opt.vocab_size, 'model_type': opt.tokenizer_model_type,
        'non_binary_cols': opt.non_binary_cols, 'process_fn': opt.process_fn}

    eval_loader_args = copy.copy(data_loader_args)
    eval_set_args = copy.copy(data_set_args)
    eval_set_args['split']=[1.]
    # if optional eval args were set then replace their equivalent values in the arg dict
    if opt.eval_batch_size != 0:
        eval_loader_args['batch_size'] = eval_batch_size
    if opt.eval_seq_length != 0:
        eval_set_args['seq_length'] = eval_seq_length
        if opt.data_set_type == 'L2R':
            eval_loader_args['seq_len'] = eval_seq_length
    if opt.eval_text_key is not None:
        eval_set_args['text_key'] = opt.eval_text_key
    if opt.eval_label_key is not None:
        eval_set_args['label_key'] = opt.eval_label_key

    train = None
    valid = None
    test = None

    if opt.data is not None:
        train, tokenizer = data_utils.make_dataset(**data_set_args)
        if should_split(split):
            train, valid, test = train
    eval_set_args['tokenizer'] = tokenizer

    if opt.valid is not None:
        eval_set_args['path'] = opt.valid
        valid, _ = data_utils.make_dataset(**eval_set_args)
    if test is None and opt.test is not None:
        eval_set_args['path'] = opt.test
        test, _ = data_utils.make_dataset(**eval_set_args)


    if train is not None and opt.batch_size > 0:
        train = loader_type(train, **data_loader_args)
    if valid is not None:
        valid = loader_type(valid, **eval_loader_args)
    if test is not None:
        test = loader_type(test, **eval_loader_args)
    return (train, valid, test), tokenizer

def should_split(split):
    return max(split) != 1.

def get_split(opt):
    splits = []
    if opt.split.find(',') != -1: 
        splits = [float(s) for s in opt.split.split(',')]
    elif opt.split.find('/') != -1:
        splits = [float(s) for s in opt.split.split('/')]
    else:
        splits = [float(opt.split)]
    split_total = sum(splits)
    if split_total < 1.:
        splits.append(1-split_total)
    while len(splits) < 3:
        splits.append(0.)
    splits = splits[:3]
    if opt.valid is not None:
        splits[1] = 0.
    if opt.test is not None:
        splits[2] = 0.
    final_sum = sum(splits)
    return [s/final_sum for s in splits]

def configure_data(parser):
    """add cmdline flags for configuring datasets"""
    main_parser = parser
    group = parser.add_argument_group('data options')
    group.add_argument('--data', nargs='+', default=['./data/imdb/unsup.json'],
                        help="""Filename for training""")
    group.add_argument('--valid', nargs='*', default=None,
                        help="""Filename for validation""")
    group.add_argument('--test', nargs='*', default=None,
                        help="""Filename for testing""")
    group.add_argument('--process-fn', type=str, default='process_str', choices=['process_str', 'process_tweet'],
                        help='what preprocessing function to use to process text. One of [process_str, process_tweet].')
    group.add_argument('--batch-size', type=int, default=128,
                        help='Data Loader batch size')
    group.add_argument('--eval-batch-size', type=int, default=0,
                        help='Data Loader batch size for evaluation datasets')
    group.add_argument('--data-size', type=int, default=256,
                        help='number of tokens in data')
    group.add_argument('--loose-json', action='store_true',
                        help='Use loose json (one json-formatted string per newline), instead of tight json (data file is one json string)')
    group.add_argument('--preprocess', action='store_true',
                        help='force preprocessing of datasets')
    group.add_argument('--delim', default=',',
                        help='delimiter used to parse csv testfiles')
    group.add_argument('--non-binary-cols', nargs='*', default=None,
                        help='labels for columns to non-binary dataset [only works for csv datasets]')
    group.add_argument('--split', default='1.',
                        help='comma-separated list of proportions for training, validation, and test split')
    group.add_argument('--text-key', default='sentence',
                        help='key to use to extract text from json/csv')
    group.add_argument('--label-key', default='label',
                        help='key to use to extract labels from json/csv')
    group.add_argument('--eval-text-key', default=None,
                        help='key to use to extract text from json/csv evaluation datasets')
    group.add_argument('--eval-label-key', default=None,
                        help='key to use to extract labels from json/csv evaluation datasets')
    # tokenizer arguments
    group.add_argument('--tokenizer-type', type=str, default='CharacterLevelTokenizer', choices=['CharacterLevelTokenizer', 'SentencePieceTokenizer'],
                        help='what type of tokenizer to use')
    group.add_argument('--tokenizer-model-type', type=str, default='bpe', choices=['bpe', 'char', 'unigram', 'word'],
                        help='Model type to use for sentencepiece tokenization')
    group.add_argument('--vocab-size', type=int, default=256,
                        help='vocab size to use for non-character-level tokenization')
    group.add_argument('--tokenizer-path', type=str, default='tokenizer.model',
                        help='path used to save/load sentencepiece tokenization models')
    # These are options that are relevant to data loading functionality, but are not meant to be exposed to the command line user.
    # These options are intneded to be set in code by specific scripts.
    defaults = {
                'world_size': 1,
                'rank': -1,
                'persist_state': 0,
                'lazy': False,
                'shuffle': False,
                'transpose': False,
                'data_set_type': 'supervised',
                'seq_length': 256,
                'eval_seq_length': 256,
                'samples_per_shard': 100
               }
    return DataConfig(main_parser, defaults=defaults), group

# PROJECT: NVIDIA_sentiment-discovery FILE: model/RNN_utils/RNN/RNNBackend.py
import torch
import torch.nn as nn
from torch.autograd import Variable

import torch.nn.functional as F

import math


#This function could have some real bad perf penalties if used incorrectly
#Uses in the RNN API should be fine. DIDN'T USE!
def reverse_dir_tensor(tensor, dim=0):
    """
    reverse_dir_tensor stub
    """
    chunked = [sub_tensor for sub_tensor in
               tensor.chunk(tensor.size(dim), dim)]
    chunked = chunked[::-1]
    return torch.cat( chunked, dim=dim).view(*tensor.size())

def is_iterable(maybe_iterable):
    return isinstance(maybe_iterable, (list, tuple))

def flatten_list(tens_list):
    """
    flatten_list stub
    """
    if not (is_iterable(tens_list)):
        return tens_list
    
    return torch.cat(tens_list, dim=0).view(len(tens_list), *tens_list[0].size() )


#These modules always assumes batch_first
class bidirectionalRNN(nn.Module):
    """
    bidirectionalRNN stub
    """
    def __init__(self, inputRNN, num_layers=1, dropout = 0):
        super(bidirectionalRNN, self).__init__()
        self.dropout = dropout
        self.fwd = stackedRNN(inputRNN, num_layers=num_layers, dropout = dropout)
        self.bckwrd = stackedRNN(inputRNN.new_like(), num_layers=num_layers, dropout = dropout)
        self.rnns = nn.ModuleList([self.fwd, self.bckwrd])
        
    #collect hidden option will return all hidden/cell states from entire RNN
    def forward(self, input, collect_hidden=False):
        """
        forward() stub
        """
        seq_len = input.size(0)
        bsz = input.size(1)
        
        fwd_out, fwd_hiddens = list(self.fwd(input, collect_hidden = collect_hidden))
        bckwrd_out, bckwrd_hiddens = list(self.bckwrd(input, reverse=True, collect_hidden = collect_hidden))

        output = torch.cat( [fwd_out, bckwrd_out], -1 )
        hiddens = tuple( torch.cat(hidden, -1) for hidden in zip( fwd_hiddens, bckwrd_hiddens) )

        return output, hiddens

    def reset_parameters(self):
        """
        reset_parameters() stub
        """
        for rnn in self.rnns:
            rnn.reset_parameters()
        
    def init_hidden(self, bsz):
        """
        init_hidden() stub
        """
        for rnn in self.rnns:
            rnn.init_hidden(bsz)

    def detach_hidden(self):
        """
        detach_hidden() stub
        """
        for rnn in self.rnns:
            rnn.detachHidden()
        
    def reset_hidden(self, bsz, reset_mask=None):
        """
        reset_hidden() stub
        """
        for rnn in self.rnns:
            rnn.reset_hidden(bsz, reset_mask=reset_mask)

    def init_inference(self, bsz):    
        """
        init_inference() stub
        """
        for rnn in self.rnns:
            rnn.init_inference(bsz)

   
#assumes hidden_state[0] of inputRNN is output hidden state
#constructor either takes an RNNCell or list of RNN layers
class stackedRNN(nn.Module):        
    """
    stackedRNN stub
    """
    def __init__(self, inputRNN, num_layers=1, dropout=0):
        super(stackedRNN, self).__init__()
        
        self.dropout = dropout
        
        if isinstance(inputRNN, RNNCell):
            self.rnns = [inputRNN]
            for i in range(num_layers-1):
                self.rnns.append(inputRNN.new_like(inputRNN.output_size))
        elif isinstance(inputRNN, list):
            assert len(inputRNN) == num_layers, "RNN list length must be equal to num_layers"
            self.rnns=inputRNN
        else:
            raise RuntimeError()
        
        self.nLayers = len(self.rnns)
        
        self.rnns = nn.ModuleList(self.rnns)


    '''
    Returns output as hidden_state[0] Tensor([sequence steps][batch size][features])
    If collect hidden will also return Tuple(
        [n_hidden_states][sequence steps] Tensor([layer][batch size][features])
    )
    If not collect hidden will also return Tuple(
        [n_hidden_states] Tensor([layer][batch size][features])
    '''
    def forward(self, input, collect_hidden=False, reverse=False, reset_mask=None):
        """
        forward() stub
        """
        
        seq_len = input.size(0)
        bsz = input.size(1)
        inp_iter = reversed(range(seq_len)) if reverse else range(seq_len)

        hidden_states = [[] for i in range(self.nLayers)]
        outputs = []

        for seq in inp_iter:
            if not reverse and reset_mask is not None:
                self.reset_hidden(bsz, reset_mask=reset_mask[seq])
            for layer in range(self.nLayers):
                if layer == 0:
                    prev_out = input[seq]
                    
                outs = self.rnns[layer](prev_out)

                if collect_hidden:
                    hidden_states[layer].append(outs)
                elif seq == seq_len-1:
                    hidden_states[layer].append(outs)
                    
                prev_out = outs[0]
            if reverse and reset_mask is not None:
                self.reset_hidden(bsz, reset_mask=reset_mask[seq])

            outputs.append(prev_out)

        if reverse:
            outputs = list(reversed(outputs))
        '''
        At this point outputs is in format:
        list( [seq_length] x Tensor([bsz][features]) )
        need to convert it to:
        list( Tensor([seq_length][bsz][features]) )
        '''
        output = flatten_list(outputs)

        '''
        hidden_states at this point is in format:
        list( [layer][seq_length][hidden_states] x Tensor([bsz][features]) )
        need to convert it to:
          For not collect hidden:
            list( [hidden_states] x Tensor([layer][bsz][features]) )
          For collect hidden:
            list( [hidden_states][seq_length] x Tensor([layer][bsz][features]) )
        '''
        if not collect_hidden:
            seq_len = 1
        n_hid = self.rnns[0].n_hidden_states
        new_hidden = [ [ [ None for k in range(self.nLayers)] for j in range(seq_len) ] for i in range(n_hid) ]

        for i in range(n_hid):
            for j in range(seq_len):
                for k in range(self.nLayers):
                    new_hidden[i][j][k] = hidden_states[k][j][i]

        hidden_states = new_hidden
        #Now in format list( [hidden_states][seq_length][layer] x Tensor([bsz][features]) )
        #Reverse seq_length if reverse
        if reverse:
            hidden_states = list( list(reversed(list(entry))) for entry in hidden_states)

        #flatten layer dimension into tensor
        hiddens = list( list(
            flatten_list(seq) for seq in hidden )
                        for hidden in hidden_states )
        
        #Now in format list( [hidden_states][seq_length] x Tensor([layer][bsz][features]) )
        #Remove seq_length dimension if not collect_hidden
        if not collect_hidden:
            hidden_states = list( entry[0] for entry in hidden_states)
        return output, hidden_states
    
    def reset_parameters(self):
        """
        reset_parameters() stub
        """
        for rnn in self.rnns:
            rnn.reset_parameters()
        
    def init_hidden(self, bsz):
        """
        init_hidden() stub
        """
        for rnn in self.rnns:
            rnn.init_hidden(bsz)

    def detach_hidden(self):
        """
        detach_hidden() stub
        """
        for rnn in self.rnns:
            rnn.detach_hidden()
        
    def reset_hidden(self, bsz, reset_mask=None):
        """
        reset_hidden() stub
        """
        for rnn in self.rnns:
            rnn.reset_hidden(bsz, reset_mask=reset_mask)

    def init_inference(self, bsz):    
        """ 
        init_inference() stub
        """
        for rnn in self.rnns:
            rnn.init_inference(bsz)

class RNNCell(nn.Module):
    """ 
    RNNCell stub
    gate_multiplier is related to the architecture you're working with
    For LSTM-like it will be 4 and GRU-like will be 3.
    Always assumes input is NOT batch_first.
    Output size that's not hidden size will use output projection
    Hidden_states is number of hidden states that are needed for cell
    if one will go directly to cell as tensor, if more will go as list
    """
    def __init__(self, gate_multiplier, input_size, hidden_size, cell, n_hidden_states = 2, bias = False, output_size = None):
        super(RNNCell, self).__init__()

        self.gate_multiplier = gate_multiplier
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.cell = cell
        self.bias = bias
        self.output_size = output_size
        if output_size is None:
            self.output_size = hidden_size

        self.gate_size = gate_multiplier * self.hidden_size
        self.n_hidden_states = n_hidden_states

        self.w_ih = nn.Parameter(torch.Tensor(self.gate_size, self.input_size))
        self.w_hh = nn.Parameter(torch.Tensor(self.gate_size, self.output_size))

        #Check if there's recurrent projection
        if(self.output_size != self.hidden_size):
            self.w_ho = nn.Parameter(torch.Tensor(self.output_size, self.hidden_size))

        self.b_ih = self.b_hh = None
        if self.bias:
            self.b_ih = nn.Parameter(torch.Tensor(self.gate_size))
            self.b_hh = nn.Parameter(torch.Tensor(self.gate_size))
            
        #hidden states for forward
        self.hidden = [ None for states in range(self.n_hidden_states)]

        self.reset_parameters()

    def new_like(self, new_input_size=None):
        """
        new_like() stub
        """
        if new_input_size is None:
            new_input_size = self.input_size
            
        return type(self)(self.gate_multiplier,
                       new_input_size,
                       self.hidden_size,
                       self.cell,
                       self.n_hidden_states,
                       self.bias,
                       self.output_size)

    
    #Use xavier where we can (weights), otherwise use uniform (bias)
    def reset_parameters(self, gain=1):
        """
        reset_parameters() stub
        """
        stdev = 1.0 / math.sqrt(self.hidden_size)
        for param in self.parameters():
            param.data.uniform_(-stdev, stdev)
    '''
    def reset_parameters(self, gain=1):
        stdv = 1.0 / math.sqrt(self.gate_size)

        self.w_ih.uniform_(-stdv, stdv)
        self.w_hh.uniform_(-stdv, stdv)
        if self.bias:
            self.b_ih.uniform_(-stdv/2, stdv/2)
            self.b_hh.uniform_(-stdv/2, stdv/2)
        
        #for param in self.parameters():
        #    #if (param.dim() > 1):
        #    #    torch.nn.init.xavier_normal(param, gain)
        #    #else:
        #    param.data.uniform_(-stdv, stdv)
    '''
    def init_hidden(self, bsz):
        """
        init_hidden() stub
        """
        for param in self.parameters():
            if param is not None:
                a_param = param
                break

        for i, _ in enumerate(self.hidden):
            if(self.hidden[i] is None or self.hidden[i].data.size()[0] != bsz):

                if i==0:
                    hidden_size = self.output_size
                else:
                    hidden_size = self.hidden_size

                tens = a_param.data.new(bsz, hidden_size).zero_()
                self.hidden[i] = Variable(tens, requires_grad=False)
        
    def reset_hidden(self, bsz, reset_mask=None):
        """
        reset_hidden() stub
        """
        if reset_mask is not None:
            if (reset_mask != 0).any():
                for i, v in enumerate(self.hidden):
                    if reset_mask.numel() == 1:
                        self.hidden[i] = v.data.zero_()
                    else:
                        reset_mask = reset_mask.view(self.hidden[i].size(0), 1).contiguous()
                        self.hidden[i] = v * (1 - reset_mask).type_as(v.data)
            return

        for i, _ in enumerate(self.hidden):
            self.hidden[i] = None
        self.init_hidden(bsz)

    def detach_hidden(self):
        """
        detach_hidden() stub
        """
        for i, _ in enumerate(self.hidden):
            if self.hidden[i] is None:
                raise RuntimeError("Must inialize hidden state before you can detach it")
        for i, _ in enumerate(self.hidden):
            self.hidden[i] = self.hidden[i].detach()
        
    def forward(self, input):
        """
        forward() stub
        if not inited or bsz has changed this will create hidden states
        """
        self.init_hidden(input.size()[0])

        hidden_state = self.hidden[0] if self.n_hidden_states == 1 else self.hidden

        self.hidden = self.cell(input, hidden_state, self.w_ih, self.w_hh, b_ih=self.b_ih, b_hh=self.b_hh)
        if(self.n_hidden_states > 1):
            self.hidden = list(self.hidden)
        else:
            self.hidden=[self.hidden]


        if self.output_size != self.hidden_size:
            self.hidden[0] = F.linear(self.hidden[0], self.w_ho)

        return tuple(self.hidden)

# PROJECT: NVIDIA_sentiment-discovery FILE: model/RNN_utils/RNN/models.py
import torch

# from torch.nn._functions.rnn import LSTMCell, RNNReLUCell, RNNTanhCell, GRUCell

from .RNNBackend import bidirectionalRNN, stackedRNN, RNNCell
from .cells import mLSTMRNNCell, mLSTMCell

_VF = torch._C._VariableFunctions
_rnn_impls = {
    'LSTM': _VF.lstm_cell,
    'GRU': _VF.gru_cell,
    'RNN_TANH': _VF.rnn_tanh_cell,
    'RNN_RELU': _VF.rnn_relu_cell,
}

def toRNNBackend(inputRNN, num_layers, bidirectional=False, dropout = 0):
    """
    toRNNBackend stub
    """

    if bidirectional:
        return bidirectionalRNN(inputRNN, num_layers, dropout = dropout)
    else:
        return stackedRNN(inputRNN, num_layers, dropout = dropout)


def LSTM(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):
    """
    LSTM stub
    """
    inputRNN = RNNCell(4, input_size, hidden_size, _rnn_impls['LSTM'], 2, bias, output_size)
    # inputRNN = RNNCell(4, input_size, hidden_size, LSTMCell, 2, bias, output_size)
    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)

def GRU(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):
    """
    GRU stub
    """
    inputRNN = RNNCell(3, input_size, hidden_size, _rnn_impls['GRU'], 1, bias, output_size)
    # inputRNN = RNNCell(3, input_size, hidden_size, GRUCell, 1, bias, output_size)
    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)

def ReLU(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):
    """
    ReLU stub
    """
    inputRNN = RNNCell(1, input_size, hidden_size, _rnn_impls['RNN_RELU'], 1, bias, output_size)
    # inputRNN = RNNCell(1, input_size, hidden_size, RNNReLUCell, 1, bias, output_size)
    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)

def Tanh(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):
    """
    Tanh stub
    """
    inputRNN = RNNCell(1, input_size, hidden_size, _rnn_impls['RNN_TANH'], 1, bias, output_size)
    inputRNN = RNNCell(1, input_size, hidden_size, RNNTanhCell, 1, bias, output_size)
    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)
        
def mLSTM(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):
    """
    mLSTM stub
    """
    print("Creating mlstm")
    inputRNN = mLSTMRNNCell(input_size, hidden_size, bias=bias, output_size=output_size)
    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)

# PROJECT: NVIDIA_sentiment-discovery FILE: model/RNN_utils/RNN/__init__.py
from .models import LSTM, GRU, ReLU, Tanh, mLSTM

__all__ = ['models']

# PROJECT: NVIDIA_sentiment-discovery FILE: model/RNN_utils/RNN/cells.py
import torch
import torch.nn as nn
import torch.nn.functional as F

from .RNNBackend import RNNCell

# from torch.nn._functions.thnn import rnnFusedPointwise as fusedBackend
_VF = torch._C._VariableFunctions

import math 


class mLSTMRNNCell(RNNCell):
    """
    mLSTMRNNCell stub
    """

    def __init__(self, input_size, hidden_size, bias = False, output_size = None):
        gate_multiplier = 4
        super(mLSTMRNNCell, self).__init__(gate_multiplier, input_size, hidden_size, mLSTMCell, n_hidden_states = 2, bias = bias, output_size = output_size)

        self.w_mih = nn.Parameter(torch.Tensor(self.output_size, self.input_size))
        self.w_mhh = nn.Parameter(torch.Tensor(self.output_size, self.output_size))

        self.reset_parameters()

    def forward(self, input):
        """
        mLSTMRNNCell.forward() stub
        """
        #if not inited or bsz has changed this will create hidden states
        self.init_hidden(input.size()[0])

        hidden_state = self.hidden[0] if self.n_hidden_states == 1 else self.hidden

        self.hidden = list(
                           self.cell(input, hidden_state, self.w_ih, self.w_hh, self.w_mih, self.w_mhh,
                           b_ih=self.b_ih, b_hh=self.b_hh)
        )

        if self.output_size != self.hidden_size:
            self.hidden[0] = F.linear(self.hidden[0], self.w_ho)
        return tuple(self.hidden)


    def new_like(self, new_input_size=None):
        if new_input_size is None:
            new_input_size = self.input_size
        
        return type(self)(
            new_input_size,
            self.hidden_size,
            self.bias,
            self.output_size)

def mLSTMCell(input, hidden, w_ih, w_hh, w_mih, w_mhh, b_ih=None, b_hh=None):
    """
    mLSTMCell stub
    """

    # TODO: look into fusedLSTM not getting proper results.
    if input.is_cuda:
        m = F.linear(input, w_mih) * F.linear(hidden[0], w_mhh)

        state = _VF.lstm_cell
        return state(input, (m, hidden[1]), w_ih, w_hh, b_ih, b_hh)

    hx, cx = hidden
    
    m = F.linear(input, w_mih) * F.linear(hidden[0], w_mhh)
    igates = F.linear(input, w_ih, b_ih) + F.linear(m, w_hh, b_hh)

    ingate, forgetgate, cellgate, outgate = igates.chunk(4, 1)

    ingate = F.sigmoid(ingate)
    forgetgate = F.sigmoid(forgetgate)
    cellgate = F.tanh(cellgate)
    outgate = F.sigmoid(outgate)
    
    cy = (forgetgate * cx) + (ingate * cellgate)
    hy = outgate * F.tanh(cy)
    
    return hy, cy
                                                                            

# PROJECT: NVIDIA_sentiment-discovery FILE: model/checkpoint.py
###############################################################################
# BSD 3-Clause License
#
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#   
# Copyright (c) 2016, Facebook, inc (Adam Paszke). All rights reserved.
###############################################################################
'''
Code adapted from https://github.com/pytorch/pytorch/blob/master/torch/utils/checkpoint.py
Introduced rng management in order to get correct results with random layers (eg. dropout)
'''

from __future__ import absolute_import, division, print_function, unicode_literals
import torch
import warnings


def detach_variable(inputs):
    if isinstance(inputs, tuple):
        out = []
        for inp in inputs:
            x = inp.detach()
            x.requires_grad = inp.requires_grad
            out.append(x)
        return tuple(out)
    else:
        raise RuntimeError(
            "Only tuple of tensors is supported. Got Unsupported input type: ", type(inputs).__name__)


def check_backward_validity(inputs):
    if not any(inp.requires_grad for inp in inputs):
        warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")


class CheckpointFunction(torch.autograd.Function):

    @staticmethod
    def forward(ctx, run_function, *args):
        check_backward_validity(args)
        ctx.run_function = run_function
        ctx.random_state = torch.cuda.get_rng_state()
        ctx.save_for_backward(*args)
        with torch.no_grad():
            outputs = run_function(*args)
        return outputs

    @staticmethod
    def backward(ctx, *args):
        if not torch.autograd._is_checkpoint_valid():
            raise RuntimeError("Checkpointing is not compatible with .grad(), please use .backward() if possible")
        inputs = ctx.saved_tensors
        detached_inputs = detach_variable(inputs)

        backward_rng = torch.cuda.get_rng_state()
        torch.cuda.set_rng_state(ctx.random_state)
        with torch.enable_grad():
            outputs = ctx.run_function(*detached_inputs)
        torch.cuda.set_rng_state(backward_rng)

        if isinstance(outputs, torch.Tensor):
            outputs = (outputs,)
        torch.autograd.backward(outputs, args)
        return (None,) + tuple(inp.grad for inp in detached_inputs)


def checkpoint(function, *args):
    r"""Checkpoint a model or part of the model

    Checkpointing works by trading compute for memory. Rather than storing all
    intermediate activations of the entire computation graph for computing
    backward, the checkpointed part does **not** save intermediate activations,
    and instead recomputes them in backward pass. It can be applied on any part
    of a model.

    Specifically, in the forward pass, :attr:`function` will run in
    :func:`torch.no_grad` manner, i.e., not storing the intermediate
    activations. Instead, the forward pass saves the inputs tuple and the
    :attr:`function` parameter. In the backwards pass, the saved inputs and
    :attr:`function` is retreived, and the forward pass is computed on
    :attr:`function` again, now tracking the intermediate activations, and then
    the gradients are calculated using these activation values.

    .. warning::
        Checkpointing doesn't work with :func:`torch.autograd.grad`, but only
        with :func:`torch.autograd.backward`.

    .. warning::
        If :attr:`function` invocation during backward does anything different
        than the one during forward, e.g., due to some global variable, the
        checkpointed version won't be equivalent, and unfortunately it can't be
        detected.

    .. warning:
        At least one of the inputs needs to have :code:`requires_grad=True` if
        grads are needed for model inputs, otherwise the checkpointed part of the
        model won't have gradients.

    Args:
        function: describes what to run in the forward pass of the model or
            part of the model. It should also know how to handle the inputs
            passed as the tuple. For example, in LSTM, if user passes
            ``(activation, hidden)``, :attr:`function` should correctly use the
            first input as ``activation`` and the second input as ``hidden``
        args: tuple containing inputs to the :attr:`function`

    Returns:
        Output of running :attr:`function` on :attr:`*args`
    """
    return CheckpointFunction.apply(function, *args)


def checkpoint_sequential(functions, segments, *inputs):
    r"""A helper function for checkpointing sequential models.

    Sequential models execute a list of modules/functions in order
    (sequentially). Therefore, we can divide such a model in various segments
    and checkpoint each segment. All segments except the last will run in
    :func:`torch.no_grad` manner, i.e., not storing the intermediate
    activations. The inputs of each checkpointed segment will be saved for
    re-running the segment in the backward pass.

    See :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.

    .. warning::
        Checkpointing doesn't work with :func:`torch.autograd.grad`, but only
        with :func:`torch.autograd.backward`.

    .. warning:
        At least one of the inputs needs to have :code:`requires_grad=True` if
        grads are needed for model inputs, otherwise the checkpointed part of the
        model won't have gradients.

    Args:
        functions: A :class:`torch.nn.Sequential` or the list of modules or
            functions (comprising the model) to run sequentially.
        segments: Number of chunks to create in the model
        inputs: tuple of Tensors that are inputs to :attr:`functions`

    Returns:
        Output of running :attr:`functions` sequentially on :attr:`*inputs`

    Example:
        >>> model = nn.Sequential(...)
        >>> input_var = checkpoint_sequential(model, chunks, input_var)
    """

    def run_function(start, end, functions):
        def forward(*inputs):
            input = inputs[0]
            for j in range(start, end + 1):
                input = functions[j](input)
            return input
        return forward

    if isinstance(functions, torch.nn.Sequential):
        functions = list(functions.children())

    segment_size = len(functions) // segments
    # the last chunk has to be non-volatile
    end = -1
    for start in range(0, segment_size * (segments - 1), segment_size):
        end = start + segment_size - 1
        inputs = checkpoint(run_function(start, end, functions), *inputs)
        if not isinstance(inputs, tuple):
            inputs = (inputs,)
    return run_function(end + 1, len(functions) - 1, functions)(*inputs)
# PROJECT: NVIDIA_sentiment-discovery FILE: model/transformer_utils.py
###############################################################################
# BSD 3-Clause License
#
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#   
# Copyright (c) 2017, Facebook, inc. All rights reserved.
###############################################################################
'''
Code adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py
Introduced optimal gradient checkpointing for intermediate layers in ./transformer.py
'''

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

from collections import defaultdict

class MultiheadAttention(nn.Module):
    """Multi-headed attention.

    See "Attention Is All You Need" for more details.
    """
    def __init__(self, embed_dim, num_heads, dropout=0., bias=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
        self.scaling = self.head_dim**-0.5
        self._mask = None

        self.in_proj_weight = nn.Parameter(torch.Tensor(3*embed_dim, embed_dim))
        if bias:
            self.in_proj_bias = nn.Parameter(torch.Tensor(3*embed_dim))
        else:
            self.register_parameter('in_proj_bias', None)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.in_proj_weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.in_proj_bias is not None:
            nn.init.constant_(self.in_proj_bias, 0.)
            nn.init.constant_(self.out_proj.bias, 0.)

    def forward(self, query, key, value, mask_future_timesteps=False,
                key_padding_mask=None, incremental_state=None,
                need_weights=True, static_kv=False):
        """Input shape: Time x Batch x Channel

        Self-attention can be implemented by passing in the same arguments for
        query, key and value. Future timesteps can be masked with the
        `mask_future_timesteps` argument. Padding elements can be excluded from
        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:
        batch x src_len, where padding elements are indicated by 1s.
        """

        qkv_same = query.data_ptr() == key.data_ptr() == value.data_ptr()
        kv_same = key.data_ptr() == value.data_ptr()

        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.embed_dim
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        assert key.size() == value.size()

        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if 'prev_key' in saved_state:
                # previous time steps are cached - no need to recompute
                # key and value if they are static
                if static_kv:
                    assert kv_same and not qkv_same
                    key = value = None
        else:
            saved_state = None

        if qkv_same:
            # self-attention
            q, k, v = self.in_proj_qkv(query)
        elif kv_same:
            # encoder-decoder attention
            q = self.in_proj_q(query)
            if key is None:
                assert value is None
                # this will allow us to concat it with previous value and get
                # just get the previous value
                k = v = q.new(0)
            else:
                k, v = self.in_proj_kv(key)
        else:
            q = self.in_proj_q(query)
            k = self.in_proj_k(key)
            v = self.in_proj_v(value)
        q *= self.scaling

        if saved_state is not None:
            if 'prev_key' in saved_state:
                k = torch.cat((saved_state['prev_key'], k), dim=0)
            if 'prev_value' in saved_state:
                v = torch.cat((saved_state['prev_value'], v), dim=0)
            saved_state['prev_key'] = k
            saved_state['prev_value'] = v
            self._set_input_buffer(incremental_state, saved_state)

        src_len = k.size(0)

        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == src_len

        q = q.contiguous().view(tgt_len, bsz*self.num_heads, self.head_dim).transpose(0, 1)
        k = k.contiguous().view(src_len, bsz*self.num_heads, self.head_dim).transpose(0, 1)
        v = v.contiguous().view(src_len, bsz*self.num_heads, self.head_dim).transpose(0, 1)

        attn_weights = torch.bmm(q, k.transpose(1, 2))
        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]

        # only apply masking at training time (when incremental state is None)
        if mask_future_timesteps and incremental_state is None:
            assert query.size() == key.size(), \
                'mask_future_timesteps only applies to self-attention'
            attn_weights += self.buffered_mask(attn_weights).unsqueeze(0)
        if key_padding_mask is not None:
            # don't attend to padding symbols
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.float().masked_fill(
                key_padding_mask.unsqueeze(1).unsqueeze(2),
                float('-inf'),
            ).type_as(attn_weights)  # FP16 support: cast to float and back
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        attn_weights = F.softmax(attn_weights.float(), dim=-1).type_as(attn_weights)
        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)

        attn = torch.bmm(attn_weights, v)
        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
        attn = self.out_proj(attn)

        if need_weights:
            # average attention weights over heads
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.sum(dim=1) / self.num_heads
        else:
            attn_weights = None

        return attn, attn_weights

    def in_proj_k(self, key):
        return self._in_proj(key, start=self.embed_dim, end=2*self.embed_dim)

    def in_proj_v(self, value):
        return self._in_proj(value, start=2*self.embed_dim)

    def _in_proj(self, input, start=None, end=None):
        weight = self.in_proj_weight
        bias = self.in_proj_bias
        if end is not None:
            weight = weight[:end, :]
            if bias is not None:
                bias = bias[:end]
        if start is not None:
            weight = weight[start:, :]
            if bias is not None:
                bias = bias[start:]
        return F.linear(input.type_as(weight), weight, bias)

    def buffered_mask(self, tensor):
        attn = self.out_proj(attn)

        if need_weights:
            # average attention weights over heads
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.sum(dim=1) / self.num_heads
        else:
            attn_weights = None

        return attn, attn_weights

    def in_proj_qkv(self, query):
        return self._in_proj(query).chunk(3, dim=-1)

    def in_proj_kv(self, key):
        return self._in_proj(key, start=self.embed_dim).chunk(2, dim=-1)

    def in_proj_q(self, query):
        return self._in_proj(query, end=self.embed_dim)

    def in_proj_k(self, key):
        return self._in_proj(key, start=self.embed_dim, end=2*self.embed_dim)

    def in_proj_v(self, value):
        return self._in_proj(value, start=2*self.embed_dim)

    def _in_proj(self, input, start=None, end=None):
        weight = self.in_proj_weight
        bias = self.in_proj_bias
        if end is not None:
            weight = weight[:end, :]
            if bias is not None:
                bias = bias[:end]
        if start is not None:
            weight = weight[start:, :]
            if bias is not None:
                bias = bias[start:]
        return F.linear(input.type_as(weight), weight, bias)

    def buffered_mask(self, tensor):
        dim = tensor.size(-1)
        if self._mask is None:
            self._mask = torch.triu(fill_with_neg_inf(tensor.new(dim, dim)), 1)
        if self._mask.size(0) < dim:
            self._mask = torch.triu(fill_with_neg_inf(self._mask.resize_(dim, dim)), 1)
        return self._mask[:dim, :dim]

    def reorder_incremental_state(self, incremental_state, new_order):
        """Reorder buffered internal state (for incremental generation)."""
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            for k in input_buffer.keys():
                input_buffer[k] = input_buffer[k].index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state):
        return get_incremental_state(
            self,
            incremental_state,
            'attn_state',
        ) or {}

    def _set_input_buffer(self, incremental_state, buffer):
        set_incremental_state(
            self,
            incremental_state,
            'attn_state',
            buffer,
        )


class LearnedPositionalEmbedding(nn.Embedding):
    """This module learns positional embeddings up to a fixed maximum size.

    Padding symbols are ignored, but it is necessary to specify whether padding
    is added on the left side (left_pad=True) or right side (left_pad=False).
    """

    def __init__(self, num_embeddings, embedding_dim, padding_idx, left_pad):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        self.left_pad = left_pad

    def forward(self, input, incremental_state=None):
        """Input is expected to be of size [bsz x seqlen]."""
        if incremental_state is not None:
            # positions is the same for every token when decoding a single step
            positions = input.data.new(1, 1).fill_(self.padding_idx + input.size(1))
        else:
            positions = make_positions(input.data, self.padding_idx, self.left_pad)
        return super().forward(positions)

    def max_positions(self):
        """Maximum number of supported positions."""
        return self.num_embeddings - self.padding_idx - 1


class SinusoidalPositionalEmbedding(nn.Module):
    """This module produces sinusoidal positional embeddings of any length.

    Padding symbols are ignored, but it is necessary to specify whether padding
    is added on the left side (left_pad=True) or right side (left_pad=False).
    """

    def __init__(self, embedding_dim, padding_idx, left_pad, init_size=1024):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.padding_idx = padding_idx
        self.left_pad = left_pad
        self.weights = SinusoidalPositionalEmbedding.get_embedding(
            init_size,
            embedding_dim,
            padding_idx,
        )
        self.register_buffer('_float_tensor', torch.FloatTensor())

    @staticmethod
    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):
        """Build sinusoidal embeddings.

        This matches the implementation in tensor2tensor, but differs slightly
        from the description in Section 3.5 of "Attention Is All You Need".
        """
        half_dim = embedding_dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)
        if embedding_dim % 2 == 1:
            # zero pad
            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
        if padding_idx is not None:
            emb[padding_idx, :] = 0
        return emb

    def forward(self, input, incremental_state=None):
        """Input is expected to be of size [bsz x seqlen]."""
        # recompute/expand embeddings if needed
        bsz, seq_len = input.size()
        max_pos = self.padding_idx + 1 + seq_len
        if self.weights is None or max_pos > self.weights.size(0):
            self.weights = SinusoidalPositionalEmbedding.get_embedding(
                max_pos,
                self.embedding_dim,
                self.padding_idx,
            )
        self.weights = self.weights.type_as(self._float_tensor)

        if incremental_state is not None:
            # positions is the same for every token when decoding a single step
            return self.weights[self.padding_idx + seq_len, :].expand(bsz, 1, -1)

        positions = make_positions(input.data, self.padding_idx, self.left_pad)
        return self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()

    def max_positions(self):
        """Maximum number of supported positions."""
        return int(1e5)  # an arbitrary large number


class GeLU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + F.tanh(0.79788456080 * (x + 0.044715 * x * x * x)))


def Embedding(num_embeddings, embedding_dim, padding_idx):
    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)
    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)
    return m

def LayerNorm(embedding_dim):
    m = nn.LayerNorm(embedding_dim)
    return m

def Linear(in_features, out_features, bias=True):
    m = nn.Linear(in_features, out_features, bias)
    nn.init.xavier_uniform_(m.weight)
    nn.init.constant_(m.bias, 0.)
    return m

def PositionalEmbedding(num_embeddings, embedding_dim, padding_idx, left_pad, learned=False):
    if learned:
        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx, left_pad)
        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)
        nn.init.constant_(m.weight[padding_idx], 0)
    else:
        m = SinusoidalPositionalEmbedding(embedding_dim, padding_idx, left_pad, num_embeddings)
    return m

INCREMENTAL_STATE_INSTANCE_ID = defaultdict(lambda: 0)

def _get_full_incremental_state_key(module_instance, key):
    module_name = module_instance.__class__.__name__

    # assign a unique ID to each module instance, so that incremental state is
    # not shared across module instances
    if not hasattr(module_instance, '_fairseq_instance_id'):
        INCREMENTAL_STATE_INSTANCE_ID[module_name] += 1
        module_instance._fairseq_instance_id = INCREMENTAL_STATE_INSTANCE_ID[module_name]

    return '{}.{}.{}'.format(module_name, module_instance._fairseq_instance_id, key)

def get_incremental_state(module, incremental_state, key):
    """Helper for getting incremental state for an nn.Module."""
    full_key = _get_full_incremental_state_key(module, key)
    if incremental_state is None or full_key not in incremental_state:
        return None
    return incremental_state[full_key]

def set_incremental_state(module, incremental_state, key, value):
    """Helper for setting incremental state for an nn.Module."""
    if incremental_state is not None:
        full_key = _get_full_incremental_state_key(module, key)
        incremental_state[full_key] = value

def fill_with_neg_inf(t):
    """FP16-compatible function that fills a tensor with -inf."""
    return t.float().fill_(float('-inf')).type_as(t)

def make_positions(tensor, padding_idx, left_pad):
    """Replace non-padding symbols with their position numbers.

    Position numbers begin at padding_idx+1.

    Padding symbols are ignored, but it is necessary to specify whether padding
    is added on the left side (left_pad=True) or right side (left_pad=False).
    """
    max_pos = padding_idx + 1 + tensor.size(1)
    if not hasattr(make_positions, 'range_buf'):
        make_positions.range_buf = tensor.new()
    make_positions.range_buf = make_positions.range_buf.type_as(tensor)
    if make_positions.range_buf.numel() < max_pos:
        torch.arange(padding_idx + 1, max_pos, out=make_positions.range_buf)
    mask = tensor.ne(padding_idx)
    positions = make_positions.range_buf[:tensor.size(1)].expand_as(tensor)
    if left_pad:
        positions = positions - mask.size(1) + mask.long().sum(dim=1).unsqueeze(1)
    return tensor.clone().masked_scatter_(mask, positions[mask])

# PROJECT: NVIDIA_sentiment-discovery FILE: model/__init__.py
from .distributed import *
from .model import *
from .sentiment_classifier import *
from .transformer import *
from .transformer_utils import *
# PROJECT: NVIDIA_sentiment-discovery FILE: model/model.py
import math

import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F

from .RNN_utils import RNN
from .transformer_utils import Embedding
from .transformer import TransformerDecoder

class RNNModel(nn.Module):
    """Container module with an encoder, a recurrent module, and a decoder."""

    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):
        super(RNNModel, self).__init__()
        self.drop = nn.Dropout(dropout)
        self.encoder = nn.Embedding(ntoken, ninp)
        self.decoder = nn.Linear(nhid, ntoken)
        self.rnn=getattr(RNN, rnn_type)(ninp, nhid, nlayers, dropout=dropout)
        # Optionally tie weights as in:
        # "Using the Output Embedding to Improve Language Models" (Press & Wolf 2016)
        # https://arxiv.org/abs/1608.05859
        # and
        # "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling" (Inan et al. 2016)
        # https://arxiv.org/abs/1611.01462
        if tie_weights:
            if nhid != ninp:
                raise ValueError('When using the tied flag, nhid must be equal to emsize')
            self.decoder.weight = self.encoder.weight

        self.decoder.bias.data.fill_(0)
        self.rnn_type = rnn_type
        self.nhid = nhid
        self.nlayers = nlayers

    def forward(self, input, reset_mask=None, chkpt_grad=False, **kwargs):
        emb = self.drop(self.encoder(input))
        self.rnn.detach_hidden()

        output, hidden = self.rnn(emb, reset_mask=reset_mask)
        output = self.drop(output)
        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        sd = {}
        sd['encoder'] = self.encoder.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd['rnn'] = self.rnn.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd = {'encoder': sd}
        sd['decoder'] = self.decoder.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        return sd

    def load_state_dict(self, state_dict, strict=True):
        if 'decoder' in state_dict:
            self.decoder.load_state_dict(state_dict['decoder'], strict=strict)
        self.encoder.load_state_dict(state_dict['encoder']['encoder'], strict=strict)
        self.rnn.load_state_dict(state_dict['encoder']['rnn'], strict=strict)


class RNNFeaturizer(nn.Module):
    """Container module with an encoder, a recurrent module, and a decoder."""

    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, all_layers=False, concat_pools=[False] * 3, hidden_warmup=False, residuals=False, get_lm_out=False):
        super(RNNFeaturizer, self).__init__()
        self.drop = nn.Dropout(dropout)
        self.encoder = nn.Embedding(ntoken, ninp)
        self.rnn=getattr(RNN, rnn_type)(ninp, nhid, nlayers, dropout=dropout)#, residuals=residuals)
        # self.rnn=getattr(RNN, rnn_type)(ninp, nhid, nlayers, dropout=dropout, residuals=residuals)

        self.rnn_type = rnn_type
        self.nhid = nhid
        self.nlayers = nlayers
        self.all_layers = all_layers
        self.hidden_warmup = hidden_warmup
        self.aux_lm_loss = get_lm_out
        if self.aux_lm_loss:
            self.decoder = nn.Linear(nhid, ntoken)
        self.concat_max, self.concat_min, self.concat_mean = concat_pools
        self.output_size = self.nhid if not self.all_layers else self.nhid * self.nlayers
        self.output_size *= (1 + sum(concat_pools))

    def forward(self, input, seq_len=None, get_hidden=False, chkpt_grad=False, **kwargs):
        if not self.hidden_warmup:
            self.rnn.reset_hidden(input.size(1))
        if self.aux_lm_loss:
            outs = []
        if seq_len is None:
            for i in range(input.size(0)):
                emb = self.drop(self.encoder(input[i]))
                out, hidden = self.rnn(emb.unsqueeze(0), collect_hidden=True, chkpt_grad=chkpt_grad)
                if self.aux_lm_loss:
                    outs.append(out)
            cell = self.get_features(hidden)
            if self.concat_pools:
                cell = torch.cat((cell, torch.mean(cell, -1), torch.max(cell, -1)))
            if get_hidden:
                cell = (self.get_features(hidden, get_hidden=True), cell)
        else:
            last_cell = last_hidden = 0
            ops = ['max', 'min', 'add']
            maps = {
                k : {'last_c' : 0, 'last_h' : 0, 'c' : None, 'h' : None, 'op' : ops[i]}
                for i, k in enumerate(['concat_max', 'concat_min', 'concat_mean']) if getattr(self, k)
            }
            full_emb = self.drop(self.encoder(input))
            for i in range(input.size(0)):
                emb = full_emb[i]
                out, hidden = self.rnn(emb.unsqueeze(0), collect_hidden=True)
                if self.aux_lm_loss:
                    outs.append(out)
                # print(hidden)                 -> [[[tensor(...)]], [[tensor(...)]]]
                # print(hidden[0][0][0].size()) -> torch.Size([128, 4096])

                cell = self.get_features(hidden)
                if i == 0: # instantiate pools for cell
                    for k, d in maps.items():
                        d['c'] = cell
                if get_hidden:
                    hidden = self.get_features(hidden, get_hidden=True)
                    if i == 0: # instantiate pools for hidden
                        for k, d in maps.items():
                            d['h'] = hidden
                if i > 0:
                    cell = get_valid_outs(i, seq_len, cell, last_cell)
                    for k, d in maps.items():
                        d['c'] = getattr(torch, d['op'])(d['c'], cell)
                        d['c'] = get_valid_outs(i, seq_len, d['c'], d['last_c'])
                    if get_hidden:
                        for k, d in maps.items():
                            d['h'] = getattr(torch, d['op'])(d['h'], hidden)
                            d['h'] = get_valid_outs(i, seq_len, d['h'], d['last_h'])
                        hidden = get_valid_outs(i, seq_len, hidden, last_hidden)
                last_cell = cell
                for k, d in maps.items():
                    d['last_c'] = d['c']
                if get_hidden:
                    last_hidden = hidden
                    for k, d in maps.items():
                        d['last_h'] = d['h']
            # print("Cell dimensions: ", cell.size()) -> torch.Size([128, 4096])
            seq_len = seq_len.view(-1, 1).float()
            if self.concat_mean:
                maps['concat_mean']['c'] /= seq_len
                if get_hidden:
                    maps['concat_mean']['h'] /= seq_len
            for k, d in maps.items():
                cell = torch.cat([cell, d['c']], -1)
                if get_hidden:
                    hidden = torch.cat([hidden, d['h']], -1)
            if get_hidden:
                cell = (hidden, cell)
        if self.aux_lm_loss:
            return cell, self.decoder(torch.cat(outs, 0))
        else:
            return cell, None

    def get_features(self, hidden, get_hidden=False):
        if not get_hidden:
            cell = hidden[1]
        else:
            cell = hidden[0]
        #get cell state from layers
        cell = cell[0]
        if self.all_layers:
            return torch.cat(cell, -1)
        else:
            return cell[-1]

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        sd = {}
        sd['encoder'] = self.encoder.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd['rnn'] = self.rnn.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd = {'encoder': sd}
        if self.aux_lm_loss:
            sd['decoder'] = self.decoder.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        return sd

    def load_state_dict(self, state_dict, strict=True):
        self.encoder.load_state_dict(state_dict['encoder']['encoder'], strict=strict)
        self.rnn.load_state_dict(state_dict['encoder']['rnn'], strict=strict)
        if self.aux_lm_loss:
            self.decoder.load_state_dict(state_dict['decoder'], strict=strict)

def get_valid_outs(timestep, seq_len, out, last_out):
    invalid_steps = timestep >= seq_len
    if (invalid_steps.long().sum() == 0):
        return out
    return selector_circuit(out, last_out, invalid_steps)

def selector_circuit(val0, val1, selections):
    selections = selections.type_as(val0.data).view(-1, 1).contiguous()
    return (val0*(1-selections)) + (val1*selections)

class TransformerDecoderModel(nn.Module):
    """Base class for encoder-decoder models."""

    def __init__(self, args):
        super().__init__()
        self._is_generation_fast = False
        self.encoder = TransformerDecoder(args, Embedding(args.data_size, args.decoder_embed_dim, padding_idx=args.padding_idx))

    def forward(self, src_tokens, get_attention=True, chkpt_grad=False, **kwargs):
        decoder_out, attn = self.encoder(src_tokens, src_tokens, chkpt_grad=chkpt_grad)
        if get_attention:
            return decoder_out, attn
        return decoder_out

    def max_positions(self):
        """Maximum length supported by the model."""
        return self.encoder.max_positions()

    def get_targets(self, sample, net_output):
        """Get targets from either the sample or the net's output."""
        return sample['target']

    def get_normalized_probs(self, net_output, log_probs, sample=None):
        """Get normalized probabilities (or log probs) from a net's output."""
        return self.encoder.get_normalized_probs(net_output, log_probs, sample)

    def max_decoder_positions(self):
        """Maximum length supported by the decoder."""
        return self.encoder.max_positions()

    def load_state_dict(self, state_dict, strict=True):
        """Copies parameters and buffers from state_dict into this module and
        its descendants.

        Overrides the method in nn.Module; compared with that method this
        additionally "upgrades" state_dicts from old checkpoints.
        """
        state_dict = self.upgrade_state_dict(state_dict)
        super().load_state_dict(state_dict, strict)

    def upgrade_state_dict(self, state_dict):
        assert state_dict is not None

        def do_upgrade(m):
            if m != self and hasattr(m, 'upgrade_state_dict'):
                m.upgrade_state_dict(state_dict)

        self.apply(do_upgrade)
        sd = {}
        for k,v in state_dict.items():
            if k.startswith('decoder'):
                k = k.replace('decoder', 'encoder')
            sd[k] = v
        return sd

    def make_generation_fast_(self, **kwargs):
        """Optimize model for faster generation."""
        if self._is_generation_fast:
            return  # only apply once
        self._is_generation_fast = True

        # remove weight norm from all modules in the network
        def apply_remove_weight_norm(module):
            try:
                nn.utils.remove_weight_norm(module)
            except ValueError:  # this module didn't have weight norm
                return

        self.apply(apply_remove_weight_norm)

        def apply_make_generation_fast_(module):
            if module != self and hasattr(module, 'make_generation_fast_'):
                module.make_generation_fast_(**kwargs)

        self.apply(apply_make_generation_fast_)

        def train(mode):
            if mode:
                raise RuntimeError('cannot train after make_generation_fast')

        # this model should no longer be used for training
        self.eval()
        self.train = train

class TransformerFeaturizer(nn.Module):
    def __init__(self, get_lm_out, args):
        super(TransformerFeaturizer, self).__init__()
        args.use_final_embed = True
        self.encoder = TransformerDecoderModel(args)
        self.aux_lm_loss = get_lm_out

    def forward(self, input, seq_len=None, get_hidden=False, chkpt_grad=False, **kwargs):
        encoder_out = self.encoder(input, get_attention=get_hidden, chkpt_grad=chkpt_grad, **kwargs)
        if get_hidden:
            encoder_out = encoder_out[0]
        feats = encoder_out[seq_len.squeeze(), torch.arange(seq_len.size(0))]
        if get_hidden:
            feats = [feats, None]
        lm_out = None
        if self.aux_lm_loss:
            lm_out = F.linear(encoder_out, self.encoder.encoder.embed_out)
        return feats, lm_out

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        return self.encoder.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)

    def load_state_dict(self, state_dict, strict=True):
        return self.encoder.load_state_dict(state_dict, strict=strict)

# PROJECT: NVIDIA_sentiment-discovery FILE: model/sentiment_classifier.py
import torch
from torch import nn
import torch.nn.functional as F

import numpy as np
from itertools import chain
from .model import RNNFeaturizer, TransformerFeaturizer
from .transformer_utils import GeLU

class BinaryClassifier(nn.Module):
    def __init__(self, num_features=4096, **kwargs):
        super().__init__()
        self.nclasses = 2
        self.dense0 = nn.Linear(num_features, 1)
        self.neurons = None
        self.thresholds = torch.tensor(np.array([.5])).float()
        self.final = 1
        self.device = -1
        print('init BinaryClassifier with %d features' % num_features)

    def cuda(self, device=None):
        super(BinaryClassifier, self).cuda(device)
        self.device = device
        self.thresholds = self.thresholds.cuda(device)

    def cpu(self):
        super(BinaryClassifier, self).cpu()
        self.device = -1
        self.thresholds = self.thresholds.cpu()

    def forward(self, X, **kwargs):
        out = torch.sigmoid(self.linear(X)).float()
        return threshold_predictions(out, self.thresholds)
        #return F.sigmoid(self.linear(X), dim=-1).float()

    def linear(self, X):
        weight = self.dense0.weight
        if self.neurons is not None:
            #weight = weight[torch.arange(weight.size(0)).unsqueeze(1), self.neurons].contiguous()
            weight = weight[:, self.neurons].contiguous()
            if X.size(-1) == self.dense0.weight.size(-1):
                X = X[:, self.neurons].contiguous()
            torch.cuda.synchronize()
        return F.linear(X, weight, self.dense0.bias)

    def set_neurons(self, num_neurons=None):
        if num_neurons is None:
            self.neurons = None
            return self.get_neurons()
        neurons, values = self.get_neurons(num_neurons=num_neurons)
        self.neurons = neurons
        return neurons, values

    def get_neurons(self, num_neurons=None):
        if num_neurons is None:
            return self.dense0.weight
        values, neurons = torch.topk(self.dense0.weight.abs().float(), num_neurons, 1)
        neurons = neurons[0]
        values = self.dense0.weight[:, neurons]
        return neurons, values

    def get_thresholds(self):
        return self.thresholds

    def set_thresholds(self, thresholds, **kwargs):
        if isinstance(thresholds, float):
            thresholds = [thresholds]
        if isinstance(thresholds, (list, tuple, np.ndarray, np.generic)):
            thresholds = torch.tensor(thresholds).float()
        if self.device == -1:
            thresholds = thresholds.cpu()
        else:
            thresholds = thresholds.cuda(self.device)
        self.thresholds = thresholds
        return None

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        sd = self.dense0.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd['neurons'] = self.neurons
        sd['thresholds'] = self.get_thresholds()
        return sd

    def load_state_dict(self, state_dict, strict=True):
        if 'neurons' in state_dict:
            self.neurons = state_dict['neurons']

        if 'thresholds' in state_dict:
            self.set_thresholds(state_dict['thresholds'])

        sd = {}
        for k, v in state_dict.items():
            if k != 'neurons' and k != 'thresholds':
                sd[k] = v

        self.dense0.load_state_dict(sd, strict=strict)

NONLINEARITY_MAP = {
    'prelu': nn.PReLU,
    'relu': nn.ReLU,
    'elu': nn.ELU,
    'selu': nn.SELU,
    'leaky': nn.LeakyReLU,
    'gelu': GeLU
}

class MultiLayerBinaryClassifier(nn.Module):
    def __init__(self, input_layer_size, layer_sizes, dropout=0.1, init_dropout=True, heads_per_class=1,
                 nonlinearity='PReLU', softmax=False, double_threshold=False, dual_threshold=False, **kwargs):
        super(MultiLayerBinaryClassifier, self).__init__()
        self.heads_per_class = heads_per_class
        self.nclasses = int(layer_sizes[-1])
        self.thresholds = torch.tensor(np.array([.5]*self.nclasses)).float()
        self.double_threshold = double_threshold
        self.dual_threshold = dual_threshold
        self.device = -1
        if self.heads_per_class > 1:
            print('Using multiple heads per class: %d' % heads_per_class)
            layer_sizes = list(layer_sizes)
            layer_sizes[-1] = int(layer_sizes[-1]) * heads_per_class
        self.neurons = None
        self.layer_sizes = [input_layer_size] + list(map(int, layer_sizes))
        self.final = self.layer_sizes[-1]
        self.dropout = dropout
        assert nonlinearity.lower() in NONLINEARITY_MAP
        self.nonlinearity = NONLINEARITY_MAP[nonlinearity.lower()]()
        # layer_sizes are sizes of the input and hidden layers, so the final 1 is assumed.
        layer_list = []
        # Since we recieve input from the Transformer bottleneck... it may make sense to dropout on that input first
        if init_dropout:
            layer_list.extend([nn.Dropout(p=self.dropout)])
        layer_list.extend(list(chain.from_iterable(
            [[nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1]), self.nonlinearity, nn.Dropout(p=self.dropout)] for i in range(len(self.layer_sizes) - 2)]
        )))
        self.final_layer = nn.Linear(*self.layer_sizes[-2:])
        extend_list = [self.final_layer]
        if not softmax:
            extend_list += [nn.Sigmoid()]
        layer_list.extend(extend_list)

        self.model = nn.Sequential(*layer_list)
        self.neurons = None
        self.softmax = softmax

        print('init MultiLayerBinaryClassifier with layers %s and dropout %s' % (self.layer_sizes[1:], self.dropout))

    def forward(self, X, **kwargs):
        out = self.model(X).float()
        if self.heads_per_class <= 1:
            if self.softmax:
                clf_out = torch.max(out, -1, keepdim=True)[1]
            else:
                out, clf_out = threshold_predictions(out, self.thresholds, double_threshold=self.double_threshold,
                                                     dual_threshold=self.dual_threshold)
            return out, clf_out
        out = out.view(out.size(0), -1, self.heads_per_class)
        probs = out
        if self.softmax:
            probs = F.softmax(probs, 1)
        clf_mean = probs.mean(dim=2)
        if self.softmax:
            clf_out = torch.max(clf_mean, -1, keepdim=True)[1]
        else:
            clf_mean, clf_out = threshold_predictions(clf_mean, self.thresholds, double_threshold=self.double_threshold,
                                                      dual_threshold=self.dual_threshold)
        clf_std = probs.std(dim=2)
        return out, clf_mean, clf_std, clf_out

    # HACK -- parameter to measure *variation* between last layer of the MLP.
    # Why? To support multihead -- for the same category, where we want multiple heads to predict with different functions
    # (similar to training a mixture of models) -- useful for uncertainty sampling
    def get_last_layer_variance(self, eps=.00001):
        final_layer_weight = self.final_layer.weight
        fl_norm = torch.norm(final_layer_weight,2,1)
        final_layer_weight = final_layer_weight * (1.0 / fl_norm).unsqueeze(1)
        final_layer_dot = final_layer_weight @ torch.transpose(final_layer_weight, 0, 1)
        # Compute matrix of all NxN layers -- in normalized form
        final_layer_dot_loss = (torch.norm(final_layer_dot,2,1) - 1.)
        final_layer_dot_loss = final_layer_dot_loss/(self.final_layer.weight.shape[0]+eps)
        final_layer_dot_loss = torch.sum(final_layer_dot_loss)
        # Return the average loss -- per dot comparison.
        return final_layer_dot_loss 

    def cuda(self, device=None):
        super(MultiLayerBinaryClassifier, self).cuda(device)
        self.device = device
        self.thresholds = self.thresholds.cuda(device)

    def cpu(self):
        super(MultiLayerBinaryClassifier, self).cpu()
        self.device = -1
        self.thresholds = self.thresholds.cpu()

    def get_thresholds(self):
        return self.thresholds

    def set_thresholds(self, thresholds, double_threshold=False, dual_threshold=False, **kwargs):
        self.dual_threshold = dual_threshold
        self.double_threshold = double_threshold
        if isinstance(thresholds, float):
            thresholds = [thresholds]
        if isinstance(thresholds, (list, tuple, np.ndarray, np.generic)):
            thresholds = torch.tensor(thresholds).float()
        if self.device == -1:
            thresholds = thresholds.cpu()
        else:
            thresholds = thresholds.cuda(self.device)
        self.thresholds = thresholds
        return None

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        sd = {}
        sd['sd'] = super(MultiLayerBinaryClassifier, self).state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd['thresholds'] = self.get_thresholds()
        sd['dual_threshold'] = self.dual_threshold
        sd['double_threshold'] = self.double_threshold
        return sd

    def load_state_dict(self, state_dict, strict=True):
        double_threshold = state_dict['double_threshold'] if 'double_threshold' in state_dict else False
        dual_threshold = state_dict['dual_threshold'] if 'dual_threshold' in state_dict else False
        self.set_thresholds(state_dict['thresholds'], double_threshold, dual_threshold)
        return super(MultiLayerBinaryClassifier, self).load_state_dict(state_dict['sd'], strict=strict)        

    def get_neurons(self, *args, **kwargs):
        return None

    def set_neurons(self, *args, **kwargs):
        return None

def threshold_predictions(predictions, thresholds, double_threshold=False, dual_threshold=False):
    if double_threshold:
        positive = (predictions > thresholds.max()).float()
        neutral = ((1-positive) * (predictions > thresholds.min()).float())*.5
        return predictions, (positive+neutral)
    preds = (predictions > thresholds).float()
    if dual_threshold:
        positive = preds[:,0]
        negative = preds[:,1]
        equals = (positive==negative).float().view(-1,1)
        preds = torch.cat([preds*(1-equals), equals.view(-1,1)], dim=-1)
        predictions = torch.cat([predictions, XOR(predictions[:,0], predictions[:,1]).view(-1, 1)], dim=-1)
        # print(preds, predictions)
        # exit()
    return predictions, preds

def XOR(A, B):
    return A+B-(2*A*B)

class MultiHeadBCELoss(torch.nn.BCELoss):
    def __init__(self, weight=None, size_average=None, reduce=None, reduction='elementwise_mean', heads_per_class=1):
        super(MultiHeadBCELoss, self).__init__(weight=weight, size_average=size_average, reduce=reduce, reduction=reduction)
        self.heads_per_class = heads_per_class

    def forward(self, input, target):
        input = input.permute(0, 2, 1)
        target = target.unsqueeze(1).expand(-1, self.heads_per_class, -1)
        return super(MultiHeadBCELoss, self).forward(input, target)

class MultiHeadCrossEntropyLoss(torch.nn.CrossEntropyLoss):
    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='elementwise_mean'):
        super(MultiHeadCrossEntropyLoss, self).__init__(weight=weight, size_average=size_average, reduce=reduce, reduction=reduction, ignore_index=ignore_index)
        self.heads_per_class = heads_per_class

    def forward(self, input, target):
        input = input.permute(0, 2, 1)
        target = target.unsqueeze(1).expand(-1, self.heads_per_class)
        return super(MultiHeadCrossEntropyLoss, self).forward(input, target)

class SentimentClassifier(nn.Module):
    """Container module with an encoder, a recurrent module, and a decoder."""

    def __init__(self, model_type, ntoken, ninp, nhid, nlayers, classifier_hidden_layers=None, dropout=0.5, all_layers=False, concat_pools=[False] * 3, get_lm_out=False, args=None):
        super().__init__()
        self.model_type = model_type
        if model_type == 'transformer':
            self.lm_encoder = TransformerFeaturizer(get_lm_out, args)
            out_size = args.decoder_embed_dim
        else:
            # NOTE: Dropout is for Classifier. Add separate RNN dropout or via params, if needed.
            self.lm_encoder = RNNFeaturizer(model_type, ntoken, ninp, nhid, nlayers, dropout=0.0, all_layers=all_layers,
                                         concat_pools=concat_pools, get_lm_out=get_lm_out, hidden_warmup=args.num_hidden_warmup > 0)
            out_size = self.lm_encoder.output_size
        self.encoder_dim = out_size

        if classifier_hidden_layers is None:
            self.classifier = BinaryClassifier(num_features=self.encoder_dim, double_threshold=args.double_thresh, dual_threshold=args.dual_thresh)
        else:
            self.classifier = MultiLayerBinaryClassifier(self.encoder_dim, classifier_hidden_layers, dropout=dropout, heads_per_class=args.heads_per_class,
                                                         softmax=args.use_softmax, double_threshold=args.double_thresh, dual_threshold=args.dual_thresh and not args.joint_binary_train)
        self.out_dim = self.classifier.final
        self.nclasses = self.classifier.nclasses
        self.neurons_ = None
        self.thresholds = self.classifier.thresholds
        # If we want to output multiple heads, and average the output
        self.heads_per_class = args.heads_per_class

    def cuda(self, device=None):
        self.lm_encoder.cuda(device)
        self.classifier.cuda(device)
        return self

    def cpu(self):
        self.lm_encoder.cpu()
        self.classifier.cpu()
        return self


    def forward(self, input, seq_len=None, get_hidden=False):
        hidden, lm_out = self.lm_encoder(input, seq_len, get_hidden)
        if get_hidden:
            hidden = hidden[0]
        if self.neurons is not None:
            hidden = hidden[:, self.neurons].contiguous()
        classifier_in = hidden
        class_out = self.classifier(classifier_in)

        return class_out, (lm_out, classifier_in)

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        sd = {}
        sd['lm_encoder'] = self.lm_encoder.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        sd['classifier'] = self.classifier.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        return sd

    def load_state_dict(self, state_dict, strict=True):
        self.lm_encoder.load_state_dict(state_dict['lm_encoder'], strict=strict)
        self.classifier.load_state_dict(state_dict['classifier'], strict=strict)
        self.neurons = self.classifier.neurons
        self.thresholds = self.classifier.thresholds

    def get_thresholds(self):
        return self.classifier.get_thresholds()

    def set_thresholds(self, thresholds, double_threshold=False, dual_threshold=False):
        rtn = self.classifier.set_thresholds(thresholds, double_threshold=double_threshold,
                                             dual_threshold=dual_threshold)
        self.thresholds = self.classifier.thresholds
        return rtn

    def get_neurons(self, **kwargs):
        return self.classifier.get_neurons(**kwargs)

    def set_neurons(self, num_neurons=None):
        rtn = self.classifier.set_neurons(num_neurons=num_neurons)
        self.neurons_ = self.classifier.neurons
        return rtn

    @property
    def neurons(self):
        return self.neurons_

    @neurons.setter
    def neurons(self, val):
        self.neurons_ = val
        self.classifier.neurons = val

# PROJECT: NVIDIA_sentiment-discovery FILE: model/distributed.py
import torch
from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
import torch.distributed as dist
from torch.nn.modules import Module
from torch.autograd import Variable


class DistributedDataParallel(Module):

    def __init__(self, module):
        super(DistributedDataParallel, self).__init__()
        self.warn_on_half = True if dist._backend == dist.dist_backend.GLOO else False

        self.module = module

        for p in self.module.parameters():
            if torch.is_tensor(p):
                dist.broadcast(p, 0)

        def allreduce_params():
            if(self.needs_reduction):
                self.needs_reduction = False
                buckets = {}
                for name, param in self.module.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        tp = type(param.data)
                        if tp not in buckets:
                            buckets[tp] = []
                        buckets[tp].append(param)
                if self.warn_on_half:
                    if torch.cuda.HalfTensor in buckets:
                        print("WARNING: gloo dist backend for half parameters may be extremely slow." +
                              " It is recommended to use the NCCL backend in this case.")
                        self.warn_on_half = False

                for tp in buckets:
                    bucket = buckets[tp]
                    grads = [param.grad.data for param in bucket]
                    coalesced = _flatten_dense_tensors(grads)
                    dist.all_reduce(coalesced)
                    torch.cuda.synchronize()
                    coalesced /= dist.get_world_size()
                    for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):
                        buf.copy_(synced)
        self.hook_handles = []
        self.hooks = []
        for param in list(self.module.parameters()):
            def allreduce_hook(*unused):
                Variable._execution_engine.queue_callback(allreduce_params)
        #    handle = param.register_hook(allreduce_hook)
            #self.hooks.append(allreduce_hook)
            #self.hook_handles.append(handle)
        self.allreduce_params = allreduce_params

    def forward(self, *inputs, **kwargs):
        self.needs_reduction = True
        return self.module(*inputs, **kwargs)

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        #[h.remove() for h in self.hook_handles]
        sd = self.module.state_dict(destination, prefix, keep_vars)
       # for handle, hook in zip(self.hook_handles, self.hooks):
       #     d = handle.hooks_dict_ref()
       #     d[handle.id] = hook

        return sd

    def load_state_dict(self, state_dict, strict=True):
        self.module.load_state_dict(state_dict, strict=strict)

    '''
    def _sync_buffers(self):
        buffers = list(self.module._all_buffers())
        if len(buffers) > 0:
            # cross-node buffer sync
            flat_buffers = _flatten_dense_tensors(buffers)
            dist.broadcast(flat_buffers, 0)
            for buf, synced in zip(buffers, _unflatten_dense_tensors(flat_buffers, buffers)):
                buf.copy_(synced)
    def train(self, mode=True):
        # Clear NCCL communicator and CUDA event cache of the default group ID,
        # These cache will be recreated at the later call. This is currently a
        # work-around for a potential NCCL deadlock.
        if dist._backend == dist.dist_backend.NCCL:
            dist._clear_group_cache()
        super(DistributedDataParallel, self).train(mode)
        self.module.train(mode)
    '''


# PROJECT: NVIDIA_sentiment-discovery FILE: model/transformer.py
###############################################################################
# BSD 3-Clause License
#
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#   
# Copyright (c) 2017, Facebook, inc. All rights reserved.
###############################################################################
'''
Code adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py
Introduced optimal gradient checkpointing for intermediate layers
'''


import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from .transformer_utils import *
import torch.utils.checkpoint as checkpoint

class TransformerModel(nn.Module):
    """Base class for encoder-decoder models."""

    def __init__(self, encoder, decoder):
        super().__init__()
        self._is_generation_fast = False
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src_tokens, get_attention=True, **kwargs):
        encoder_out = self.encoder(src_tokens)
        decoder_out, attn = self.decoder(src_tokens, encoder_out)
        if get_attention:
            return decoder_out, attn
        return decoder_out

    def max_positions(self):
        """Maximum length supported by the model."""
        return (self.encoder.max_positions(), self.decoder.max_positions())

    def get_targets(self, sample, net_output):
        """Get targets from either the sample or the net's output."""
        return sample['target']

    def get_normalized_probs(self, net_output, log_probs, sample=None):
        """Get normalized probabilities (or log probs) from a net's output."""
        return self.decoder.get_normalized_probs(net_output, log_probs, sample)

    def max_decoder_positions(self):
        """Maximum length supported by the decoder."""
        return self.decoder.max_positions()

    def load_state_dict(self, state_dict, strict=True):
        """Copies parameters and buffers from state_dict into this module and
        its descendants.

        Overrides the method in nn.Module; compared with that method this
        additionally "upgrades" state_dicts from old checkpoints.
        """
        self.upgrade_state_dict(state_dict)
        super().load_state_dict(state_dict, strict)

    def upgrade_state_dict(self, state_dict):
        assert state_dict is not None

        def do_upgrade(m):
            if m != self and hasattr(m, 'upgrade_state_dict'):
                m.upgrade_state_dict(state_dict)

        self.apply(do_upgrade)

    def make_generation_fast_(self, **kwargs):
        """Optimize model for faster generation."""
        if self._is_generation_fast:
            return  # only apply once
        self._is_generation_fast = True

        # remove weight norm from all modules in the network
        def apply_remove_weight_norm(module):
            try:
                nn.utils.remove_weight_norm(module)
            except ValueError:  # this module didn't have weight norm
                return

        self.apply(apply_remove_weight_norm)

        def apply_make_generation_fast_(module):
            if module != self and hasattr(module, 'make_generation_fast_'):
                module.make_generation_fast_(**kwargs)

        self.apply(apply_make_generation_fast_)

        def train(mode):
            if mode:
                raise RuntimeError('cannot train after make_generation_fast')

        # this model should no longer be used for training
        self.eval()
        self.train = train


class DecoderPreprocessor(nn.Module):
    def __init__(self, args, embed_tokens, left_pad=True):
        super().__init__()

    def forward(self, src_tokens):
        return {
            'encoder_out': src_tokens,  # T x B x C
            'encoder_padding_mask': None,  # B x T
        }


class TransformerEncoder(nn.Module):
    """Transformer encoder."""

    def __init__(self, args, embed_tokens, left_pad=False):
        super().__init__()
        self.dropout = args.dropout

        embed_dim = embed_tokens.embedding_dim
        self.padding_idx = embed_tokens.padding_idx

        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(
            256, embed_dim, self.padding_idx,
            left_pad=left_pad,
            learned=args.encoder_learned_pos,
        )

        self.layers = nn.ModuleList([])
        self.layers.extend([
            TransformerEncoderLayer(args)
            for i in range(args.encoder_layers)
        ])

    def forward(self, src_tokens, **kwargs):
        # embed tokens and positions
        x = self.embed_scale * self.embed_tokens(src_tokens)
        x += self.embed_positions(src_tokens)
        x = F.dropout(x, p=self.dropout, training=self.training)

        # B x T x C -> T x B x C
        #x = x.transpose(0, 1)

        # compute padding mask
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        if not encoder_padding_mask.any():
            encoder_padding_mask = None

        # encoder layers
        for layer in self.layers:
            x = layer(x, encoder_padding_mask)

        return {
            'encoder_out': x,  # T x B x C
            'encoder_padding_mask': encoder_padding_mask,  # B x T
        }

    def reorder_encoder_out(self, encoder_out_dict, new_order):
        if encoder_out_dict['encoder_out'] is not None:
            encoder_out_dict['encoder_out'] = \
                encoder_out_dict['encoder_out'].index_select(1, new_order)
        if encoder_out_dict['encoder_padding_mask'] is not None:
            encoder_out_dict['encoder_padding_mask'] = \
                encoder_out_dict['encoder_padding_mask'].index_select(0, new_order)
        return encoder_out_dict

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        return self.embed_positions.max_positions()

    def upgrade_state_dict(self, state_dict):
        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):
            if 'encoder.embed_positions.weights' in state_dict:
                del state_dict['encoder.embed_positions.weights']
            if 'encoder.embed_positions._float_tensor' not in state_dict:
                state_dict['encoder.embed_positions._float_tensor'] = torch.FloatTensor()
        return state_dict


class TransformerEncoderLayer(nn.Module):
    """Encoder layer block.

    In the original paper each operation (multi-head attention or FFN) is
    postprocessed with: dropout -> add residual -> layernorm.
    In the tensor2tensor code they suggest that learning is more robust when
    preprocessing each layer with layernorm and postprocessing with:
    dropout -> add residual.
    We default to the approach in the paper, but the tensor2tensor approach can
    be enabled by setting `normalize_before=True`.
    """

    def __init__(self, args):
        super().__init__()
        self.embed_dim = args.encoder_embed_dim
        self.self_attn = MultiheadAttention(
            self.embed_dim, args.encoder_attention_heads,
            dropout=args.attention_dropout,
        )
        self.dropout = args.dropout
        self.relu_dropout = args.relu_dropout
        self.normalize_before = args.encoder_normalize_before
        self.fc1 = Linear(self.embed_dim, args.encoder_ffn_embed_dim)
        self.fc2 = Linear(args.encoder_ffn_embed_dim, self.embed_dim)
        self.layer_norms = nn.ModuleList([LayerNorm(self.embed_dim) for i in range(2)])

    def forward(self, x, encoder_padding_mask):
        residual = x
        x = self.maybe_layer_norm(0, x, before=True)
        x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = self.maybe_layer_norm(0, x, after=True)

        residual = x
        x = self.maybe_layer_norm(1, x, before=True)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.relu_dropout, training=self.training)
        x = self.fc2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = self.maybe_layer_norm(1, x, after=True)
        return x

    def maybe_layer_norm(self, i, x, before=False, after=False):
        assert before ^ after
        if after ^ self.normalize_before:
            return self.layer_norms[i](x)
        else:
            return x


class TransformerDecoder(nn.Module):
    """Transformer decoder."""

    def __init__(self, args, embed_tokens, left_pad=False):
        super().__init__()
        self.dropout = args.dropout
        self.share_input_output_embed = args.share_decoder_input_output_embed

        num_tokens = embed_tokens.num_embeddings
        embed_dim = embed_tokens.embedding_dim
        padding_idx = embed_tokens.padding_idx

        if hasattr(args, 'mos') and (args.mos or args.mos_reduce_dim is not None):
            assert not args.use_final_embed
            self.mos_layer = MixtureOfSoftmax(
                input_size=embed_dim, output_size=num_tokens, reduce_dim_size=args.mos_reduce_dim,
                num_experts=args.mos_num_experts, dropout=0.1, dropoutl=0.1
            )

        self.use_final_embed = args.use_final_embed
        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(
            256, embed_dim, padding_idx,
            left_pad=left_pad,
            learned=args.decoder_learned_pos,
        )

        self.layers = nn.ModuleList([])
        self.layers.extend([
            TransformerDecoderLayer(args)
            for i in range(args.decoder_layers)
        ])

        if not self.share_input_output_embed:
            self.embed_out = nn.Parameter(torch.Tensor(num_tokens, embed_dim))
            nn.init.normal_(self.embed_out, mean=0, std=embed_dim ** -0.5)

    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, chkpt_grad=False, **kwargs):
        # embed positions
        positions = self.embed_positions(
            prev_output_tokens,
            incremental_state=incremental_state,
        )

        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            positions = positions[:, -1:]

        # embed tokens and positions
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)

        # B x T x C -> T x B x C
        #x = x.transpose(0, 1)

        def custom(start, end):
            def custom_forward(*inputs):
                layers = self.layers[start:end]
                x_ = inputs[0]
                for layer in layers:
                    x_, attn = layer(x_, None, None, None)
                return x_
            return custom_forward

        if self.training and chkpt_grad:
            l = 0
            num_layers = len(self.layers)
            chunk_length = math.ceil(math.sqrt(num_layers))
            while l < num_layers:
                x = checkpoint.checkpoint(custom(l, l+chunk_length), x)
                l += chunk_length
            attn = None
            # decoder layers
        else:
            for layer in self.layers:
                x, attn = layer(x, None, None, None)

        # T x B x C -> B x T x C
        #x = x.transpose(0, 1)

        # project back to size of vocabulary
        if self.share_input_output_embed:
            x = F.linear(x, self.embed_tokens.weight)
        elif not self.use_final_embed:
            if hasattr(self, 'mos_layer'):
                x = self.mos_layer(x)
            else:
                x = F.linear(x, self.embed_out)

        return x, attn

    def get_normalized_probs(self, net_output, log_probs, _):
        """Get normalized probabilities (or log probs) from a net's output."""
        logits = net_output[0].float()
        if log_probs:
            return F.log_softmax(logits, dim=-1)
        else:
            return F.softmax(logits, dim=-1)

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        return self.embed_positions.max_positions()

    def upgrade_state_dict(self, state_dict):
        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):
            if 'decoder.embed_positions.weights' in state_dict:
                del state_dict['decoder.embed_positions.weights']
            if 'decoder.embed_positions._float_tensor' not in state_dict:
                state_dict['decoder.embed_positions._float_tensor'] = torch.FloatTensor()
        return state_dict


class TransformerDecoderLayer(nn.Module):
    """Decoder layer block."""

    def __init__(self, args):
        super().__init__()
        self.GeLU = GeLU()
        self.embed_dim = args.decoder_embed_dim
        self.self_attn = MultiheadAttention(
            self.embed_dim, args.decoder_attention_heads,
            dropout=args.attention_dropout,
        )
        self.dropout = args.dropout
        self.relu_dropout = args.relu_dropout
        self.normalize_before = args.decoder_normalize_before
        self.encoder_attn = MultiheadAttention(
            self.embed_dim, args.decoder_attention_heads,
            dropout=args.attention_dropout,
        )
        self.fc1 = Linear(self.embed_dim, args.decoder_ffn_embed_dim)
        self.fc2 = Linear(args.decoder_ffn_embed_dim, self.embed_dim)
        self.layer_norms = nn.ModuleList([LayerNorm(self.embed_dim) for i in range(2)])

    def forward(self, x, encoder_out, encoder_padding_mask, incremental_state):
        residual = x
        x = self.maybe_layer_norm(0, x, before=True)
        x, attn = self.self_attn(
            query=x,
            key=x,
            value=x,
            mask_future_timesteps=True,
            incremental_state=incremental_state,
            need_weights=False,
        )
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = self.maybe_layer_norm(0, x, after=True)

        residual = x
        x = self.maybe_layer_norm(1, x, before=True)
        x = self.GeLU(self.fc1(x))
        x = F.dropout(x, p=self.relu_dropout, training=self.training)
        x = self.fc2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = self.maybe_layer_norm(1, x, after=True)
        return x, attn

    def maybe_layer_norm(self, i, x, before=False, after=False):
        assert before ^ after
        if after ^ self.normalize_before:
            return self.layer_norms[i](x)
        else:
            return x


# PROJECT: NVIDIA_sentiment-discovery FILE: main.py
import argparse
import os
import sys
import time
import math
import torch
import torch.nn as nn
from torch.autograd import Variable


from fp16 import FP16_Module, FP16_Optimizer

import data
import model
from model import DistributedDataParallel as DDP

from apex.reparameterization import apply_weight_norm, remove_weight_norm
from configure_data import configure_data
from learning_rates import LinearLR

parser = argparse.ArgumentParser(description='PyTorch Sentiment-Discovery Language Modeling')
parser.add_argument('--model', type=str, default='mLSTM',
                    help='type of recurrent net (Tanh, ReLU, LSTM, mLSTM, GRU)')
parser.add_argument('--emsize', type=int, default=64,
                    help='size of word embeddings')
parser.add_argument('--nhid', type=int, default=4096,
                    help='number of hidden units per layer')
parser.add_argument('--nlayers', type=int, default=1,
                    help='number of layers')
parser.add_argument('--lr', type=float, default=5e-4,
                    help='initial learning rate')
parser.add_argument('--constant_decay', type=int, default=None,
                    help='number of iterations to decay LR over,' + \
                         ' None means decay to zero over training')
parser.add_argument('--clip', type=float, default=0,
                    help='gradient clipping')
parser.add_argument('--epochs', type=int, default=1,
                    help='upper epoch limit')
parser.add_argument('--dropout', type=float, default=0.0,
                    help='dropout applied to layers (0 = no dropout)')
parser.add_argument('--tied', action='store_true',
                    help='tie the word embedding and softmax weights')
parser.add_argument('--seed', type=int, default=1234,
                    help='random seed')
parser.add_argument('--log-interval', type=int, default=100, metavar='N',
                    help='report interval')
parser.add_argument('--save', type=str,  default='lang_model.pt',
                    help='path to save the final model')
parser.add_argument('--load', type=str, default='',
                    help='path to a previously saved model checkpoint')
parser.add_argument('--load_optim', action='store_true',
                    help='load most recent optimizer to resume training')
parser.add_argument('--save_iters', type=int, default=2000, metavar='N',
                    help='save current model progress interval')
parser.add_argument('--save_optim', action='store_true',
                    help='save most recent optimizer')
parser.add_argument('--fp16', action='store_true',
                    help='Run model in pseudo-fp16 mode (fp16 storage fp32 math).')
parser.add_argument('--dynamic_loss_scale', action='store_true',
                    help='Dynamically look for loss scalar for fp16 convergance help.')
parser.add_argument('--no_weight_norm', action='store_true',
                    help='Add weight normalization to model.')
parser.add_argument('--loss_scale', type=float, default=1,
                    help='Static loss scaling, positive power of 2 values can improve fp16 convergence.')
parser.add_argument('--world_size', type=int, default=1,
                    help='number of distributed workers')
parser.add_argument('--distributed_backend', default='gloo',
                    help='which backend to use for distributed training. One of [gloo, nccl]')
parser.add_argument('--rank', type=int, default=-1,
                    help='distributed worker rank. Typically set automatically from multiproc.py')
parser.add_argument('--base-gpu', type=int, default=0,
                    help='base gpu to use as gpu 0')
parser.add_argument('--optim', default='Adam',
                    help='One of PyTorch\'s optimizers (Adam, SGD, etc). Default: Adam')
parser.add_argument('--tcp-port', type=int, default=6000,
                   help='tcp port so as to avoid address already in use errors')

# Add dataset args to argparser and set some defaults
data_config, data_parser = configure_data(parser)
data_config.set_defaults(data_set_type='L2R', transpose=True)
data_parser.set_defaults(split='100,1,1')
data_parser = parser.add_argument_group('language modeling data options')
data_parser.add_argument('--seq_length', type=int, default=256,
                         help="Maximum sequence length to process (for unsupervised rec)")
data_parser.add_argument('--eval_seq_length', type=int, default=256,
                         help="Maximum sequence length to process for evaluation")
data_parser.add_argument('--lazy', action='store_true',
                         help='whether to lazy evaluate the data set')
data_parser.add_argument('--persist_state', type=int, default=1,
                         help='0=reset state after every sample in a shard, 1=reset state after every shard, -1=never reset state')
data_parser.add_argument('--num_shards', type=int, default=102,
                         help="""number of total shards for unsupervised training dataset. If a `split` is specified,
                                 appropriately portions the number of shards amongst the splits.""")
data_parser.add_argument('--val_shards', type=int, default=0,
                         help="""number of shards for validation dataset if validation set is specified and not split from training""")
data_parser.add_argument('--test_shards', type=int, default=0,
                         help="""number of shards for test dataset if test set is specified and not split from training""")
data_parser.add_argument('--train-iters', type=int, default=1000,
                        help="""number of iterations per epoch to run training for""")
data_parser.add_argument('--eval-iters', type=int, default=100,
                        help="""number of iterations per epoch to run validation/test for""")

args = parser.parse_args()

torch.backends.cudnn.enabled = False
args.cuda = torch.cuda.is_available()

# initialize distributed process group and set device
if args.rank > 0 or args.base_gpu != 0:
    torch.cuda.set_device((args.rank+args.base_gpu) % torch.cuda.device_count())

if args.world_size > 1:
    distributed_init_file = os.path.splitext(args.save)[0]+'.distributed.dpt'
    torch.distributed.init_process_group(backend=args.distributed_backend, world_size=args.world_size,
                                         init_method='tcp://localhost:{}'.format(args.tcp_port), rank=args.rank)
#                                                    init_method='file://'+distributed_init_file, rank=args.rank)

# Set the random seed manually for reproducibility.
if args.seed > 0:
    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

if args.loss_scale != 1 and args.dynamic_loss_scale:
    raise RuntimeError("Static loss scale and dynamic loss scale cannot be used together.")

###############################################################################
# Load data
###############################################################################

# Starting from sequential data, the unsupervised dataset type loads the corpus
# into rows. With the alphabet as the our corpus and batch size 4, we get
#  a b c d e f 
#  g h i j k l 
#  m n o p q r 
#  s t u v w x .
# These rows are treated as independent by the model, which means that the
# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient
# batch processing.
#
# The unsupervised dataset further splits the corpus into shards through which
# the hidden state is persisted. The dataset also produces a hidden state
# reset mask that resets the hidden state at the start of every shard. A valid
# mask might look like
#  1 0 0 0 0 0 ... 0 0 0 1 0 0 ... 
#  1 0 0 0 0 0 ... 0 1 0 0 0 0 ... 
#  1 0 0 0 0 0 ... 0 0 1 0 0 0 ... 
#  1 0 0 0 0 0 ... 1 0 0 0 0 0 ... .
# With 1 indicating to reset hidden state at that particular minibatch index
(train_data, val_data, test_data), tokenizer = data_config.apply(args)

###############################################################################
# Build the model
###############################################################################
args.data_size = tokenizer.num_tokens
ntokens = args.data_size
model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)
print('* number of parameters: %d' % sum([p.nelement() for p in model.parameters()]))
if args.cuda:
    model.cuda()

rnn_model = model

optim = None
if args.load != '':
    sd = torch.load(args.load, map_location='cpu')
    if args.load_optim:
        optim_sd = torch.load(os.path.join(os.path.dirname(args.load), 'optim.pt'), map_location='cpu')
        rng = torch.load(os.path.join(os.path.dirname(args.load), 'rng.pt'))
        torch.cuda.set_rng_state(rng[0])
        torch.set_rng_state(rng[1])
    try:
        model.load_state_dict(sd)
    except:
        apply_weight_norm(model.rnn, hook_child=False)
        model.load_state_dict(sd)
        remove_weight_norm(model.rnn)

if not args.no_weight_norm:
    apply_weight_norm(model, 'rnn', hook_child=False)

# create optimizer and fp16 models
if args.fp16:
    model = FP16_Module(model)
    optim = eval('torch.optim.'+args.optim)(model.parameters(), lr=args.lr)
    optim = FP16_Optimizer(optim,
                           static_loss_scale=args.loss_scale,	
                           dynamic_loss_scale=args.dynamic_loss_scale)
else:
    optim = eval('torch.optim.'+args.optim)(model.parameters(), lr=args.lr)

if args.load_optim:
    pass
    optim.load_state_dict(optim_sd)

# add linear learning rate scheduler
if train_data is not None:
    if args.constant_decay:
        num_iters = args.constant_decay
    else:
        num_iters = args.train_iters * args.epochs

    init_step = -1
    if args.load_optim:
        init_step = optim_sd['iter']-optim_sd['skipped_iter']
        train_data.batch_sampler.start_iter = (optim_sd['iter'] % len(train_data)) + 1

    LR = LinearLR(optim, num_iters, last_iter=init_step)

# wrap model for distributed training
if args.world_size > 1:
    model = DDP(model)

criterion = nn.CrossEntropyLoss()

###############################################################################
# Training code
###############################################################################

# get_batch subdivides the source data into chunks of length args.seq_length.
# If source is equal to the example output of the data loading example, with
# a seq_length limit of 2, we'd get the following two Variables for i = 0:
#  a g m s   b h n t 
#  b h n t   c i o u 
# Note that despite the name of the function, the subdivison of data is not
# done along the batch dimension (i.e. dimension 1), since that was handled
# by the data loader. The chunks are along dimension 0, corresponding
# to the seq_len dimension in the LSTM. A Variable representing an appropriate
# shard reset mask of the same dimensions is also returned.

def get_batch(data):
    reset_mask_batch = data[1].long()
    data = data[0].long()
    if args.cuda:
        data = data.cuda()
        reset_mask_batch = reset_mask_batch.cuda()
    text_batch = Variable(data[:, :-1].t().contiguous(), requires_grad=False)
    target_batch = Variable(data[:, 1:].t().contiguous(), requires_grad=False)
    reset_mask_batch = Variable(reset_mask_batch[:,:text_batch.size(0)].t().contiguous(), requires_grad=False)
    return text_batch, target_batch, reset_mask_batch

def init_hidden(batch_size):
    return rnn_model.rnn.init_hidden(args.batch_size)

def evaluate(data_source, max_iters):
    # Turn on evaluation mode which disables dropout.
    model.eval()
    init_hidden(args.batch_size)
    total_loss = 0
    ntokens = args.data_size
    with torch.no_grad():
        data_iter = iter(data_source)
        i = 0
        while i < max_iters:
            batch = next(data_iter)
            data, targets, reset_mask = get_batch(batch)
            output, hidden = model(data, reset_mask=reset_mask)
            output_flat = output.view(-1, ntokens).contiguous().float()
            loss = criterion(output_flat, targets.view(-1).contiguous())
            if isinstance(model, DDP):
                torch.distributed.all_reduce(loss.data)
                loss.data /= args.world_size
            total_loss += loss.data[0]
            i += 1
    return total_loss / max(max_iters, 1)

def train(max_iters, total_iters=0, skipped_iters=0, elapsed_time=False):
    # Turn on training mode which enables dropout.
    model.train()
    total_loss = 0
    start_time = time.time()
    t0 = start_time
    ntokens = args.data_size
    hidden = init_hidden(args.batch_size)
    curr_loss = 0.
    distributed = isinstance(model, DDP)
    def log(epoch, i, lr, ms_iter, total_time, loss, scale):
        print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:.2E} | ms/batch {:.3E} | total time {:.3E}\
                  loss {:.2E} | ppl {:8.2f} | loss scale {:8.2f}'.format(
                      epoch, i, max_iters, lr,
                      ms_iter, total_time, loss, math.exp(min(loss, 20)), scale
                  )
        )
    data_iter = iter(train_data)
    i = 0
    while i < max_iters:
        batch = next(data_iter)
        data, targets, reset_mask = get_batch(batch)
        optim.zero_grad()
        output, hidden = model(data, reset_mask=reset_mask)
        loss = criterion(output.view(-1, ntokens).contiguous().float(), targets.view(-1).contiguous())
        total_loss += loss.data.float()

        if args.fp16:
            optim.backward(loss, update_master_grads=False)
        else:
            loss.backward()

        if distributed:
            torch.distributed.all_reduce(loss.data)
            loss.data /= args.world_size
            model.allreduce_params()

        # clipping gradients helps prevent the exploding gradient problem in RNNs / LSTMs.
        if args.clip > 0:
            if not args.fp16:
                torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
            else:
                optim.clip_master_grads(clip=args.clip)

        if args.fp16:
            optim.update_master_grads()

        optim.step()

        # step learning rate and log training progress
        lr = optim.param_groups[0]['lr']
        if not args.fp16:
            LR.step()
        else:
            # if fp16 optimizer skips gradient step due to explosion do not step lr
            if not optim.overflow:
                LR.step()
            else:
                skipped_iters += 1

        # log current results
        if ((i+1) % args.log_interval == 0) and (i != max_iters - 1):
            cur_loss = total_loss[0] / args.log_interval
            cur_time = time.time()
            elapsed = cur_time - start_time
            total_elapsed = cur_time - t0 + elapsed_time
            log(epoch, i+1, lr, elapsed * 1000 / args.log_interval, total_elapsed, 
                cur_loss, args.loss_scale if not args.fp16 else optim.loss_scale)
            total_loss = 0
            start_time = cur_time
            sys.stdout.flush()

        # save current model progress. If distributed only save from worker 0
        if args.save_iters and total_iters % args.save_iters == 0 and total_iters > 0 and args.rank < 1:
            if args.rank < 1:
                with open(os.path.join(os.path.splitext(args.save)[0], 'e%s.pt'%(str(total_iters),)), 'wb') as f:
                    torch.save(model.state_dict(), f)
                if args.save_optim:
                    with open(os.path.join(os.path.splitext(args.save)[0], 'optim.pt'), 'wb') as f:
                        optim_sd = optim.state_dict()
                        optim_sd['iter'] = total_iters
                        optim_sd['skipped_iter'] = skipped_iters
                        torch.save(optim_sd, f)
                        del optim_sd

                    with open(os.path.join(os.path.splitext(args.save)[0], 'rng.pt'), 'wb') as f:
                        torch.save((torch.cuda.get_rng_state(), torch.get_rng_state()),f)
            if args.cuda:
                torch.cuda.synchronize()
        total_iters += 1
        i += 1
    #final logging
    elapsed_iters = max_iters % args.log_interval
    if elapsed_iters == 0:
        elapsed_iters = args.log_interval
    cur_loss = total_loss[0] / elapsed_iters
    cur_time = time.time()
    elapsed = cur_time - start_time
    total_elapsed = cur_time - t0 + elapsed_time
    log(epoch, max_iters, lr, elapsed * 1000/ elapsed_iters, total_elapsed,
        cur_loss, args.loss_scale if not args.fp16 else optim.loss_scale)

    return cur_loss, skipped_iters

# Loop over epochs.
lr = args.lr
best_val_loss = None

# If saving process intermittently create directory for saving
if args.save_iters > 0 and not os.path.exists(os.path.splitext(args.save)[0]) and args.rank < 1:
    os.makedirs(os.path.splitext(args.save)[0])

# At any point you can hit Ctrl + C to break out of training early.
try:
    total_iters = 0
    elapsed_time = 0
    skipped_iters = 0
    if args.load_optim:
        total_iters = optim_sd['iter']
        skipped_iters = optim_sd['skipped_iter']
    for epoch in range(1, args.epochs+1):
        epoch_start_time = time.time()
        val_loss, skipped_iters = train(args.train_iters, total_iters, skipped_iters, elapsed_time)
        elapsed_time += time.time() - epoch_start_time
        total_iters += args.train_iters
        if val_data is not None:
            print('entering eval')
            val_loss = evaluate(val_data, args.eval_iters)
        print('-' * 89)
        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
                                         val_loss, math.exp(min(val_loss, 20))))
        print('-' * 89)
        # Save the model if the validation loss is the best we've seen so far.
        if not best_val_loss or val_loss < best_val_loss and args.rank <= 0:
            torch.save(model.state_dict(), args.save)
            best_val_loss = val_loss

except KeyboardInterrupt:
    print('-' * 89)
    print('Exiting from training early')

# Load the best saved model.
if os.path.exists(args.save):
    model.load_state_dict(torch.load(args.save, 'cpu'))

if not args.no_weight_norm and args.rank <= 0:
    remove_weight_norm(rnn_model)
    with open(args.save, 'wb') as f:
        torch.save(model.state_dict(), f)

if test_data is not None:
    # Run on test data.
    test_loss = evaluate(test_data, args.eval_iters)
    print('=' * 89)
    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(
        test_loss, math.exp(test_loss)))
    print('=' * 89)

# PROJECT: NVIDIA_sentiment-discovery FILE: fp16/fp16.py
import torch
from torch import nn
from torch.autograd import Variable
from torch.nn.parameter import Parameter
from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors

from .loss_scaler import DynamicLossScaler, LossScaler
from .fp16util import model_grads_to_master_grads, master_params_to_model_params

FLOAT_TYPES = (torch.FloatTensor, torch.cuda.FloatTensor)
HALF_TYPES = (torch.HalfTensor, torch.cuda.HalfTensor)

def conversion_helper(val, conversion):
    """Apply conversion to val. Recursively apply conversion if `val` is a nested tuple/list structure."""
    if not isinstance(val, (tuple, list)):
        return conversion(val)
    rtn =  [conversion_helper(v, conversion) for v in val]
    if isinstance(val, tuple):
        rtn = tuple(rtn)
    return rtn

def fp32_to_fp16(val):
    """Convert fp32 `val` to fp16"""
    def half_conversion(val):
        val_typecheck = val
        if isinstance(val_typecheck, (Parameter, Variable)):
            val_typecheck = val.data
        if isinstance(val_typecheck, FLOAT_TYPES):
            val = val.half()
        return val
    return conversion_helper(val, half_conversion)

def fp16_to_fp32(val):
    """Convert fp16 `val` to fp32"""
    def float_conversion(val):
        val_typecheck = val
        if isinstance(val_typecheck, (Parameter, Variable)):
            val_typecheck = val.data
        if isinstance(val_typecheck, HALF_TYPES):
            val = val.float()
        return val
    return conversion_helper(val, float_conversion)

class FP16_Module(nn.Module):
    def __init__(self, module):
        super(FP16_Module, self).__init__()
        self.add_module('module', module.half())

    def forward(self, *inputs, **kwargs):
        return fp16_to_fp32(self.module(*(fp32_to_fp16(inputs)), **kwargs))

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        return self.module.state_dict(destination, prefix, keep_vars)

    def load_state_dict(self, state_dict, strict=True):
        self.module.load_state_dict(state_dict, strict=strict)

class FP16_Optimizer(object):
    """
    :class:`FP16_Optimizer` is designed to wrap an existing PyTorch optimizer, 
    and manage (dynamic) loss scaling and master weights in a manner transparent to the user.
    For standard use, only two lines must be changed:  creating the :class:`FP16_Optimizer` instance,
    and changing the call to ``backward``.

    Example::

        model = torch.nn.Linear(D_in, D_out).cuda().half()
        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
        # Name the FP16_Optimizer instance to replace the existing optimizer
        # (recommended but not required):
        optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)
        ...
        # loss.backward() becomes:
        optimizer.backward(loss)
        ...

    Example with dynamic loss scaling::

        ...
        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)
                                   # optional arg to control dynamic loss scaling behavior
                                   # dynamic_loss_args={'scale_window' : 500})
                                   # Usually, dynamic_loss_args is not necessary. 

    Args:
        init_optimizer (torch.optim.optimizer):  Existing optimizer created with the parameters to optimize.  Internally, :class:`FP16_Optimizer` replaces the passed optimizer's fp16 parameters, if any, with fp32 master parameters copied from the original ones.  :class:`FP16_Optimizer` also stores references to the original fp16 parameters, and updates these fp16 parameters from the master fp32 copy at the end of each :attr:`step`.  
        static_loss_scale (float, optional, default=1.0):  Loss scale used internally to scale gradients computed by the model.  Any fp16 gradients will be copied to fp32, then downscaled before being applied to the fp32 master params, so ``static_loss_scale`` should not affect learning rate.
        dynamic_loss_scale (bool, optional, default=False):  Use dynamic loss scaling.  If True, this will override any ``static_loss_scale`` option.
        dynamic_loss_args (dict, optional, default=None):  Dict of kwargs that will be forwarded to the internal :class:`DynamicLossScaler` instance's constructor.  Keys of this dict must match kwargs accepted by :class:`DynamicLossScaler`'s constructor.  If ``dynamic_loss_args`` is unspecified, :class:`DynamicLossScaler`'s defaults will be used.

    ``init_optimizer`` is expected to have been constructed in the ordinary way.  
    It is recommended (although not required) that the newly constructed :class:`FP16_Optimizer` instance be 
    named to replace ``init_optimizer``, for two reasons:  
    First, it means that references to the same name
    later in the file will not have to change.  
    Second, :class:`FP16_Optimizer` reserves the right (as an implementation detail) to 
    modify ``init_optimizer``.  If you do choose a unique name for the new
    :class:`FP16_Optimizer` instance, you should only work with this new instance,
    because the preexisting optimizer might no longer behave as expected.

    ``init_optimizer`` may be any Pytorch optimizer. 
    It may contain a mixture of fp16 and fp32 parameters organized into any number of 
    ``param_groups`` with different hyperparameters.  The :class:`FP16_Optimizer` constructor will 
    ingest these ``param_groups`` and remember them. 

    Calls to ::

        loss.backward() 

    must be replaced with ::

        optimizer.backward(loss)  

    because :class:`FP16_Optimizer` requires ownership of the backward pass to implement 
    loss scaling and copies to master gradients.

    .. note::
        Loss scaling, either static or dynamic, is orthogonal to learning rate, because gradients
        are downscaled before being applied.  This means that adjusting the loss scale, or using
        dynamic loss scaling, should not require retuning the learning rate or any other 
        hyperparameters.


    **Advanced options**

    **Closures**:  :class:`FP16_Optimizer` can wrap a Pytorch optimizer that receives a closure.
    See docstring for :attr:`step`.

    **Gradient clipping**:  Use :attr:`clip_master_grads`.
    
    **Multiple losses**:  If your model accumulates gradients from multiple losses,
    this can be made more efficient by supplying ``update_master_grads=False``
    to :attr:`backward`.  See docstring for :attr:`backward`.

    **Manually adjusting loss scale**:  The current loss scale can be retrieved or set via ::

        print(optimizer.loss_scale)
        optimizer.loss_scale = new_loss_scale

    For static loss scaling, manually adjusting the loss scale over time is a reasonable
    thing to do.  During later epochs, gradients may become smaller, and a 
    higher loss scale may be required, analogous to scheduling the learning rate.  Dynamic loss
    scaling is more subtle (see :class:`DynamicLossScaler`) and in this case, manually adjusting 
    the loss scale is not recommended.

    **Multi_GPU training**:  If the wrapped ``init_optimizer`` was created from a model wrapped in
    Pytorch DistributedDataParallel or Apex DistributedDataParallel, :class:`FP16_Optimizer` 
    should still work as intended.
    """

    def __init__(self, 
                 init_optimizer, 
                 static_loss_scale=1.0, 
                 dynamic_loss_scale=False,
                 dynamic_loss_args=None):
        if not torch.cuda.is_available:
            raise SystemError("Cannot use fp16 without CUDA.")

        self.fp16_groups = []
        self.fp32_from_fp16_groups = []
        self.fp32_from_fp32_groups = []
        for i, param_group in enumerate(init_optimizer.param_groups):
            print("FP16_Optimizer processing param group {}:".format(i))
            fp16_params_this_group = []
            fp32_params_this_group = []
            for param in param_group['params']:
                if param.requires_grad:
                    if param.type() == 'torch.cuda.HalfTensor':
                        print("FP16_Optimizer received torch.cuda.HalfTensor with {}"
                              .format(param.size()))
                        fp16_params_this_group.append(param)
                    elif param.type() == 'torch.cuda.FloatTensor':
                        print("FP16_Optimizer received torch.cuda.FloatTensor with {}"
                              .format(param.size()))
                        fp32_params_this_group.append(param)
                    else:
                        raise TypeError("Wrapped parameters must be either "
                                        "torch.cuda.FloatTensor or torch.cuda.HalfTensor. "  
                                        "Received {}".format(param.type()))
            
            fp32_from_fp16_params_this_group = [param.detach().clone().float() 
                                                for param in fp16_params_this_group]
            for param in fp32_from_fp16_params_this_group:
                param.requires_grad = True

            param_group['params'] = fp32_from_fp16_params_this_group + fp32_params_this_group

            self.fp16_groups.append(fp16_params_this_group)
            self.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)
            self.fp32_from_fp32_groups.append(fp32_params_this_group)

        self.optimizer = init_optimizer#.__class__(init_optimizer.param_groups)

        if dynamic_loss_scale:
            self.dynamic_loss_scale = True
            if dynamic_loss_args is not None:
                self.loss_scaler = DynamicLossScaler(**dynamic_loss_args)
            else:
                self.loss_scaler = DynamicLossScaler()
        else:
            self.dynamic_loss_scale = False
            self.loss_scaler = LossScaler(static_loss_scale)

        self.overflow = False
        self.first_closure_call_this_step = True

    def __getstate__(self):
        raise RuntimeError("FP16_Optimizer should be serialized using state_dict().")

    def __setstate__(self, state):
        raise RuntimeError("FP16_Optimizer should be deserialized using load_state_dict().")

    def zero_grad(self):
        """
        Zero fp32 and fp16 parameter grads.
        """
        # In principle, only the .grad attributes of the model params need to be zeroed,
        # because gradients are copied into the FP32 master params.  However, we zero
        # all gradients owned by the optimizer, just to be safe:
        self.optimizer.zero_grad()
        # Zero fp16 gradients owned by the model:
        for fp16_group in self.fp16_groups:
            for param in fp16_group:
                if param.grad is not None:
                    param.grad.detach_() # as in torch.optim.optimizer.zero_grad()
                    param.grad.zero_()

    def _check_overflow(self):
        params = [] 
        for group in self.fp16_groups:
            for param in group:
                params.append(param)
        for group in self.fp32_from_fp32_groups:
            for param in group:
                params.append(param)
        self.overflow = self.loss_scaler.has_overflow(params)

    def _update_scale(self, has_overflow=False):
        self.loss_scaler.update_scale(has_overflow)

    def _master_params_to_model_params(self):
        for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):
            master_params_to_model_params(fp16_group, fp32_from_fp16_group)

    # To consider:  Integrate distributed with this wrapper by registering a hook on each variable 
    # that does the overflow check, gradient copy + downscale, and fp32 allreduce in a different stream.
    def _model_grads_to_master_grads(self):
        for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):
            model_grads_to_master_grads(fp16_group, fp32_from_fp16_group)

    def _downscale_master(self):
        if self.loss_scale != 1.0: 
            for group in self.optimizer.param_groups:
                for param in group['params']:
                    if param.grad is not None:
                        param.grad.data.mul_(1./self.loss_scale)

    def clip_master_grads(self, max_norm, norm_type=2):
        """
        Clips fp32 master gradients via ``torch.nn.utils.clip_grad_norm``.

        Args:
            max_norm (float or int): max norm of the gradients
            norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for
                infinity norm.

        Returns:
            Total norm of the current fp32 gradients (viewed as a single vector).

        .. warning::
            Returns -1 if the most recently computed fp16 gradients overflowed (that is, if ``self.overflow`` is ``True``).
        """
        if not self.overflow:
            fp32_params = []
            for param_group in self.optimizer.param_groups:
                for param in param_group['params']:
                    fp32_params.append(param)
            return torch.nn.utils.clip_grad_norm(fp32_params, max_norm, norm_type)
        else:
            return -1

    def state_dict(self):
        """
        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.
        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict
        of the contained Pytorch optimizer.
        Example::

            checkpoint = {}
            checkpoint['model'] = model.state_dict()
            checkpoint['optimizer'] = optimizer.state_dict()
            torch.save(checkpoint, "saved.pth")
        """
        state_dict = {}
        state_dict['loss_scaler'] = self.loss_scaler
        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale
        state_dict['overflow'] = self.overflow
        state_dict['first_closure_call_this_step'] = self.first_closure_call_this_step
        state_dict['optimizer_state_dict'] = self.optimizer.state_dict()
        state_dict['fp32_from_fp16'] = self.fp32_from_fp16_groups
        return state_dict

    def load_state_dict(self, state_dict):
        """
        Loads a state_dict created by an earlier call to state_dict(). 
        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, 
        whose parameters in turn came from ``model``, it is expected that the user 
        will call ``model.load_state_dict()`` before
        ``fp16_optimizer_instance.load_state_dict()`` is called.

        Example::

            model = torch.nn.Linear(D_in, D_out).cuda().half()
            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)
            ...
            checkpoint = torch.load("saved.pth")
            model.load_state_dict(checkpoint['model'])
            optimizer.load_state_dict(checkpoint['optimizer'])
        """
        # I think it should actually be ok to reload the optimizer before the model.
        self.loss_scaler = state_dict['loss_scaler']
        self.dynamic_loss_scale = state_dict['dynamic_loss_scale']
        self.overflow = state_dict['overflow']
        self.first_closure_call_this_step = state_dict['first_closure_call_this_step']
        self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])
        # At this point, the optimizer's references to the model's fp32 parameters are up to date.
        # The optimizer's hyperparameters and internal buffers are also up to date.  
        # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
        # out of date.  There are two options.  
        # 1:  Refresh the master params from the model's fp16 params.  
        # This requires less storage but incurs precision loss.
        # 2:  Save and restore the fp32 master copies separately.
        # We choose option 2.
        # 
        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device 
        # of their associated parameters, because it's possible those buffers might not exist yet in 
        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been 
        # constructed in the same way as the one whose state_dict we are loading, the same master params
        # are guaranteed to exist, so we can just copy_() from the saved master params.
        for current_group, saved_group in zip(self.fp32_from_fp16_groups, state_dict['fp32_from_fp16']):
            for current, saved in zip(current_group, saved_group):
                current.data.copy_(saved.data)

    def step(self, closure=None): # could add clip option.
        """
        If no closure is supplied, :attr:`step` should be called after 
        ``fp16_optimizer_obj.backward(loss)``.
        :attr:`step` updates the fp32 master copy of parameters using the optimizer supplied to
        :class:`FP16_Optimizer`'s constructor, then copies the updated fp32 params into the fp16 params
        originally referenced by :class:`FP16_Optimizer`'s constructor, so the user may immediately run
        another forward pass using their model.

        If a closure is supplied, :attr:`step` may be called without a prior call to 
        :attr:`backward(loss)`.
        This control flow is identical to `ordinary Pytorch optimizer use`_ with closures.
        However, the user should take care that any ``loss.backward()`` call within the closure
        has been replaced by ``fp16_optimizer_obj.backward(loss)``.

        Args:
           closure (optional):  Closure that will be supplied to the underlying optimizer originally passed to :class:`FP16_Optimizer`'s constructor.  closure should call :attr:`zero_grad()` on the :class:`FP16_Optimizer` object, compute the loss, call :attr:`backward(loss)`, and return the loss.

        Example with closure::

            # optimizer is assumed to be an FP16_Optimizer object, previously constructed from an 
            # existing pytorch optimizer.
            for input, target in dataset:
                def closure():
                    optimizer.zero_grad()
                    output = model(input)
                    loss = loss_fn(output, target)
                    # loss.backward() becomes:
                    optimizer.backward(loss)
                    return loss
                optimizer.step(closure)

        .. warning::
            Currently, calling :attr:`step` with a closure is not compatible with dynamic loss scaling.

        .. _`ordinary Pytorch optimizer use`:
            http://pytorch.org/docs/master/optim.html#optimizer-step-closure
        """
        if closure is not None and isinstance(self.loss_scaler, DynamicLossScaler):
            raise TypeError("Using step with a closure is currently not "
                            "compatible with dynamic loss scaling.")

        scale = self.loss_scaler.loss_scale
        self._update_scale(self.overflow)

        if self.overflow:
            print("OVERFLOW! Skipping step. Attempted loss scale: {}, reducing to {}"
                .format(scale, self.loss_scale))
            return
        
        if closure is not None:
            retval = self._step_with_closure(closure)
        else:
            retval = self.optimizer.step()

        self._master_params_to_model_params()

        return retval

    def _step_with_closure(self, closure):
        def wrapped_closure():
            if self.first_closure_call_this_step:
                # We expect that the fp16 params are initially fresh on entering self.step(),
                # so _master_params_to_model_params() is unnecessary the first time wrapped_closure()
                # is called within self.optimizer.step().
                self.first_closure_call_this_step = False
            else:
                # If self.optimizer.step() internally calls wrapped_closure more than once,
                # it may update the fp32 params after each call.  However, self.optimizer 
                # doesn't know about the fp16 params at all.  If the fp32 params get updated,
                # we can't rely on self.optimizer to refresh the fp16 params.  We need
                # to handle that manually:
                self._master_params_to_model_params()
            # Our API expects the user to give us ownership of the backward() call by
            # replacing all calls to loss.backward() with optimizer.backward(loss).
            # This requirement holds whether or not the call to backward() is made within a closure.
            # If the user is properly calling optimizer.backward(loss) within "closure," 
            # calling closure() here will give the fp32 master params fresh gradients
            # for the optimizer to play with, so all wrapped_closure needs to do is call 
            # closure() and return the loss.
            temp_loss = closure() 
            return temp_loss

        retval = self.optimizer.step(wrapped_closure)

        self.first_closure_call_this_step = True

        return retval

    def backward(self, loss, update_master_grads=True):
        """ 
        :attr:`backward` performs the following conceptual steps:

        1. fp32_loss = loss.float() (see first Note below)
        2. scaled_loss = fp32_loss*loss_scale
        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's leaves (which may be fp16, fp32, or a mixture, depending how your model was defined).
        4. fp16 grads are then copied to the master params' ``.grad`` attributes (see second Note), which are guaranteed to be fp32.
        5. Finally, master grads are divided by loss_scale.

        In this way, after :attr:`backward`, the master params have fresh gradients,
        and :attr:`step` may be called.

        .. note::
            :attr:`backward` internally converts the loss to fp32 before applying the loss scale.
            This provides some additional safety against overflow if the user has supplied an 
            fp16 loss value.  
            However, for maximum overflow safety, the user should
            compute the loss criterion (MSE, cross entropy, etc) in fp32 before supplying it to 
            :attr:`backward`.

        .. warning::
            The gradients found in a model's leaves after the call to 
            :attr:`backward` should not be regarded as valid in general, 
            because it's possible 
            they have been scaled (and in the case of dynamic loss scaling, 
            the scale factor may change over time).  
            If the user wants to inspect gradients after a call to :attr:`backward`,  
            only the master gradients should be regarded as valid.  These can be retrieved via
            :attr:`inspect_master_grad_data()`.

        Args:
            loss:  The loss output by the user's model.  loss may be either float or half (but see first Note above).
            update_master_grads (bool, optional, default=True):  Option to copy fp16 grads to fp32 grads on this call.  By setting this to False, the user can delay the copy, which is useful to eliminate redundant fp16->fp32 grad copies if :attr:`backward` is being called on multiple losses in one iteration.  If set to False, the user becomes responsible for calling :attr:`update_master_grads` before calling :attr:`step`.

        Example::

            # Ordinary operation:
            optimizer.backward(loss)

            # Naive operation with multiple losses (technically valid, but less efficient):
            # fp32 grads will be correct after the second call,  but 
            # the first call incurs an unnecessary fp16->fp32 grad copy.
            optimizer.backward(loss1)
            optimizer.backward(loss2)

            # More efficient way to handle multiple losses:
            # The fp16->fp32 grad copy is delayed until fp16 grads from all 
            # losses have been accumulated.
            optimizer.backward(loss1, update_master_grads=False)
            optimizer.backward(loss2, update_master_grads=False)
            optimizer.update_master_grads()
        """ 
        # To consider:  try multiple backward passes using retain_grad=True to find 
        # a loss scale that works.  After you find a loss scale that works, do a final dummy
        # backward pass with retain_graph=False to tear down the graph.  Doing this would avoid 
        # discarding the iteration,  but probably wouldn't improve overall efficiency.  
        self.loss_scaler.backward(loss.float())
        if update_master_grads:
            self.update_master_grads()

    def update_master_grads(self):
        """
        Copy the ``.grad`` attribute from stored references to fp16 parameters to 
        the ``.grad`` attribute of the fp32 master parameters that are directly 
        updated by the optimizer.  :attr:`update_master_grads` only needs to be called if
        ``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.
        """
        if self.dynamic_loss_scale:
            self._check_overflow()
            if self.overflow: return
        self._model_grads_to_master_grads()
        self._downscale_master()

    def inspect_master_grad_data(self):
        """
        When running with :class:`FP16_Optimizer`, 
        ``.grad`` attributes of a model's fp16 leaves should not be
        regarded as truthful, because they might be scaled.  
        After a call to :attr:`fp16_optimizer_obj.backward(loss)`, if no overflow was encountered,
        the fp32 master params' ``.grad``
        attributes will contain valid gradients properly divided by the loss scale.  However, 
        because :class:`FP16_Optimizer` flattens some parameters, accessing them may be 
        nonintuitive.  :attr:`inspect_master_grad_data`
        allows those gradients to be viewed with shapes corresponding to their associated model leaves.

        Returns:
            List of lists (one list for each parameter group).  The list for each parameter group
            is a list of the ``.grad.data`` attributes of the fp32 master params belonging to that group.                 
        """
        raise NotImplementedError("Currently not implemented, working on it...")
        fp32_grads_each_group = []
        if self.overflow:
            print("Warning:  calling FP16_Optimizer.inspect_master_grad_data while in an overflow state.  "
                  "Gradients are currently invalid (may be inf, nan, or stale).  Returning None.")
            return None
        else:
            return None

    # Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
    def _get_loss_scale(self):
        return self.loss_scaler.loss_scale

    def _set_loss_scale(self, value):
        self.loss_scaler.cur_scale = value

    loss_scale = property(_get_loss_scale, _set_loss_scale)

    # Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
    def _get_state(self):
        return self.optimizer.state

    def _set_state(self, value):
        self.optimizer.state = value

    state = property(_get_state, _set_state)

    # Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
    # (for example, to adjust the learning rate)
    def _get_param_groups(self):
        return self.optimizer.param_groups

    def _set_param_groups(self, value):
        self.optimizer.param_groups = value

    param_groups = property(_get_param_groups, _set_param_groups)


# PROJECT: NVIDIA_sentiment-discovery FILE: fp16/__init__.py
from .fp16 import *
from .loss_scaler import *

# PROJECT: NVIDIA_sentiment-discovery FILE: fp16/fp16util.py
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors

class tofp16(nn.Module):
    """
    Model wrapper that implements::

        def forward(self, input):
            return input.half()
    """

    def __init__(self):
        super(tofp16, self).__init__()

    def forward(self, input):
        return input.half()


def BN_convert_float(module):
    '''
    Designed to work with network_to_half.
    BatchNorm layers need parameters in single precision.
    Find all layers and convert them back to float. This can't
    be done with built in .apply as that function will apply
    fn to all modules, parameters, and buffers. Thus we wouldn't
    be able to guard the float conversion based on the module type.
    '''
    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):
        module.float()
    for child in module.children():
        BN_convert_float(child)
    return module


def network_to_half(network):
    """
    Convert model to half precision in a batchnorm-safe way.
    """
    return nn.Sequential(tofp16(), BN_convert_float(network.half()))


def backwards_debug_hook(grad):
    raise RuntimeError("master_params recieved a gradient in the backward pass!")

def prep_param_lists(model, flat_master=False):
    """
    Creates a list of FP32 master parameters for a given model, as in 
    `Training Neural Networks with Mixed Precision:  Real Examples`_.

    Args:
        model (torch.nn.Module): Existing Pytorch model
        flat_master (bool, optional, default=False):  Flatten the master parameters into a single tensor, as a performance optimization.
    Returns:
        A tuple (``model_params``, ``master_params``). ``model_params`` is a list of the model's parameters for later use with :func:`model_grads_to_master_grads` and :func:`master_params_to_model_params`.  ``master_params`` is a list of FP32 master gradients.  If ``flat_master=True``, ``master_params`` will be a list with one element.

    Example::

        model_params, master_params = prep_param_lists(model)

    .. warning::
        Currently, if ``flat_master=True``, all the model's parameters must be the same type.  If the model has parameters of different types, use ``flat_master=False``, or use :class:`FP16_Optimizer`.

    .. _`Training Neural Networks with Mixed Precision:  Real Examples`:
        http://on-demand.gputechconf.com/gtc/2018/video/S81012/
    """
    model_params = [param for param in model.parameters() if param.requires_grad]

    if flat_master:
        # Give the user some more useful error messages
        try:
            # flatten_dense_tensors returns a contiguous flat array.
            # http://pytorch.org/docs/master/_modules/torch/_utils.html
            master_params = _flatten_dense_tensors([param.data for param in model_params]).float()
        except:
            print("Error in prep_param_lists:  model may contain a mixture of parameters "
                      "of different types.  Use flat_master=False, or use F16_Optimizer.")
            raise
        master_params = torch.nn.Parameter(master_params)
        master_params.requires_grad = True
        # master_params.register_hook(backwards_debug_hook)
        if master_params.grad is None:
            master_params.grad = master_params.new(*master_params.size())
        return model_params, [master_params]
    else:
        master_params = [param.clone().float().detach() for param in model_params]
        for param in master_params:
            param.requires_grad = True
        return model_params, master_params


def model_grads_to_master_grads(model_params, master_params, flat_master=False):
    """
    Copy model gradients to master gradients.  

    Args:
        model_params:  List of model parameters created by :func:`prep_param_lists`.
        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`model_grads_to_master_grads`.
    """
    if flat_master:
        # The flattening may incur one more deep copy than is necessary.
        master_params[0].grad.data.copy_(
            _flatten_dense_tensors([p.grad.data for p in model_params]))
    else:
        for model, master in zip(model_params, master_params):
            if model.grad is not None:
                if master.grad is None:
                    master.grad = Variable(master.data.new(*master.data.size()))
                master.grad.data.copy_(model.grad.data)
            else:
                master.grad = None


def master_params_to_model_params(model_params, master_params, flat_master=False):
    """
    Copy master parameters to model parameters.

    Args:
        model_params:  List of model parameters created by :func:`prep_param_lists`.
        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`master_params_to_model_params`.
    """
    if flat_master:
        for model, master in zip(model_params, 
                                 _unflatten_dense_tensors(master_params[0].data, model_params)):
            model.data.copy_(master)
    else:
        for model, master in zip(model_params, master_params):
            model.data.copy_(master.data)

# item() is a recent addition, so this helps with backward compatibility.
def to_python_float(t):
    if hasattr(t, 'item'):
        return t.item()
    else:
        return t[0]

# PROJECT: NVIDIA_sentiment-discovery FILE: fp16/loss_scaler.py
import torch

class LossScaler:

    def __init__(self, scale=1):
        self.cur_scale = scale

    # `params` is a list / generator of torch.Variable
    def has_overflow(self, params):
        return False

    # `x` is a torch.Tensor
    def _has_inf_or_nan(x):
        return False

    # `overflow` is boolean indicating whether we overflowed in gradient
    def update_scale(self, overflow):
        pass

    @property
    def loss_scale(self):
        return self.cur_scale

    def scale_gradient(self, module, grad_in, grad_out):
        return tuple(self.loss_scale * g for g in grad_in)

    def backward(self, loss):
        scaled_loss = loss*self.loss_scale
        scaled_loss.backward()

class DynamicLossScaler:

    def __init__(self,
                 init_scale=2**32,
                 scale_factor=2.,
                 scale_window=1000):
        self.cur_scale = init_scale
        self.cur_iter = 0
        self.last_overflow_iter = -1
        self.scale_factor = scale_factor
        self.scale_window = scale_window

    # `params` is a list / generator of torch.Variable
    def has_overflow(self, params):
#        return False
        for p in params:
            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):
                return True

        return False

    # `x` is a torch.Tensor
    def _has_inf_or_nan(x):
        try:
            # Stopgap until upstream fixes sum() on HalfTensors
            cpu_sum = float(x.float().sum())
            # cpu_sum = float(x.sum())
            # print(cpu_sum)
        except RuntimeError as instance:
            # We want to check if inst is actually an overflow exception.
            # RuntimeError could come from a different error.
            # If so, we still want the exception to propagate.
            if "value cannot be converted" not in instance.args[0]:
                raise
            return True
        else:
            if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:
                return True
            return False

    # `overflow` is boolean indicating whether we overflowed in gradient
    def update_scale(self, overflow):
        if overflow:
            #self.cur_scale /= self.scale_factor
            self.cur_scale = max(self.cur_scale/self.scale_factor, 1)
            self.last_overflow_iter = self.cur_iter
        else:
            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:
                self.cur_scale *= self.scale_factor
#        self.cur_scale = 1
        self.cur_iter += 1

    @property
    def loss_scale(self):
        return self.cur_scale

    def scale_gradient(self, module, grad_in, grad_out):
        return tuple(self.loss_scale * g for g in grad_in)

    def backward(self, loss):
        scaled_loss = loss*self.loss_scale
        scaled_loss.backward()
        
##############################################################        
# Example usage below here -- assuming it's in a separate file
##############################################################
if __name__ == "__main__":
    import torch
    from torch.autograd import Variable
    from dynamic_loss_scaler import DynamicLossScaler

    # N is batch size; D_in is input dimension;
    # H is hidden dimension; D_out is output dimension.
    N, D_in, H, D_out = 64, 1000, 100, 10

    # Create random Tensors to hold inputs and outputs, and wrap them in Variables.
    x = Variable(torch.randn(N, D_in), requires_grad=False)
    y = Variable(torch.randn(N, D_out), requires_grad=False)

    w1 = Variable(torch.randn(D_in, H), requires_grad=True)
    w2 = Variable(torch.randn(H, D_out), requires_grad=True)
    parameters = [w1, w2]

    learning_rate = 1e-6
    optimizer = torch.optim.SGD(parameters, lr=learning_rate)
    loss_scaler = DynamicLossScaler()

    for t in range(500):
        y_pred = x.mm(w1).clamp(min=0).mm(w2)
        loss = (y_pred - y).pow(2).sum() * loss_scaler.loss_scale
        print('Iter {} loss scale: {}'.format(t, loss_scaler.loss_scale))
        print('Iter {} scaled loss: {}'.format(t, loss.data[0]))
        print('Iter {} unscaled loss: {}'.format(t, loss.data[0] / loss_scaler.loss_scale))

        # Run backprop
        optimizer.zero_grad()
        loss.backward()
        
        # Check for overflow
        has_overflow = DynamicLossScaler.has_overflow(parameters)
        
        # If no overflow, unscale grad and update as usual
        if not has_overflow:
            for param in parameters:
                param.grad.data.mul_(1. / loss_scaler.loss_scale)
            optimizer.step()
        # Otherwise, don't do anything -- ie, skip iteration
        else:
            print('OVERFLOW!')

        # Update loss scale for next iteration
        loss_scaler.update_scale(has_overflow)


# PROJECT: NVIDIA_sentiment-discovery FILE: multiproc.py
import torch
import sys
import os
import subprocess

argslist = list(sys.argv)[1:]

LOGDIR = 'distributed_logs'
if '--save' in argslist:
    savepath = os.path.splitext(os.path.basename(argslist[argslist.index('--save')+1]))[0]
else:
    savepath = 'model'
LOGDIR = os.path.join(LOGDIR, savepath)
if not os.path.exists(LOGDIR):
    os.makedirs(LOGDIR)

if '--world-size' in argslist:
    world_size = int(argslist[argslist.index('--world-size')+1])
else:
    world_size = torch.cuda.device_count()
    argslist.append('--world-size')
    argslist.append(str(world_size))

for i in range(world_size):
    if '--rank' in argslist:
        argslist[argslist.index('--rank')+1] = str(i)
    else:
        argslist.append('--rank')
        argslist.append(str(i))
    #stdout = open(os.path.join(LOGDIR, str(i)+".log"), "w")
    stdout = None if i == 0 else open(os.path.join(LOGDIR, str(i)+".log"), "w")
    call = subprocess.Popen
    if i == world_size-1:
        call = subprocess.call
    call([str(sys.executable)]+argslist, stdout=stdout)



